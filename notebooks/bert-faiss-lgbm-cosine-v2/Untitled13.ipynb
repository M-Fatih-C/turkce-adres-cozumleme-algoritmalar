{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyO9/I0ftWgr/JPTdvWlz2aY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"Untitled12.ipynb\n","\n","Automatically generated by Colab.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ecGAOKl2Nmjc9C1zPkF7wHT8lZ4xttPW\n","\"\"\"\n","\n","# Teknofest x Hepsiburada Address Matching Pipeline\n","# Complete ML solution for Turkish address deduplication and matching\n","\n","# ================================================================\n","# STEP 0: SETUP AND INSTALLATIONS\n","# ================================================================\n","!pip install sentence-transformers faiss-cpu lightgbm scikit-learn pandas numpy\n","!pip install unidecode regex tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cq9lgfYOPKxp","executionInfo":{"status":"ok","timestamp":1756365543909,"user_tz":-180,"elapsed":20492,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}},"outputId":"8c6356d1-3f89-48b5-9dbd-236423206ac8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.8)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.12/dist-packages (1.4.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import string\n","from collections import defaultdict, Counter\n","import pickle\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import GroupKFold\n","from sklearn.preprocessing import LabelEncoder\n","import lightgbm as lgb\n","\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","from unidecode import unidecode\n","from tqdm import tqdm\n","import gc\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)"],"metadata":{"id":"gphxF0wqPNu4","executionInfo":{"status":"ok","timestamp":1756365571722,"user_tz":-180,"elapsed":27809,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# STEP 1: DATA PREPROCESSING & NORMALIZATION\n","# ================================================================\n","\n","class TurkishAddressNormalizer:\n","    \"\"\"Comprehensive Turkish address normalizer\"\"\"\n","\n","    def __init__(self):\n","        # Turkish abbreviation mappings\n","        self.abbreviations = {\n","            # Neighborhood/District\n","            'mh': 'mahallesi', 'mah': 'mahallesi', 'mahalle': 'mahallesi',\n","\n","            # Street types\n","            'cd': 'caddesi', 'cad': 'caddesi', 'cadde': 'caddesi',\n","            'sk': 'sokagi', 'sok': 'sokagi', 'sokak': 'sokagi',\n","            'blv': 'bulvari', 'bulv': 'bulvari', 'bulvar': 'bulvari',\n","            'osb': 'organize sanayi bolge', 'km': 'kilometre',\n","\n","            # Building types\n","            'apt': 'apartmani', 'ap': 'apartmani', 'apartman': 'apartmani',\n","            'sit': 'sitesi', 'site': 'sitesi',\n","            'blk': 'blok', 'blok': 'blok',\n","            'plz': 'plaza', 'plaza': 'plaza',\n","            'avm': 'alisveris merkezi',\n","\n","            # Address components\n","            'no': 'numara', 'nu': 'numara',\n","            'kt': 'kat', 'kat': 'kat',\n","            'dr': 'daire', 'daire': 'daire', 'da': 'daire',\n","            'pst': 'posta kodu',\n","\n","            # Directions\n","            'kz': 'kuzey', 'gy': 'guney', 'dt': 'dogu', 'bt': 'bati',\n","\n","            # Common words\n","            'yrm': 'yurdu', 'otel': 'oteli', 'hst': 'hastanesi',\n","            'unv': 'universitesi', 'lise': 'lisesi', 'okl': 'okulu'\n","        }\n","\n","        # Regex patterns for component extraction\n","        self.patterns = {\n","            'number': r'(?:no[:\\s]*|numara[:\\s]*|n[:\\s]*)?(\\d+)(?:[/\\-](\\d+))?',\n","            'floor': r'(?:kat[:\\s]*|kt[:\\s]*|k[:\\s]*)?(\\d+)(?:\\s*\\.?\\s*kat)?',\n","            'apartment': r'(?:daire[:\\s]*|dr[:\\s]*|d[:\\s]*)?(\\d+)(?:\\s*\\.?\\s*daire)?',\n","            'block': r'(?:blok[:\\s]*|blk[:\\s]*|b[:\\s]*)?([a-zA-Z]?\\d*)(?:\\s*\\.?\\s*blok)?',\n","            'postal_code': r'\\b(\\d{5})\\b'\n","        }\n","\n","    def normalize_turkish_chars(self, text):\n","        \"\"\"Convert Turkish characters and remove diacritics\"\"\"\n","        turkish_chars = {\n","            'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',\n","            'Ã‡': 'C', 'Äž': 'G', 'Ä°': 'I', 'Ã–': 'O', 'Åž': 'S', 'Ãœ': 'U'\n","        }\n","        for tr_char, en_char in turkish_chars.items():\n","            text = text.replace(tr_char, en_char)\n","        return text\n","\n","    def expand_abbreviations(self, text):\n","        \"\"\"Expand common Turkish abbreviations\"\"\"\n","        words = text.split()\n","        expanded_words = []\n","\n","        for word in words:\n","            # Remove punctuation for matching\n","            clean_word = word.strip('.,;:()-').lower()\n","            if clean_word in self.abbreviations:\n","                expanded_words.append(self.abbreviations[clean_word])\n","            else:\n","                expanded_words.append(word)\n","\n","        return ' '.join(expanded_words)\n","\n","    def standardize_numbers(self, text):\n","        \"\"\"Standardize number formats\"\"\"\n","        # Handle \"No:12\", \"No=12\", \"12/3\" patterns\n","        text = re.sub(r'no[:\\s=]*(\\d+)', r'numara \\1', text, flags=re.IGNORECASE)\n","        text = re.sub(r'(\\d+)[/\\-](\\d+)', r'numara \\1 daire \\2', text)\n","\n","        # Handle floor patterns\n","        text = re.sub(r'(\\d+)\\.?\\s*kat', r'\\1 kat', text, flags=re.IGNORECASE)\n","\n","        # Handle apartment patterns\n","        text = re.sub(r'(\\d+)\\.?\\s*daire', r'\\1 daire', text, flags=re.IGNORECASE)\n","\n","        return text\n","\n","    def clean_punctuation(self, text):\n","        \"\"\"Remove unnecessary punctuation and normalize spacing\"\"\"\n","        # Remove special characters but keep Turkish letters\n","        text = re.sub(r'[^\\w\\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ]', ' ', text)\n","\n","        # Normalize whitespace\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        return text.strip()\n","\n","    def extract_components(self, text):\n","        \"\"\"Extract address components using regex\"\"\"\n","        components = {}\n","\n","        for component, pattern in self.patterns.items():\n","            match = re.search(pattern, text, re.IGNORECASE)\n","            if match:\n","                if component == 'number' and match.group(2):\n","                    components['number'] = match.group(1)\n","                    components['apartment'] = match.group(2)\n","                else:\n","                    components[component] = match.group(1)\n","\n","        return components\n","\n","    def normalize(self, address):\n","        \"\"\"Apply full normalization pipeline\"\"\"\n","        if pd.isna(address) or not isinstance(address, str):\n","            return \"\"\n","\n","        # Convert to lowercase\n","        address = address.lower()\n","\n","        # Normalize Turkish characters\n","        address = self.normalize_turkish_chars(address)\n","\n","        # Expand abbreviations\n","        address = self.expand_abbreviations(address)\n","\n","        # Standardize numbers\n","        address = self.standardize_numbers(address)\n","\n","        # Clean punctuation\n","        address = self.clean_punctuation(address)\n","\n","        return address\n","\n","# Initialize normalizer\n","normalizer = TurkishAddressNormalizer()\n","\n","# ================================================================\n","# STEP 2: DATA LOADING AND PREPROCESSING\n","# ================================================================\n","\n","def load_and_preprocess_data():\n","    \"\"\"Load and preprocess the datasets\"\"\"\n","    print(\"Loading datasets...\")\n","\n","    # Load data (adjust paths as needed)\n","    train_df = pd.read_csv('train.csv')\n","    test_df = pd.read_csv('test.csv')\n","\n","    print(f\"Train shape: {train_df.shape}\")\n","    print(f\"Test shape: {test_df.shape}\")\n","    print(f\"Unique labels in train: {train_df['label'].nunique()}\")\n","\n","    # Normalize addresses\n","    print(\"Normalizing addresses...\")\n","    train_df['normalized_address'] = train_df['address'].apply(normalizer.normalize)\n","    test_df['normalized_address'] = test_df['address'].apply(normalizer.normalize)\n","\n","    # Remove empty addresses\n","    train_df = train_df[train_df['normalized_address'].str.len() > 0].reset_index(drop=True)\n","    test_df = test_df[test_df['normalized_address'].str.len() > 0].reset_index(drop=True)\n","\n","    print(f\"After cleaning - Train: {len(train_df)}, Test: {len(test_df)}\")\n","\n","    return train_df, test_df\n","\n","# Load data\n","train_df, test_df = load_and_preprocess_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16Tmdfb-PQ0Y","executionInfo":{"status":"ok","timestamp":1756365626950,"user_tz":-180,"elapsed":55225,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}},"outputId":"a8ebb1cc-5d32-4bf2-cd57-63f908066263"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading datasets...\n","Train shape: (848237, 2)\n","Test shape: (217241, 2)\n","Unique labels in train: 10390\n","Normalizing addresses...\n","After cleaning - Train: 848234, Test: 217241\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12D0YfYnO3NE","outputId":"0dad7ce3-6726-44e3-e2c5-8b1f6e36c148"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting TF-IDF vectorizer...\n"]}],"source":["# ================================================================\n","# STEP 3: BASELINE MODEL - TF-IDF CENTROID\n","# ================================================================\n","\n","class TFIDFCentroidModel:\n","    def __init__(self, ngram_range=(2,11), max_features=None, checkpoint_every=50000):\n","        self.vectorizer = TfidfVectorizer(\n","            analyzer='char',\n","            ngram_range=ngram_range,\n","            max_features=max_features,\n","            lowercase=True,\n","            strip_accents='unicode',\n","            sublinear_tf=True,\n","            min_df=2,\n","            max_df=0.95,\n","            norm='l2',\n","            use_idf=True,\n","            smooth_idf=True,\n","            dtype=np.float32\n","        )\n","        self.label_encoder = LabelEncoder()\n","        self.centroids = None\n","        self.labels = None\n","        self.checkpoint_every = checkpoint_every\n","\n","    def fit(self, addresses, labels):\n","        print(\"Fitting TF-IDF vectorizer...\")\n","        encoded_labels = self.label_encoder.fit_transform(labels)\n","        self.labels = self.label_encoder.classes_\n","\n","        X = self.vectorizer.fit_transform(addresses)\n","        self.centroids = np.zeros((len(self.labels), X.shape[1]), dtype=np.float32)\n","\n","        print(\"Computing centroids with checkpoints...\")\n","        for i, label in enumerate(self.labels):\n","            mask = encoded_labels == i\n","            if mask.sum() > 0:\n","                self.centroids[i] = X[mask].mean(axis=0).A1.astype(np.float32)\n","            # Checkpoint save\n","            if (i+1) % self.checkpoint_every == 0:\n","                checkpoint_path = f'model_checkpoint_{i+1}.npz'\n","                np.savez_compressed(checkpoint_path, centroids=self.centroids, labels=self.labels)\n","                print(f\"[Checkpoint] Saved at label {i+1} -> {checkpoint_path}\")\n","        print(f\"Model fitted with {len(self.labels)} labels\")\n","\n","    def predict(self, addresses, top_k=1):\n","        X = self.vectorizer.transform(addresses)\n","        similarities = cosine_similarity(X, self.centroids)\n","        if top_k == 1:\n","            predictions = similarities.argmax(axis=1)\n","            return self.label_encoder.inverse_transform(predictions)\n","        else:\n","            top_indices = np.argsort(similarities, axis=1)[:, -top_k:][:, ::-1]\n","            top_labels = [self.label_encoder.inverse_transform(indices) for indices in top_indices]\n","            return top_labels, similarities\n","\n","# ================================================================\n","# TRAIN AND SAVE\n","# ================================================================\n","\n","tfidf_model = TFIDFCentroidModel(checkpoint_every=50000)\n","tfidf_model.fit(train_df['normalized_address'], train_df['label'])\n","\n","tfidf_predictions = tfidf_model.predict(test_df['normalized_address'])\n","\n","baseline_submission = pd.DataFrame({\n","    'id': test_df['id'],\n","    'label': tfidf_predictions\n","})\n","baseline_submission.to_csv('baseline_submission.csv', index=False)\n","print(\"Baseline submission saved with checkpoint support!\")"]}]}