{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPG+k00wgJ7hNUI+QsrU4pl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install pandas numpy scikit-learn lightgbm xgboost \\\n","            rapidfuzz unidecode faiss-cpu \\\n","            sentence-transformers networkx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jn0wrbBk8JZ6","executionInfo":{"status":"ok","timestamp":1755784999365,"user_tz":-180,"elapsed":11240,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"a9e96a37-50ba-4e9e-f5cf-5b1ecdd0656b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n","Collecting rapidfuzz\n","  Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting unidecode\n","  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.14.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n","Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode, rapidfuzz, faiss-cpu\n","Successfully installed faiss-cpu-1.12.0 rapidfuzz-3.13.0 unidecode-1.4.0\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"dExWgf0D7fka","executionInfo":{"status":"error","timestamp":1755785259869,"user_tz":-180,"elapsed":43565,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"be0641b3-6249-4f06-a914-66dddff6fa9b"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'cluster_id'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1416241039.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# pairs + features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1416241039.py\u001b[0m in \u001b[0;36mmake_pairs\u001b[0;34m(train_df, n_pos, n_neg, seed)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpos_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# positive pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cluster_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'cluster_id'"]}],"source":["# turkish_address_pipeline.py\n","import pandas as pd, numpy as np, re, string, unidecode, json\n","from collections import defaultdict\n","from sklearn.model_selection import GroupKFold\n","from sklearn.metrics import f1_score\n","from rapidfuzz import fuzz\n","import lightgbm as lgb\n","import faiss\n","from sentence_transformers import SentenceTransformer\n","import networkx as nx\n","\n","##############################################\n","# 1. PREPROCESSING\n","##############################################\n","ABBREV_MAP = {\n","    \"mah\":\"mahalle\", \"mh\":\"mahalle\",\n","    \"cd\":\"caddesi\", \"cad\":\"caddesi\",\n","    \"sk\":\"sokak\", \"sok\":\"sokak\",\n","    \"apt\":\"apartman\", \"ap\":\"apartman\",\n","    \"blv\":\"bulvar\", \"bul\":\"bulvar\"\n","}\n","\n","def normalize_text(s: str) -> str:\n","    if pd.isna(s): return \"\"\n","    s = s.lower()\n","    s = unidecode.unidecode(s)  # remove turkish accents\n","    for k,v in ABBREV_MAP.items():\n","        s = re.sub(rf\"\\b{k}\\b\", v, s)\n","    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","def preprocess(df):\n","    df[\"address_norm\"] = df[\"address\"].astype(str).apply(normalize_text)\n","    return df\n","\n","##############################################\n","# 2. PAIR GENERATION\n","##############################################\n","def make_pairs(train_df, n_pos=50000, n_neg=50000, seed=42):\n","    np.random.seed(seed)\n","    pos_pairs, neg_pairs = [], []\n","    # positive pairs\n","    for cid, group in train_df.groupby(\"cluster_id\"):\n","        ids = group.index.tolist()\n","        if len(ids) < 2: continue\n","        chosen = np.random.choice(ids, size=min(len(ids), 20), replace=False)\n","        for i in range(len(chosen)-1):\n","            pos_pairs.append((chosen[i], chosen[i+1], 1))\n","    # negative pairs (different cluster, same il/ilce block approx)\n","    ids = train_df.index.tolist()\n","    for _ in range(n_neg):\n","        i,j = np.random.choice(ids, 2, replace=False)\n","        if train_df.loc[i,\"cluster_id\"] != train_df.loc[j,\"cluster_id\"]:\n","            neg_pairs.append((i,j,0))\n","    pairs = pos_pairs + neg_pairs\n","    return pairs\n","\n","##############################################\n","# 3. FEATURE ENGINEERING\n","##############################################\n","model_sbert = SentenceTransformer(\"emrecan/bert-base-turkish-cased-mean-nli-stsb-tr\")\n","\n","def jaccard_ngram(a,b,n=3):\n","    A = {a[i:i+n] for i in range(len(a)-n+1)}\n","    B = {b[i:i+n] for i in range(len(b)-n+1)}\n","    if not A or not B: return 0\n","    return len(A&B)/len(A|B)\n","\n","def feature_vector(a1,a2,vec1,vec2):\n","    feats = {}\n","    feats[\"fuzz_ratio\"] = fuzz.ratio(a1,a2)/100\n","    feats[\"fuzz_partial\"] = fuzz.partial_ratio(a1,a2)/100\n","    feats[\"fuzz_token_sort\"] = fuzz.token_sort_ratio(a1,a2)/100\n","    for n in [2,3,4]:\n","        feats[f\"jaccard_{n}\"] = jaccard_ngram(a1,a2,n)\n","    # cosine embedding\n","    num = np.dot(vec1,vec2)\n","    denom = np.linalg.norm(vec1)*np.linalg.norm(vec2)+1e-9\n","    feats[\"cosine\"] = num/denom\n","    feats[\"len_diff\"] = abs(len(a1)-len(a2))/(max(len(a1),len(a2))+1e-9)\n","    return feats\n","\n","def build_feature_matrix(train_df, pairs):\n","    addr = train_df[\"address_norm\"].tolist()\n","    emb = model_sbert.encode(addr, batch_size=64, show_progress_bar=True)\n","    rows=[]\n","    for i,j,y in pairs:\n","        feats = feature_vector(addr[i], addr[j], emb[i], emb[j])\n","        feats[\"y\"]=y; feats[\"id1\"]=i; feats[\"id2\"]=j\n","        rows.append(feats)\n","    return pd.DataFrame(rows)\n","\n","##############################################\n","# 4. MODEL TRAINING\n","##############################################\n","def train_model(feats):\n","    X = feats.drop(columns=[\"y\",\"id1\",\"id2\"])\n","    y = feats[\"y\"]\n","    lgbm = lgb.LGBMClassifier(\n","        n_estimators=500, learning_rate=0.05,\n","        num_leaves=63, subsample=0.8, colsample_bytree=0.8\n","    )\n","    cv = GroupKFold(n_splits=5)\n","    scores=[]\n","    for train_idx, val_idx in cv.split(X,y,groups=feats[\"id1\"]):\n","        lgbm.fit(X.iloc[train_idx], y.iloc[train_idx])\n","        preds = lgbm.predict(X.iloc[val_idx])\n","        scores.append(f1_score(y.iloc[val_idx], preds))\n","    print(\"CV F1:\", np.mean(scores))\n","    lgbm.fit(X,y)\n","    return lgbm\n","\n","##############################################\n","# 5. CANDIDATE RETRIEVAL (FAISS)\n","##############################################\n","def build_faiss_index(embeddings):\n","    d = embeddings.shape[1]\n","    index = faiss.IndexFlatIP(d)\n","    faiss.normalize_L2(embeddings)\n","    index.add(embeddings)\n","    return index\n","\n","def get_candidates(index, emb, k=20):\n","    faiss.normalize_L2(emb)\n","    D,I = index.search(emb, k)\n","    return I\n","\n","##############################################\n","# 6. CLUSTERING\n","##############################################\n","def clustering(test_df, preds, thresh=0.6):\n","    G = nx.Graph()\n","    for _,row in preds.iterrows():\n","        if row[\"prob\"]>thresh:\n","            G.add_edge(row[\"id1\"], row[\"id2\"])\n","    clusters = list(nx.connected_components(G))\n","    cluster_map={}\n","    for cid, comp in enumerate(clusters):\n","        for idx in comp:\n","            cluster_map[idx]=cid\n","    labels = [cluster_map.get(i, -1) for i in range(len(test_df))]\n","    return labels\n","\n","##############################################\n","# 7. MAIN EXECUTION\n","##############################################\n","if __name__==\"__main__\":\n","    train = pd.read_csv(\"train.csv\")\n","    test  = pd.read_csv(\"test.csv\")\n","\n","    train = preprocess(train)\n","    test = preprocess(test)\n","\n","    # pairs + features\n","    pairs = make_pairs(train)\n","    feats = build_feature_matrix(train, pairs)\n","    model = train_model(feats)\n","\n","    # embeddings for test\n","    test_emb = model_sbert.encode(test[\"address_norm\"].tolist(), batch_size=64, show_progress_bar=True)\n","    index = build_faiss_index(test_emb)\n","\n","    # candidate + predict\n","    preds=[]\n","    for i,vec in enumerate(test_emb):\n","        cands = get_candidates(index, vec.reshape(1,-1), k=5)[0]\n","        for j in cands:\n","            if i>=j: continue\n","            f = feature_vector(test.loc[i,\"address_norm\"], test.loc[j,\"address_norm\"], vec, test_emb[j])\n","            p = model.predict_proba(pd.DataFrame([f]))[0,1]\n","            preds.append({\"id1\":i,\"id2\":j,\"prob\":p})\n","    preds=pd.DataFrame(preds)\n","\n","    # clustering\n","    labels = clustering(test,preds,thresh=0.6)\n","    submission = pd.DataFrame({\"record_id\":test[\"record_id\"],\"cluster_id\":labels})\n","    submission.to_csv(\"sample_submission.csv\",index=False)\n","    print(\"Saved teknofest_submission.csv\")\n"]}]}