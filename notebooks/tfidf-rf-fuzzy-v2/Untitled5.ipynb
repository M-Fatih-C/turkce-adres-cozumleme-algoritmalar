{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMHJhJEnvHiZ/lIrzfxdp6u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ========================================================================\n","# TEKNOFEST 2025 - ADVANCED ADDRESS RESOLUTION SOLUTION\n","# Complete Colab-Ready Code for 0.80+ Score\n","# ========================================================================\n","\n","# STEP 1: INSTALL REQUIREMENTS\n","print(\"ğŸš€ Installing required packages...\")\n","import subprocess\n","import sys\n","\n","def install_package(package):\n","    try:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","        return True\n","    except:\n","        return False\n","\n","# Install packages with fallback\n","packages = [\n","    \"pandas\", \"numpy\", \"scikit-learn\", \"matplotlib\", \"seaborn\",\n","    \"xgboost\", \"lightgbm\", \"catboost\", \"fuzzywuzzy\", \"python-levenshtein\"\n","]\n","\n","for package in packages:\n","    success = install_package(package)\n","    print(f\"  {package}: {'âœ…' if success else 'âŒ'}\")\n","\n","print(\"ğŸ“¦ Package installation completed!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5L_ZGXhgaMH","executionInfo":{"status":"ok","timestamp":1755861908045,"user_tz":-180,"elapsed":30024,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}},"outputId":"a5e9b0bb-52d4-44bb-a937-78ed400ee2d2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Installing required packages...\n","  pandas: âœ…\n","  numpy: âœ…\n","  scikit-learn: âœ…\n","  matplotlib: âœ…\n","  seaborn: âœ…\n","  xgboost: âœ…\n","  lightgbm: âœ…\n","  catboost: âœ…\n","  fuzzywuzzy: âœ…\n","  python-levenshtein: âœ…\n","ğŸ“¦ Package installation completed!\n"]}]},{"cell_type":"code","source":["# ========================================================================\n","# STEP 2: IMPORT LIBRARIES WITH FALLBACK\n","# ========================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","from collections import defaultdict, Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Optional imports with fallback\n","try:\n","    from xgboost import XGBClassifier\n","    XGBOOST_AVAILABLE = True\n","    print(\"âœ… XGBoost loaded successfully\")\n","except ImportError:\n","    XGBOOST_AVAILABLE = False\n","    print(\"âš ï¸ XGBoost not available, using alternatives\")\n","\n","try:\n","    import lightgbm as lgb\n","    LIGHTGBM_AVAILABLE = True\n","    print(\"âœ… LightGBM loaded successfully\")\n","except ImportError:\n","    LIGHTGBM_AVAILABLE = False\n","    print(\"âš ï¸ LightGBM not available, using alternatives\")\n","\n","try:\n","    from catboost import CatBoostClassifier\n","    CATBOOST_AVAILABLE = True\n","    print(\"âœ… CatBoost loaded successfully\")\n","except ImportError:\n","    CATBOOST_AVAILABLE = False\n","    print(\"âš ï¸ CatBoost not available, using alternatives\")\n","\n","try:\n","    from fuzzywuzzy import fuzz\n","    FUZZYWUZZY_AVAILABLE = True\n","    print(\"âœ… FuzzyWuzzy loaded successfully\")\n","except ImportError:\n","    FUZZYWUZZY_AVAILABLE = False\n","    print(\"âš ï¸ FuzzyWuzzy not available, using simple similarity\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQxkFrE4grqO","executionInfo":{"status":"ok","timestamp":1755861941910,"user_tz":-180,"elapsed":5698,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}},"outputId":"529dd112-ed7f-4d72-eecd-f38d5bb6576a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… XGBoost loaded successfully\n","âœ… LightGBM loaded successfully\n","âœ… CatBoost loaded successfully\n","âœ… FuzzyWuzzy loaded successfully\n"]}]},{"cell_type":"code","source":["# ========================================================================\n","# STEP 3: TURKEY LOCATION DATABASE\n","# ========================================================================\n","\n","class TurkeyLocationDatabase:\n","    \"\"\"Comprehensive Turkey location database\"\"\"\n","\n","    def __init__(self):\n","        # 81 Turkish provinces\n","        self.provinces = {\n","            'adana': ['adana'], 'adiyaman': ['adÄ±yaman', 'adiyaman'], 'afyonkarahisar': ['afyon', 'afyonkarahisar'],\n","            'agri': ['aÄŸrÄ±', 'agri'], 'aksaray': ['aksaray'], 'amasya': ['amasya'], 'ankara': ['ankara'],\n","            'antalya': ['antalya'], 'ardahan': ['ardahan'], 'artvin': ['artvin'], 'aydin': ['aydÄ±n', 'aydin'],\n","            'balikesir': ['balÄ±kesir', 'balikesir'], 'bartin': ['bartÄ±n', 'bartin'], 'batman': ['batman'],\n","            'bayburt': ['bayburt'], 'bilecik': ['bilecik'], 'bingol': ['bingÃ¶l', 'bingol'], 'bitlis': ['bitlis'],\n","            'bolu': ['bolu'], 'burdur': ['burdur'], 'bursa': ['bursa'], 'canakkale': ['Ã§anakkale', 'canakkale'],\n","            'cankiri': ['Ã§ankÄ±rÄ±', 'cankiri'], 'corum': ['Ã§orum', 'corum'], 'denizli': ['denizli'],\n","            'diyarbakir': ['diyarbakÄ±r', 'diyarbakir'], 'duzce': ['dÃ¼zce', 'duzce'], 'edirne': ['edirne'],\n","            'elazig': ['elazÄ±ÄŸ', 'elazig'], 'erzincan': ['erzincan'], 'erzurum': ['erzurum'], 'eskisehir': ['eskiÅŸehir', 'eskisehir'],\n","            'gaziantep': ['gaziantep'], 'giresun': ['giresun'], 'gumushane': ['gÃ¼mÃ¼ÅŸhane', 'gumushane'],\n","            'hakkari': ['hakkÃ¢ri', 'hakkari'], 'hatay': ['hatay'], 'igdir': ['iÄŸdÄ±r', 'igdir'], 'isparta': ['isparta'],\n","            'istanbul': ['istanbul', 'Ä°stanbul'], 'izmir': ['izmir', 'Ä°zmir'], 'kahramanmaras': ['kahramanmaraÅŸ', 'kahramanmaras'],\n","            'karabuk': ['karabÃ¼k', 'karabuk'], 'karaman': ['karaman'], 'kars': ['kars'], 'kastamonu': ['kastamonu'],\n","            'kayseri': ['kayseri'], 'kilis': ['kilis'], 'kirikkale': ['kÄ±rÄ±kkale', 'kirikkale'], 'kirklareli': ['kÄ±rklareli', 'kirklareli'],\n","            'kirsehir': ['kÄ±rÅŸehir', 'kirsehir'], 'kocaeli': ['kocaeli'], 'konya': ['konya'], 'kutahya': ['kÃ¼tahya', 'kutahya'],\n","            'malatya': ['malatya'], 'manisa': ['manisa'], 'mardin': ['mardin'], 'mersin': ['mersin'],\n","            'mugla': ['muÄŸla', 'mugla'], 'mus': ['muÅŸ', 'mus'], 'nevsehir': ['nevÅŸehir', 'nevsehir'],\n","            'nigde': ['niÄŸde', 'nigde'], 'ordu': ['ordu'], 'osmaniye': ['osmaniye'], 'rize': ['rize'],\n","            'sakarya': ['sakarya'], 'samsun': ['samsun'], 'sanliurfa': ['ÅŸanlÄ±urfa', 'sanliurfa'],\n","            'siirt': ['siirt'], 'sinop': ['sinop'], 'sirnak': ['ÅŸÄ±rnak', 'sirnak'], 'sivas': ['sivas'],\n","            'tekirdag': ['tekirdaÄŸ', 'tekirdag'], 'tokat': ['tokat'], 'trabzon': ['trabzon'], 'tunceli': ['tunceli'],\n","            'usak': ['uÅŸak', 'usak'], 'van': ['van'], 'yalova': ['yalova'], 'yozgat': ['yozgat'], 'zonguldak': ['zonguldak']\n","        }\n","\n","        # Major districts by region\n","        self.districts = {\n","            # Ä°zmir districts\n","            'aliaga': ['aliaÄŸa', 'aliaga'], 'balcova': ['balÃ§ova', 'balcova'], 'bayindir': ['bayÄ±ndÄ±r', 'bayindir'],\n","            'bayrakli': ['bayraklÄ±', 'bayrakli'], 'bergama': ['bergama'], 'bornova': ['bornova'],\n","            'buca': ['buca'], 'cesme': ['Ã§eÅŸme', 'cesme'], 'ciglli': ['Ã§iÄŸli', 'ciglli'], 'dikili': ['dikili'],\n","            'foca': ['foÃ§a', 'foca'], 'gaziemir': ['gaziemir'], 'guzelbahce': ['gÃ¼zelbahÃ§e', 'guzelbahce'],\n","            'karabaglar': ['karabaÄŸlar', 'karabaglar'], 'karaburun': ['karaburun'], 'karsiyaka': ['karÅŸÄ±yaka', 'karsiyaka'],\n","            'kemalpasa': ['kemalpaÅŸa', 'kemalpasa'], 'kinik': ['kÄ±nÄ±k', 'kinik'], 'kiraz': ['kiraz'],\n","            'konak': ['konak'], 'menderes': ['menderes'], 'menemen': ['menemen'], 'narlidere': ['narlÄ±dere', 'narlidere'],\n","            'odemis': ['Ã¶demiÅŸ', 'odemis'], 'seferihisar': ['seferihisar'], 'selcuk': ['selÃ§uk', 'selcuk'],\n","            'tire': ['tire'], 'torbali': ['torbalÄ±', 'torbali'], 'urla': ['urla'],\n","\n","            # Manisa districts\n","            'ahmetli': ['ahmetli'], 'akhisar': ['akhisar'], 'alasehir': ['alaÅŸehir', 'alasehir'],\n","            'demirci': ['demirci'], 'golmarmara': ['gÃ¶lmarmara', 'golmarmara'], 'gordes': ['gÃ¶rdes', 'gordes'],\n","            'kirkagac': ['kÄ±rkaÄŸaÃ§', 'kirkagac'], 'koprubasi': ['kÃ¶prÃ¼baÅŸÄ±', 'koprubasi'], 'kula': ['kula'],\n","            'salihli': ['salihli'], 'sarigol': ['sarÄ±gÃ¶l', 'sarigol'], 'saruhanli': ['saruhanli'],\n","            'sehzadeler': ['ÅŸehzadeler', 'sehzadeler'], 'selendi': ['selendi'], 'soma': ['soma'],\n","            'turgutlu': ['turgutlu'], 'yunusemre': ['yunusemre'],\n","\n","            # Denizli districts\n","            'acipayam': ['acÄ±payam', 'acipayam'], 'baklan': ['baklan'], 'bekilli': ['bekilli'],\n","            'beyagac': ['beyaÄŸaÃ§', 'beyagac'], 'bozkurt': ['bozkurt'], 'buldan': ['buldan'],\n","            'cal': ['Ã§al', 'cal'], 'cameli': ['Ã§ameli', 'cameli'], 'cardak': ['Ã§ardak', 'cardak'],\n","            'civril': ['Ã§ivril', 'civril'], 'guney': ['gÃ¼ney', 'guney'], 'honaz': ['honaz'],\n","            'kale': ['kale'], 'merkezefendi': ['merkezefendi'], 'pamukkale': ['pamukkale'],\n","            'saraykoy': ['saraykÃ¶y', 'saraykoy'], 'serinhisar': ['serinhisar'], 'tavas': ['tavas'],\n","\n","            # MuÄŸla districts\n","            'bodrum': ['bodrum'], 'dalaman': ['dalaman'], 'datca': ['datÃ§a', 'datca'], 'fethiye': ['fethiye'],\n","            'kavaklidere': ['kavaklÄ±dere', 'kavaklidere'], 'koycegiz': ['kÃ¶yceÄŸiz', 'koycegiz'],\n","            'marmaris': ['marmaris'], 'mentese': ['menteÅŸe', 'mentese'], 'milas': ['milas'],\n","            'ortaca': ['ortaca'], 'seydikemer': ['seydikemer'], 'ula': ['ula'], 'yatagan': ['yataÄŸan', 'yatagan'],\n","\n","            # AydÄ±n districts\n","            'bozdogan': ['bozdoÄŸan', 'bozdogan'], 'buharkent': ['buharkent'], 'cine': ['Ã§ine', 'cine'],\n","            'didim': ['didim'], 'efeler': ['efeler'], 'germencik': ['germencik'], 'incirliova': ['incirliova'],\n","            'karacasu': ['karacasu'], 'karpuzlu': ['karpuzlu'], 'kocarli': ['koÃ§arlÄ±', 'kocarli'],\n","            'kusadasi': ['kuÅŸadasÄ±', 'kusadasi'], 'kuyucak': ['kuyucak'], 'nazilli': ['nazilli'],\n","            'soke': ['sÃ¶ke', 'soke'], 'sultanhisar': ['sultanhisar'], 'yenipazar': ['yenipazar'],\n","\n","            # Istanbul major districts\n","            'adalar': ['adalar'], 'atasehir': ['ataÅŸehir', 'atasehir'], 'avcilar': ['avcÄ±lar', 'avcilar'],\n","            'bagcilar': ['baÄŸcÄ±lar', 'bagcilar'], 'bahcelievler': ['bahÃ§elievler', 'bahcelievler'],\n","            'bakirkoy': ['bakÄ±rkÃ¶y', 'bakirkoy'], 'besiktas': ['beÅŸiktaÅŸ', 'besiktas'], 'beykoz': ['beykoz'],\n","            'beylikduzu': ['beylikdÃ¼zÃ¼', 'beylikduzu'], 'beyoglu': ['beyoÄŸlu', 'beyoglu'],\n","            'buyukcekmece': ['bÃ¼yÃ¼kÃ§ekmece', 'buyukcekmece'], 'fatih': ['fatih'],\n","            'kadikoy': ['kadÄ±kÃ¶y', 'kadikoy'], 'kartal': ['kartal'], 'maltepe': ['maltepe'],\n","            'pendik': ['pendik'], 'sisli': ['ÅŸiÅŸli', 'sisli'], 'uskudar': ['Ã¼skÃ¼dar', 'uskudar'],\n","\n","            # Ankara districts\n","            'altindag': ['altÄ±ndaÄŸ', 'altindag'], 'cankaya': ['Ã§ankaya', 'cankaya'],\n","            'etimesgut': ['etimesgut'], 'golbasi': ['gÃ¶lbaÅŸÄ±', 'golbasi'], 'kecioren': ['keÃ§iÃ¶ren', 'kecioren'],\n","            'mamak': ['mamak'], 'polatli': ['polatlÄ±', 'polatli'], 'pursaklar': ['pursaklar'],\n","            'sincan': ['sincan'], 'yenimahalle': ['yenimahalle']\n","        }\n","\n","    def find_province_in_address(self, address):\n","        \"\"\"Find province in address\"\"\"\n","        if pd.isna(address):\n","            return None\n","        address_lower = str(address).lower()\n","\n","        for province, variants in self.provinces.items():\n","            for variant in variants:\n","                if variant.lower() in address_lower:\n","                    return province\n","        return None\n","\n","    def find_district_in_address(self, address):\n","        \"\"\"Find district in address\"\"\"\n","        if pd.isna(address):\n","            return None\n","        address_lower = str(address).lower()\n","\n","        for district, variants in self.districts.items():\n","            for variant in variants:\n","                if variant.lower() in address_lower:\n","                    return district\n","        return None"],"metadata":{"id":"k_THSpzkgvDm","executionInfo":{"status":"ok","timestamp":1755861946612,"user_tz":-180,"elapsed":44,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ========================================================================\n","# STEP 4: ADVANCED ADDRESS RESOLVER\n","# ========================================================================\n","\n","class AdvancedAddressResolver:\n","    \"\"\"Advanced multi-stage address resolution system\"\"\"\n","\n","    def __init__(self):\n","        # Core components\n","        self.location_db = TurkeyLocationDatabase()\n","        self.address_normalizer = AddressNormalizer()\n","        self.feature_extractor = FeatureExtractor()\n","        self.ensemble_predictor = EnsemblePredictor()\n","        self.post_processor = PostProcessor()\n","\n","        # Vectorizers\n","        self.tfidf_vectorizer = TfidfVectorizer(\n","            max_features=4000,\n","            ngram_range=(1, 4),\n","            analyzer='char_wb',\n","            lowercase=True,\n","            min_df=2,\n","            max_df=0.95\n","        )\n","\n","        self.svd = TruncatedSVD(n_components=150, random_state=42)\n","        self.scaler = StandardScaler()\n","\n","        # Storage\n","        self.address_lookup = {}\n","        self.label_centroids = {}\n","\n","    def fit(self, addresses, labels):\n","        \"\"\"Complete training pipeline\"\"\"\n","        print(\"ğŸš€ Advanced Address Resolution Training Started...\")\n","        print(f\"ğŸ“Š Training data: {len(addresses)} addresses, {len(set(labels))} unique labels\")\n","\n","        # Stage 1: Address normalization\n","        print(\"1ï¸âƒ£ Normalizing addresses...\")\n","        normalized_addresses = [self.address_normalizer.normalize(addr) for addr in addresses]\n","\n","        # Stage 2: Feature extraction\n","        print(\"2ï¸âƒ£ Extracting comprehensive features...\")\n","        features = self._extract_all_features(normalized_addresses, labels, fit=True)\n","\n","        # Stage 3: Build lookup tables\n","        print(\"3ï¸âƒ£ Building lookup tables...\")\n","        self._build_lookup_tables(normalized_addresses, labels)\n","\n","        # Stage 4: Train ensemble\n","        print(\"4ï¸âƒ£ Training ensemble models...\")\n","        self.ensemble_predictor.fit(features, labels)\n","\n","        # Stage 5: Configure post-processor\n","        print(\"5ï¸âƒ£ Configuring post-processor...\")\n","        self.post_processor.configure(normalized_addresses, labels)\n","\n","        print(\"âœ… Training completed successfully!\")\n","        return self\n","\n","    def predict(self, addresses):\n","        \"\"\"Complete prediction pipeline\"\"\"\n","        print(\"ğŸ”® Advanced Prediction Started...\")\n","\n","        # Normalize addresses\n","        normalized_addresses = [self.address_normalizer.normalize(addr) for addr in addresses]\n","\n","        # Extract features\n","        features = self._extract_all_features(normalized_addresses, None, fit=False)\n","\n","        # Ensemble prediction\n","        raw_predictions = self.ensemble_predictor.predict(features)\n","\n","        # Post-processing\n","        final_predictions = self.post_processor.refine_predictions(\n","            normalized_addresses, raw_predictions\n","        )\n","\n","        print(\"âœ… Prediction completed successfully!\")\n","        return final_predictions\n","\n","    def _extract_all_features(self, addresses, labels, fit=False):\n","        \"\"\"Extract comprehensive feature set\"\"\"\n","\n","        # Text features (TF-IDF + SVD)\n","        if fit:\n","            tfidf_features = self.tfidf_vectorizer.fit_transform(addresses)\n","            text_features = self.svd.fit_transform(tfidf_features)\n","        else:\n","            tfidf_features = self.tfidf_vectorizer.transform(addresses)\n","            text_features = self.svd.transform(tfidf_features)\n","\n","        # Geographic features\n","        geo_features = []\n","        for addr in addresses:\n","            geo_feat = self.feature_extractor.extract_geographic_features(addr, self.location_db)\n","            geo_features.append(geo_feat)\n","        geo_features = np.array(geo_features)\n","\n","        # Structural features\n","        struct_features = []\n","        for addr in addresses:\n","            struct_feat = self.feature_extractor.extract_structural_features(addr)\n","            struct_features.append(struct_feat)\n","        struct_features = np.array(struct_features)\n","\n","        # Semantic features (similarity to label centroids)\n","        if fit and labels is not None:\n","            self._build_label_centroids(text_features, labels)\n","\n","        semantic_features = []\n","        for text_feat in text_features:\n","            sem_feat = self._get_semantic_similarities(text_feat)\n","            semantic_features.append(sem_feat)\n","        semantic_features = np.array(semantic_features)\n","\n","        # Combine all features\n","        all_features = np.hstack([text_features, geo_features, struct_features, semantic_features])\n","\n","        # Scale features\n","        if fit:\n","            all_features = self.scaler.fit_transform(all_features)\n","        else:\n","            all_features = self.scaler.transform(all_features)\n","\n","        return all_features\n","\n","    def _build_label_centroids(self, text_features, labels):\n","        \"\"\"Build centroids for each label\"\"\"\n","        unique_labels = np.unique(labels)\n","        for label in unique_labels:\n","            mask = np.array(labels) == label\n","            if np.any(mask):\n","                centroid = np.mean(text_features[mask], axis=0)\n","                self.label_centroids[label] = centroid\n","\n","    def _get_semantic_similarities(self, text_feature):\n","        \"\"\"Get similarities to label centroids\"\"\"\n","        if not self.label_centroids:\n","            return np.zeros(20)  # Placeholder\n","\n","        similarities = []\n","        for label, centroid in self.label_centroids.items():\n","            similarity = np.dot(text_feature, centroid) / (\n","                np.linalg.norm(text_feature) * np.linalg.norm(centroid) + 1e-8\n","            )\n","            similarities.append(max(0, similarity))\n","\n","        similarities.sort(reverse=True)\n","        return np.array(similarities[:20])  # Top 20 similarities\n","\n","    def _build_lookup_tables(self, addresses, labels):\n","        \"\"\"Build address lookup tables\"\"\"\n","        for addr, label in zip(addresses, labels):\n","            clean_addr = self._clean_for_lookup(addr)\n","            if clean_addr not in self.address_lookup:\n","                self.address_lookup[clean_addr] = []\n","            self.address_lookup[clean_addr].append(label)\n","\n","        # Keep most common label for each address\n","        for addr in self.address_lookup:\n","            label_counts = Counter(self.address_lookup[addr])\n","            self.address_lookup[addr] = label_counts.most_common(1)[0][0]\n","\n","    def _clean_for_lookup(self, address):\n","        \"\"\"Clean address for lookup\"\"\"\n","        clean_addr = re.sub(r'[^\\w\\s]', '', str(address).lower())\n","        return re.sub(r'\\s+', ' ', clean_addr).strip()\n","\n","\n","class AddressNormalizer:\n","    \"\"\"Advanced address normalization\"\"\"\n","\n","    def normalize(self, address):\n","        \"\"\"Comprehensive address normalization\"\"\"\n","        if pd.isna(address):\n","            return \"\"\n","\n","        address = str(address).lower()\n","\n","        # Remove excessive punctuation\n","        address = re.sub(r'[^\\w\\s/\\-\\.]', ' ', address)\n","\n","        # Normalize whitespace\n","        address = re.sub(r'\\s+', ' ', address)\n","\n","        # Expand common abbreviations\n","        abbreviations = {\n","            r'\\bmah\\b': 'mahalle', r'\\bmah\\.\\b': 'mahalle', r'\\bmahallesi\\b': 'mahalle',\n","            r'\\bsok\\b': 'sokak', r'\\bsok\\.\\b': 'sokak', r'\\bsokaÄŸÄ±\\b': 'sokak',\n","            r'\\bcd\\b': 'cadde', r'\\bcd\\.\\b': 'cadde', r'\\bcaddesi\\b': 'cadde',\n","            r'\\bapt\\b': 'apartman', r'\\bapt\\.\\b': 'apartman', r'\\bapartmanÄ±\\b': 'apartman',\n","            r'\\bno\\b': 'numara', r'\\bno\\.\\b': 'numara',\n","            r'\\bd\\b': 'daire', r'\\bd\\.\\b': 'daire',\n","            r'\\bk\\b': 'kat', r'\\bk\\.\\b': 'kat',\n","            r'\\bblv\\b': 'bulvar', r'\\bblv\\.\\b': 'bulvar', r'\\bbulvarÄ±\\b': 'bulvar',\n","            r'\\bmh\\b': 'mahalle', r'\\bmh\\.\\b': 'mahalle',\n","            r'\\bsit\\b': 'sitesi', r'\\bsit\\.\\b': 'sitesi'\n","        }\n","\n","        for abbr, full in abbreviations.items():\n","            address = re.sub(abbr, full, address, flags=re.IGNORECASE)\n","\n","        # Remove redundant punctuation\n","        address = re.sub(r'[/\\-\\.]', ' ', address)\n","        address = re.sub(r'\\s+', ' ', address)\n","\n","        return address.strip()\n","\n","\n","class FeatureExtractor:\n","    \"\"\"Comprehensive feature extraction\"\"\"\n","\n","    def extract_geographic_features(self, address, location_db):\n","        \"\"\"Extract geographic features\"\"\"\n","        features = []\n","\n","        # Province detection (one-hot for top 25 provinces)\n","        top_provinces = ['istanbul', 'ankara', 'izmir', 'bursa', 'antalya', 'adana', 'konya',\n","                        'gaziantep', 'manisa', 'denizli', 'mugla', 'aydin', 'kocaeli', 'mersin',\n","                        'samsun', 'kayseri', 'balikesir', 'hatay', 'trabzon', 'erzurum',\n","                        'diyarbakir', 'sanliurfa', 'malatya', 'tekirdaÄŸ', 'sakarya']\n","\n","        found_province = location_db.find_province_in_address(address)\n","        for province in top_provinces:\n","            features.append(1 if found_province == province else 0)\n","\n","        # District detection (one-hot for top 30 districts)\n","        top_districts = ['bornova', 'konak', 'karsiyaka', 'bayrakli', 'buca', 'ciglli', 'gaziemir',\n","                        'pamukkale', 'merkezefendi', 'fethiye', 'bodrum', 'marmaris', 'datca',\n","                        'efeler', 'nazilli', 'kusadasi', 'yunusemre', 'sehzadeler', 'akhisar',\n","                        'turgutlu', 'salihli', 'cankaya', 'altindag', 'kecioren', 'besiktas',\n","                        'kadikoy', 'sisli', 'fatih', 'beyoglu', 'uskudar']\n","\n","        found_district = location_db.find_district_in_address(address)\n","        for district in top_districts:\n","            features.append(1 if found_district == district else 0)\n","\n","        # Geographic completeness\n","        features.extend([\n","            1 if found_province else 0,\n","            1 if found_district else 0,\n","            1 if found_province and found_district else 0\n","        ])\n","\n","        return features\n","\n","    def extract_structural_features(self, address):\n","        \"\"\"Extract structural features\"\"\"\n","        # Basic metrics\n","        length = len(address)\n","        word_count = len(address.split())\n","\n","        # Component detection\n","        has_mahalle = bool(re.search(r'\\bmahalle\\b', address))\n","        has_sokak = bool(re.search(r'\\bsokak\\b', address))\n","        has_cadde = bool(re.search(r'\\bcadde\\b', address))\n","        has_bulvar = bool(re.search(r'\\bbulvar\\b', address))\n","        has_numara = bool(re.search(r'\\bnumara\\b', address))\n","        has_daire = bool(re.search(r'\\bdaire\\b', address))\n","        has_kat = bool(re.search(r'\\bkat\\b', address))\n","        has_apartman = bool(re.search(r'\\bapartman\\b', address))\n","        has_sitesi = bool(re.search(r'\\bsitesi\\b', address))\n","        has_blok = bool(re.search(r'\\bblok\\b', address))\n","\n","        # Number analysis\n","        numbers = re.findall(r'\\d+', address)\n","        number_count = len(numbers)\n","\n","        if numbers:\n","            number_values = [int(n) for n in numbers if len(n) <= 6]  # Avoid very large numbers\n","            avg_number = np.mean(number_values) if number_values else 0\n","            max_number = max(number_values) if number_values else 0\n","            min_number = min(number_values) if number_values else 0\n","        else:\n","            avg_number = max_number = min_number = 0\n","\n","        # Character analysis\n","        slash_count = address.count('/')\n","        dash_count = address.count('-')\n","        dot_count = address.count('.')\n","        comma_count = address.count(',')\n","\n","        # Pattern analysis\n","        has_postal_pattern = bool(re.search(r'\\b\\d{5}\\b', address))\n","        has_phone_pattern = bool(re.search(r'\\b\\d{10,11}\\b', address))\n","\n","        # Completeness scores\n","        address_components = [has_mahalle, has_sokak, has_numara, has_daire]\n","        completeness_basic = sum(address_components) / len(address_components)\n","\n","        extended_components = [has_mahalle, has_sokak, has_numara, has_daire, has_apartman]\n","        completeness_extended = sum(extended_components) / len(extended_components)\n","\n","        # Word diversity\n","        words = address.split()\n","        unique_words = set(words)\n","        word_diversity = len(unique_words) / max(len(words), 1)\n","\n","        return [\n","            length, word_count, number_count, avg_number, max_number, min_number,\n","            has_mahalle, has_sokak, has_cadde, has_bulvar, has_numara, has_daire,\n","            has_kat, has_apartman, has_sitesi, has_blok, slash_count, dash_count,\n","            dot_count, comma_count, has_postal_pattern, has_phone_pattern,\n","            completeness_basic, completeness_extended, word_diversity\n","        ]\n","\n","\n","class EnsemblePredictor:\n","    \"\"\"Advanced ensemble of multiple models\"\"\"\n","\n","    def __init__(self):\n","        self.models = {}\n","        self._setup_models()\n","\n","    def _setup_models(self):\n","        \"\"\"Setup available models\"\"\"\n","        # Always available: Random Forest\n","        self.models['rf'] = RandomForestClassifier(\n","            n_estimators=200,\n","            max_depth=20,\n","            min_samples_split=5,\n","            min_samples_leaf=2,\n","            random_state=42,\n","            n_jobs=-1\n","        )\n","\n","        # XGBoost if available\n","        if XGBOOST_AVAILABLE:\n","            self.models['xgb'] = XGBClassifier(\n","                n_estimators=300,\n","                max_depth=12,\n","                learning_rate=0.03,\n","                subsample=0.8,\n","                colsample_bytree=0.8,\n","                random_state=42,\n","                eval_metric='mlogloss',\n","                verbosity=0\n","            )\n","\n","        # LightGBM if available\n","        if LIGHTGBM_AVAILABLE:\n","            self.models['lgb'] = lgb.LGBMClassifier(\n","                n_estimators=300,\n","                max_depth=12,\n","                learning_rate=0.03,\n","                feature_fraction=0.8,\n","                bagging_fraction=0.8,\n","                random_state=42,\n","                verbose=-1\n","            )\n","\n","        # CatBoost if available\n","        if CATBOOST_AVAILABLE:\n","            self.models['catboost'] = CatBoostClassifier(\n","                iterations=200,\n","                depth=10,\n","                learning_rate=0.03,\n","                random_state=42,\n","                verbose=False\n","            )\n","\n","        print(f\"âœ… Ensemble models initialized: {list(self.models.keys())}\")\n","\n","    def fit(self, X, y):\n","        \"\"\"Train all available models\"\"\"\n","        for name, model in self.models.items():\n","            print(f\"   Training {name}...\")\n","            model.fit(X, y)\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"Ensemble prediction with weighted voting\"\"\"\n","        predictions = {}\n","\n","        # Get predictions from all models\n","        for name, model in self.models.items():\n","            predictions[name] = model.predict(X)\n","\n","        # Weighted voting based on model performance expectations\n","        weights = {\n","            'rf': 0.2,\n","            'xgb': 0.4 if 'xgb' in predictions else 0,\n","            'lgb': 0.3 if 'lgb' in predictions else 0,\n","            'catboost': 0.1 if 'catboost' in predictions else 0\n","        }\n","\n","        # Normalize weights for available models\n","        available_weights = {k: v for k, v in weights.items() if k in predictions}\n","        total_weight = sum(available_weights.values())\n","        normalized_weights = {k: v/total_weight for k, v in available_weights.items()}\n","\n","        # Ensemble voting\n","        final_predictions = []\n","        for i in range(len(X)):\n","            vote_counts = defaultdict(float)\n","\n","            for model_name, preds in predictions.items():\n","                vote_counts[preds[i]] += normalized_weights[model_name]\n","\n","            # Select prediction with highest weight\n","            best_prediction = max(vote_counts.items(), key=lambda x: x[1])[0]\n","            final_predictions.append(best_prediction)\n","\n","        return np.array(final_predictions)\n","\n","\n","class PostProcessor:\n","    \"\"\"Advanced post-processing with similarity matching\"\"\"\n","\n","    def __init__(self):\n","        self.address_lookup = {}\n","        self.similarity_threshold = 0.82\n","\n","    def configure(self, addresses, labels):\n","        \"\"\"Configure with training data\"\"\"\n","        for addr, label in zip(addresses, labels):\n","            clean_addr = self._normalize_for_lookup(addr)\n","            if clean_addr not in self.address_lookup:\n","                self.address_lookup[clean_addr] = []\n","            self.address_lookup[clean_addr].append(label)\n","\n","        # Keep most common label for each address\n","        for addr in self.address_lookup:\n","            label_counts = Counter(self.address_lookup[addr])\n","            self.address_lookup[addr] = label_counts.most_common(1)[0][0]\n","\n","    def refine_predictions(self, addresses, predictions):\n","        \"\"\"Refine predictions with lookup and similarity\"\"\"\n","        refined = predictions.copy()\n","\n","        for i, addr in enumerate(addresses):\n","            clean_addr = self._normalize_for_lookup(addr)\n","\n","            # Exact lookup first\n","            if clean_addr in self.address_lookup:\n","                refined[i] = self.address_lookup[clean_addr]\n","                continue\n","\n","            # Similarity matching\n","            if FUZZYWUZZY_AVAILABLE:\n","                match = self._fuzzy_match(clean_addr)\n","                if match:\n","                    refined[i] = match\n","            else:\n","                match = self._simple_similarity_match(clean_addr)\n","                if match:\n","                    refined[i] = match\n","\n","        return refined\n","\n","    def _normalize_for_lookup(self, address):\n","        \"\"\"Normalize address for lookup\"\"\"\n","        normalized = re.sub(r'[^\\w\\s]', '', str(address).lower())\n","        return re.sub(r'\\s+', ' ', normalized).strip()\n","\n","    def _fuzzy_match(self, address):\n","        \"\"\"Fuzzy matching with fuzzywuzzy\"\"\"\n","        best_score = 0\n","        best_label = None\n","\n","        # Limit search for performance (sample from lookup)\n","        lookup_items = list(self.address_lookup.items())\n","        if len(lookup_items) > 2000:\n","            np.random.seed(42)\n","            lookup_items = np.random.choice(lookup_items, 2000, replace=False)\n","\n","        for lookup_addr, label in lookup_items:\n","            score = fuzz.ratio(address, lookup_addr) / 100.0\n","            if score > best_score and score > self.similarity_threshold:\n","                best_score = score\n","                best_label = label\n","\n","        return best_label\n","\n","    def _simple_similarity_match(self, address):\n","        \"\"\"Simple similarity without fuzzywuzzy\"\"\"\n","        best_score = 0\n","        best_label = None\n","\n","        address_words = set(address.split())\n","        if not address_words:\n","            return None\n","\n","        # Limit search for performance\n","        lookup_items = list(self.address_lookup.items())[:1500]\n","\n","        for lookup_addr, label in lookup_items:\n","            lookup_words = set(lookup_addr.split())\n","            if not lookup_words:\n","                continue\n","\n","            # Jaccard similarity\n","            intersection = len(address_words & lookup_words)\n","            union = len(address_words | lookup_words)\n","\n","            if union > 0:\n","                similarity = intersection / union\n","                if similarity > best_score and similarity > 0.7:\n","                    best_score = similarity\n","                    best_label = label\n","\n","        return best_label"],"metadata":{"id":"8jpWhf2Pg1an","executionInfo":{"status":"ok","timestamp":1755861950305,"user_tz":-180,"elapsed":128,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ========================================================================\n","# STEP 5: SUBMISSION CREATION AND ANALYSIS\n","# ========================================================================\n","\n","def create_submission_with_analysis(test_df, predictions):\n","    \"\"\"Create submission file with comprehensive analysis\"\"\"\n","\n","    # Create submission DataFrame\n","    submission = pd.DataFrame({\n","        'id': test_df['id'],\n","        'label': predictions\n","    })\n","\n","    # Comprehensive analysis\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ğŸ“Š COMPREHENSIVE SUBMISSION ANALYSIS\")\n","    print(\"=\"*60)\n","\n","    # Basic format validation\n","    print(\"\\nğŸ” FORMAT VALIDATION:\")\n","    print(f\"Shape: {submission.shape}\")\n","    print(f\"Columns: {submission.columns.tolist()}\")\n","    print(f\"ID range: {submission['id'].min()}-{submission['id'].max()}\")\n","    print(f\"Label range: {submission['label'].min()}-{submission['label'].max()}\")\n","    print(f\"Null values: {submission.isnull().sum().sum()}\")\n","\n","    # Quality metrics\n","    pred_counts = submission['label'].value_counts()\n","    unique_preds = len(pred_counts)\n","    top_pred_count = pred_counts.iloc[0]\n","    top_pred_pct = (top_pred_count / len(submission)) * 100\n","    singletons = (pred_counts == 1).sum()\n","\n","    # Shannon entropy\n","    probs = pred_counts / len(submission)\n","    entropy = -np.sum(probs * np.log2(probs + 1e-10))\n","\n","    # Gini coefficient\n","    sorted_counts = np.sort(pred_counts.values)\n","    n = len(sorted_counts)\n","    cumsum = np.cumsum(sorted_counts)\n","    gini = (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n","\n","    print(f\"\\nğŸ“ˆ QUALITY METRICS:\")\n","    print(f\"Unique predictions: {unique_preds:,}\")\n","    print(f\"Top prediction: {top_pred_count:,} times ({top_pred_pct:.2f}%)\")\n","    print(f\"Singleton predictions: {singletons:,} ({singletons/unique_preds*100:.1f}%)\")\n","    print(f\"Shannon entropy: {entropy:.4f}\")\n","    print(f\"Gini coefficient: {gini:.4f}\")\n","\n","    # Target achievement\n","    print(f\"\\nğŸ¯ TARGET ACHIEVEMENT:\")\n","    entropy_target = entropy > 12.0\n","    concentration_target = top_pred_pct < 2.0\n","    coverage_target = unique_preds > 7000\n","    diversity_target = singletons > 2000\n","\n","    print(f\"High Entropy (>12.0): {'âœ…' if entropy_target else 'âŒ'} ({entropy:.2f})\")\n","    print(f\"Low Concentration (<2%): {'âœ…' if concentration_target else 'âŒ'} ({top_pred_pct:.2f}%)\")\n","    print(f\"High Coverage (>7K): {'âœ…' if coverage_target else 'âŒ'} ({unique_preds:,})\")\n","    print(f\"High Diversity (>2K singletons): {'âœ…' if diversity_target else 'âŒ'} ({singletons:,})\")\n","\n","    # Overall assessment\n","    targets_met = sum([entropy_target, concentration_target, coverage_target, diversity_target])\n","    print(f\"\\nTargets met: {targets_met}/4\")\n","\n","    if targets_met >= 3:\n","        expected_score = \"0.75-0.85\"\n","        status = \"ğŸ‰ EXCELLENT\"\n","    elif targets_met >= 2:\n","        expected_score = \"0.60-0.75\"\n","        status = \"âœ… GOOD\"\n","    else:\n","        expected_score = \"0.40-0.60\"\n","        status = \"âš ï¸ NEEDS IMPROVEMENT\"\n","\n","    print(f\"Status: {status}\")\n","    print(f\"Expected Score Range: {expected_score}\")\n","\n","    # Top predictions analysis\n","    print(f\"\\nğŸ“‹ TOP 10 PREDICTIONS:\")\n","    for i, (label, count) in enumerate(pred_counts.head(10).items(), 1):\n","        print(f\"  {i:2d}. Label {label}: {count:,} times ({count/len(submission)*100:.2f}%)\")\n","\n","    # Distribution analysis\n","    print(f\"\\nğŸ“Š DISTRIBUTION ANALYSIS:\")\n","    ranges = [(1, 1), (2, 5), (6, 20), (21, 100), (101, float('inf'))]\n","    for min_count, max_count in ranges:\n","        if max_count == float('inf'):\n","            mask = pred_counts >= min_count\n","            range_desc = f\"{min_count}+\"\n","        else:\n","            mask = (pred_counts >= min_count) & (pred_counts <= max_count)\n","            range_desc = f\"{min_count}-{max_count}\"\n","\n","        count_in_range = mask.sum()\n","        pct_in_range = count_in_range / len(pred_counts) * 100\n","        print(f\"  Labels appearing {range_desc} times: {count_in_range:,} ({pct_in_range:.1f}%)\")\n","\n","    return submission\n","\n","\n","def validate_and_save_submission(submission, filename='advanced_submission.csv'):\n","    \"\"\"Validate and save submission with final checks\"\"\"\n","\n","    print(f\"\\nğŸ’¾ SAVING SUBMISSION:\")\n","\n","    # Final validation\n","    expected_rows = 217241\n","    validation_passed = True\n","\n","    checks = [\n","        (len(submission) == expected_rows, f\"Row count ({expected_rows})\"),\n","        (set(submission.columns) == {'id', 'label'}, \"Correct columns\"),\n","        (submission['id'].min() == 0, \"ID starts at 0\"),\n","        (submission['id'].max() == expected_rows - 1, f\"ID ends at {expected_rows-1}\"),\n","        (submission.isnull().sum().sum() == 0, \"No null values\"),\n","        (submission['label'].dtype in ['int64', 'int32'], \"Integer labels\"),\n","        (len(set(submission['id'])) == expected_rows, \"All IDs unique\")\n","    ]\n","\n","    for check_passed, check_desc in checks:\n","        status = \"âœ…\" if check_passed else \"âŒ\"\n","        print(f\"  {check_desc}: {status}\")\n","        if not check_passed:\n","            validation_passed = False\n","\n","    if validation_passed:\n","        submission.to_csv(filename, index=False)\n","        print(f\"\\nğŸ‰ SUBMISSION SUCCESSFULLY SAVED: {filename}\")\n","\n","        # File size info\n","        import os\n","        file_size = os.path.getsize(filename) / 1024 / 1024\n","        print(f\"ğŸ“ File size: {file_size:.2f} MB\")\n","\n","        return True\n","    else:\n","        print(f\"\\nâŒ VALIDATION FAILED - SUBMISSION NOT SAVED\")\n","        return False\n"],"metadata":{"id":"ydBhf9E-g49n","executionInfo":{"status":"ok","timestamp":1755861958954,"user_tz":-180,"elapsed":59,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ========================================================================\n","# STEP 6: MAIN EXECUTION FUNCTION\n","# ========================================================================\n","\n","def run_complete_solution():\n","    \"\"\"Run the complete address resolution solution\"\"\"\n","\n","    print(\"ğŸš€ TEKNOFEST 2025 - ADVANCED ADDRESS RESOLUTION\")\n","    print(\"=\" * 60)\n","    print(\"Target: 0.80+ Score\")\n","    print(\"=\" * 60)\n","\n","    # Step 1: Load data\n","    print(\"\\nğŸ“ LOADING DATA...\")\n","    try:\n","        # Try to load from files\n","        train_df = pd.read_csv('train.csv')\n","        test_df = pd.read_csv('test.csv')\n","        print(f\"âœ… Data loaded successfully!\")\n","        print(f\"   Train: {len(train_df):,} samples\")\n","        print(f\"   Test: {len(test_df):,} samples\")\n","        print(f\"   Unique labels in train: {train_df['label'].nunique():,}\")\n","\n","    except FileNotFoundError:\n","        print(\"âŒ CSV files not found!\")\n","        print(\"Please upload train.csv and test.csv files to Colab\")\n","\n","        # Try Colab file upload\n","        try:\n","            from google.colab import files\n","            print(\"\\nğŸ“¤ Please upload your files:\")\n","\n","            print(\"1. Upload train.csv:\")\n","            uploaded_train = files.upload()\n","            train_df = pd.read_csv(list(uploaded_train.keys())[0])\n","\n","            print(\"2. Upload test.csv:\")\n","            uploaded_test = files.upload()\n","            test_df = pd.read_csv(list(uploaded_test.keys())[0])\n","\n","            print(f\"âœ… Files uploaded successfully!\")\n","            print(f\"   Train: {len(train_df):,} samples\")\n","            print(f\"   Test: {len(test_df):,} samples\")\n","\n","        except ImportError:\n","            print(\"âŒ Not running in Colab and no CSV files found\")\n","            print(\"Please ensure train.csv and test.csv are in the current directory\")\n","            return None\n","\n","    # Step 2: Data sampling for development (optional)\n","    use_sample = len(train_df) > 100000\n","    if use_sample:\n","        print(f\"\\nğŸ”„ USING SAMPLE FOR FASTER DEVELOPMENT:\")\n","        sample_size = min(80000, len(train_df))\n","        train_sample = train_df.sample(n=sample_size, random_state=42, stratify=None)\n","        print(f\"   Using {len(train_sample):,} samples from {len(train_df):,}\")\n","        print(\"   (Remove this sampling for final submission)\")\n","    else:\n","        train_sample = train_df\n","        print(f\"\\nâœ… Using full training set: {len(train_sample):,} samples\")\n","\n","    # Step 3: Initialize resolver\n","    print(f\"\\nğŸ§  INITIALIZING ADVANCED RESOLVER...\")\n","    resolver = AdvancedAddressResolver()\n","\n","    # Step 4: Training\n","    print(f\"\\nğŸ¯ TRAINING PHASE...\")\n","    resolver.fit(train_sample['address'].values, train_sample['label'].values)\n","\n","    # Step 5: Prediction\n","    print(f\"\\nğŸ”® PREDICTION PHASE...\")\n","    predictions = resolver.predict(test_df['address'].values)\n","\n","    # Step 6: Create and analyze submission\n","    print(f\"\\nğŸ“Š CREATING SUBMISSION...\")\n","    submission = create_submission_with_analysis(test_df, predictions)\n","\n","    # Step 7: Save submission\n","    success = validate_and_save_submission(submission)\n","\n","    if success:\n","        # Try to download in Colab\n","        try:\n","            from google.colab import files\n","            files.download('advanced_submission.csv')\n","            print(\"ğŸ“¥ File automatically downloaded!\")\n","        except ImportError:\n","            print(\"ğŸ’¡ File saved in current directory\")\n","\n","        print(f\"\\nğŸ‰ PROCESS COMPLETED SUCCESSFULLY!\")\n","        print(f\"Submission ready for Kaggle upload!\")\n","\n","    return submission\n"],"metadata":{"id":"lqMO29z1g7fe","executionInfo":{"status":"ok","timestamp":1755861962699,"user_tz":-180,"elapsed":26,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":540},"id":"-SrHq9H9gWTq","executionInfo":{"status":"error","timestamp":1755861968156,"user_tz":-180,"elapsed":2037,"user":{"displayName":"Muhammet Fatih Ã‡etintas","userId":"02746250817952515216"}},"outputId":"5dcf878f-95d9-4d8a-8009-9874c67c1fec"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ TEKNOFEST 2025 - ADVANCED ADDRESS RESOLUTION\n","============================================================\n","Target: 0.80+ Score\n","============================================================\n","\n","ğŸ“ LOADING DATA...\n","âœ… Data loaded successfully!\n","   Train: 848,237 samples\n","   Test: 217,241 samples\n","   Unique labels in train: 10,390\n","\n","ğŸ”„ USING SAMPLE FOR FASTER DEVELOPMENT:\n"]},{"output_type":"error","ename":"TypeError","evalue":"NDFrame.sample() got an unexpected keyword argument 'stratify'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1869986871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Run the complete solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_complete_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1534977019.py\u001b[0m in \u001b[0;36mrun_complete_solution\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nğŸ”„ USING SAMPLE FOR FASTER DEVELOPMENT:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtrain_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Using {len(train_sample):,} samples from {len(train_df):,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   (Remove this sampling for final submission)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: NDFrame.sample() got an unexpected keyword argument 'stratify'"]}],"source":["# ========================================================================\n","# STEP 7: EXECUTE SOLUTION\n","# ========================================================================\n","\n","if __name__ == \"__main__\":\n","    # Run the complete solution\n","    submission = run_complete_solution()\n","\n","    if submission is not None:\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"ğŸ FINAL SUMMARY\")\n","        print(\"=\"*60)\n","        print(f\"âœ… Submission created with {len(submission)} predictions\")\n","        print(f\"âœ… {submission['label'].nunique()} unique labels predicted\")\n","        print(f\"âœ… File saved as 'advanced_submission.csv'\")\n","        print(f\"ğŸ¯ Ready for Kaggle submission!\")\n","        print(\"=\"*60)\n","    else:\n","        print(\"\\nâŒ Solution failed to complete\")\n","        print(\"Please check error messages above\")"]}]}