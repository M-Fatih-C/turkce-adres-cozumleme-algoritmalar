{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyN/mWNHPL6CbAoFmMYhL7z2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install numpy pandas scipy scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAKiT4xwTEjv","executionInfo":{"status":"ok","timestamp":1756193731765,"user_tz":-180,"elapsed":12294,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"5088b6cb-c48c-409a-b7f2-19ac5fab8315"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["!pip install faiss-cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"COFbnVHHTZg4","executionInfo":{"status":"ok","timestamp":1756193754160,"user_tz":-180,"elapsed":19514,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"31f69e5e-dcb8-4f0c-97da-50ad0486420d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.12.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CWkGqnxmSwoK","executionInfo":{"status":"ok","timestamp":1756194568823,"user_tz":-180,"elapsed":810640,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"8485d6be-b771-4f7e-d624-ad8ee9d8acbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","                                             address  label\n","0  Akarca Mah. Adnan Menderes Cad. 864.Sok. No:15...   8831\n","1  Cumhuriye Mah. Hükümet Cad. Sivriler İşhanı No...   8810\n","   id                                            address\n","0   0    Menderes mahallesi 1013 sok No.40 Daire.2 Kat.2\n","1   1  250. Sk. No:14 B Blok Kat:5 Daire:14\\n3. Halil...\n","Preprocessing texts (train/test)...\n","Fitting TF‑IDF (char_wb 3‑5, max_features=20000)...\n","Fitting TF‑IDF (word 1‑2, max_features=20000)...\n","Stacking sparse matrices (train)...\n","Fitting TruncatedSVD to 256 dims (LSA)...\n","Computing label mean embeddings...\n","Fitting NearestNeighbors (cosine, brute)...\n","Transforming & searching test in batches of 50000...\n","Processed 50000/217241 test rows...\n","Processed 100000/217241 test rows...\n","Processed 150000/217241 test rows...\n","Processed 200000/217241 test rows...\n","Processed 217241/217241 test rows...\n","Saved: submission.csv\n"]}],"source":["# P0: Fast Address Matching (Batch + SVD + NearestNeighbors)\n","# -----------------------------------------------------------\n","# Colab-friendly, scalable baseline without Python O(N*M) loops.\n","# - Char + Word TF-IDF (sparse)\n","# - hstack -> TruncatedSVD (e.g., 256-D)\n","# - Label mean embeddings\n","# - NearestNeighbors (cosine, brute) for fast batch prediction\n","# - Memory-friendly batching for test transform & search\n","# - Float32 everywhere possible\n","#\n","# Expected speedup: >10-100x over per-address/per-label loops.\n","# -----------------------------------------------------------\n","\n","import os, sys, gc, math, time, warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.sparse import hstack, csr_matrix\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.neighbors import NearestNeighbors\n","\n","# -----------------------------\n","# 0) Utility: lightweight preprocessing\n","# -----------------------------\n","\n","TR_MAP = str.maketrans({\n","    'Ç':'ç','Ğ':'ğ','İ':'i','I':'ı','Ö':'ö','Ş':'ş','Ü':'ü',\n","    '\\u00A0': ' '  # non‑breaking space -> space\n","})\n","\n","PUNCTS = \"\\t\\n\\r\\f\\v!\\\"#$%&'()*+,./:;<=>?@[\\\\]^`{|}~\"  # Turkish letters kept\n","\n","import re\n","MULTISPACE_RE = re.compile(r\"\\s+\")\n","PUNCT_RE = re.compile(\"[\" + re.escape(PUNCTS) + \"]+\")\n","\n","\n","def preprocess_address(text: str) -> str:\n","    if not isinstance(text, str):\n","        return \"\"\n","    t = text.translate(TR_MAP)\n","    t = t.lower()\n","    # normalize punctuation/whitespace but keep numbers and Turkish chars\n","    t = PUNCT_RE.sub(\" \", t)\n","    t = MULTISPACE_RE.sub(\" \", t).strip()\n","    return t\n","\n","# -----------------------------\n","# 1) Load data\n","# -----------------------------\n","\n","# Expected columns:\n","#   train.csv: address, label\n","#   test.csv : id, address  (if no id, we auto-generate)\n","\n","TRAIN_PATH = os.getenv(\"TRAIN_PATH\", \"train.csv\")\n","TEST_PATH  = os.getenv(\"TEST_PATH\",  \"test.csv\")\n","SUBMIT_OUT = os.getenv(\"SUBMIT_OUT\", \"submission.csv\")\n","\n","print(\"Loading data...\")\n","train_df = pd.read_csv(TRAIN_PATH)\n","test_df  = pd.read_csv(TEST_PATH)\n","print(train_df.head(2))\n","print(test_df.head(2))\n","\n","assert \"address\" in train_df.columns and \"label\" in train_df.columns, \"train.csv must have columns: address,label\"\n","assert \"address\" in test_df.columns, \"test.csv must have column: address\"\n","\n","if \"id\" not in test_df.columns:\n","    test_df[\"id\"] = np.arange(len(test_df), dtype=np.int64)\n","\n","# -----------------------------\n","# 2) Preprocess (vectorized)\n","# -----------------------------\n","\n","print(\"Preprocessing texts (train/test)...\")\n","train_texts = train_df[\"address\"].astype(str).map(preprocess_address)\n","test_texts  = test_df[\"address\"].astype(str).map(preprocess_address)\n","\n","# -----------------------------\n","# 3) TF‑IDF (char + word) and SVD dimensionality reduction\n","# -----------------------------\n","\n","# NOTE: Keep sizes moderate to control RAM; tune as needed.\n","CHAR_MAX_FEATS = int(os.getenv(\"CHAR_MAX_FEATS\", 20000))\n","WORD_MAX_FEATS = int(os.getenv(\"WORD_MAX_FEATS\", 20000))\n","SVD_DIM        = int(os.getenv(\"SVD_DIM\", 256))\n","RANDOM_STATE   = 42\n","\n","print(f\"Fitting TF‑IDF (char_wb 3‑5, max_features={CHAR_MAX_FEATS})...\")\n","char_vect = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=CHAR_MAX_FEATS)\n","X_char_train = char_vect.fit_transform(train_texts)\n","\n","print(f\"Fitting TF‑IDF (word 1‑2, max_features={WORD_MAX_FEATS})...\")\n","word_vect = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=WORD_MAX_FEATS)\n","X_word_train = word_vect.fit_transform(train_texts)\n","\n","print(\"Stacking sparse matrices (train)...\")\n","X_train_sparse = hstack([X_char_train, X_word_train]).tocsr()\n","\n","print(f\"Fitting TruncatedSVD to {SVD_DIM} dims (LSA)...\")\n","svd = TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_STATE)\n","X_train_red = svd.fit_transform(X_train_sparse).astype(np.float32)\n","\n","# Free large sparse parts not needed after transform for training\n","X_char_train = X_word_train = X_train_sparse = None; gc.collect()\n","\n","# -----------------------------\n","# 4) Label mean embeddings (float32)\n","# -----------------------------\n","\n","print(\"Computing label mean embeddings...\")\n","labels = train_df[\"label\"].to_numpy()\n","unique_labels, inv = np.unique(labels, return_inverse=True)\n","\n","# Accumulate sums per label index\n","label_count = np.bincount(inv).astype(np.int32)\n","label_sum = np.zeros((len(unique_labels), SVD_DIM), dtype=np.float32)\n","for i in range(X_train_red.shape[0]):\n","    label_sum[inv[i]] += X_train_red[i]\n","\n","label_embeds = (label_sum / np.maximum(label_count[:, None], 1)).astype(np.float32)\n","\n","# -----------------------------\n","# 5) Fit NearestNeighbors (cosine, brute)\n","# -----------------------------\n","\n","print(\"Fitting NearestNeighbors (cosine, brute)...\")\n","nbrs = NearestNeighbors(metric='cosine', algorithm='brute')\n","nbrs.fit(label_embeds)\n","\n","# -----------------------------\n","# 6) Transform TEST in batches and search top‑1\n","# -----------------------------\n","\n","BATCH = int(os.getenv(\"TEST_BATCH\", 50000))  # tune to your RAM\n","K = int(os.getenv(\"TOPK\", 1))  # P0 = 1; can increase later\n","\n","print(f\"Transforming & searching test in batches of {BATCH}...\")\n","ids = test_df[\"id\"].to_numpy()\n","all_pred_idx = np.empty(len(test_df), dtype=np.int32)\n","\n","n = len(test_df)\n","start = 0\n","while start < n:\n","    end = min(start + BATCH, n)\n","    chunk = test_texts.iloc[start:end]\n","\n","    # sparse transform (char + word)\n","    Xc = char_vect.transform(chunk)\n","    Xw = word_vect.transform(chunk)\n","    Xt = hstack([Xc, Xw]).tocsr()\n","    Xc = Xw = None; gc.collect()\n","\n","    # SVD to dense float32\n","    Xt_red = svd.transform(Xt).astype(np.float32)\n","    Xt = None; gc.collect()\n","\n","    # cosine kneighbors (returns distance; smaller is better)\n","    dist, idx = nbrs.kneighbors(Xt_red, n_neighbors=K, return_distance=True)\n","    all_pred_idx[start:end] = idx[:,0].astype(np.int32)\n","\n","    print(f\"Processed {end}/{n} test rows...\")\n","    start = end\n","\n","predicted_labels = unique_labels[all_pred_idx]\n","\n","# -----------------------------\n","# 7) Save submission\n","# -----------------------------\n","\n","sub = pd.DataFrame({\"id\": ids, \"label\": predicted_labels})\n","sub.to_csv(SUBMIT_OUT, index=False)\n","print(f\"Saved: {SUBMIT_OUT}\")\n","\n","# -----------------------------\n","# 8) Notes / Tuning\n","# -----------------------------\n","# - Increase SVD_DIM (e.g., 384) if RAM allows; improves recall a bit.\n","# - If train is very large, you can SVD-fit on a stratified sample and then transform all.\n","# - For even faster search, swap NearestNeighbors with FAISS (IndexFlatIP on normalized vectors)\n","#   after L2-normalizing embeddings; cosine ~ dot on normalized vectors.\n","# - For better accuracy later (P1), do city/district blocking before kneighbors and rerank top‑K.\n"]},{"cell_type":"code","source":["# P0‑B: Instance‑level kNN (Char‑only option, SVD, L2‑normalize)\n","# -----------------------------------------------------------------\n","# Colab‑ready fast baseline that replaces label‑mean with instance kNN.\n","# Key ideas:\n","#  - TF‑IDF (char n‑grams; optional word n‑grams)\n","#  - hstack -> TruncatedSVD (e.g., 256–384D)\n","#  - L2 normalize embeddings (cosine stability)\n","#  - kNN over TRAIN INSTANCES (labels taken from the nearest train rows)\n","#  - Optional: exact‑match shortcut; optional: small K majority vote\n","#  - Streaming batches for TEST transform\n","#\n","# Env toggles (override via os.environ or Colab UI):\n","#   USE_WORD_NGRAMS=0/1 (default 0)\n","#   CHAR_MAX_FEATS=40000  WORD_MAX_FEATS=20000\n","#   CHAR_NGRAM_MIN=3  CHAR_NGRAM_MAX=6\n","#   SVD_DIM=384  TEST_BATCH=50000\n","#   K_INST=3      # k neighbors for instance search (majority vote)\n","#   USE_LABEL_MEAN=0/1 (default 0)  # optionally also build label‑mean index\n","#   K_LABEL=25    # top‑K for label‑mean (if used)\n","#   TOPK_RERANK=0 # >1 enables tiny reranker for label‑mean mode\n","#\n","# Expected input files:\n","#   train.csv => columns: address, label\n","#   test.csv  => columns: id(optional), address\n","# Output:\n","#   submission.csv => columns: id, label\n","# -----------------------------------------------------------------\n","\n","import os, gc, re, warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.sparse import hstack\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.preprocessing import normalize\n","from sklearn.neighbors import NearestNeighbors\n","\n","# -----------------------------\n","# 0) Lightweight preprocessing\n","# -----------------------------\n","TR_MAP = str.maketrans({\n","    'Ç':'ç','Ğ':'ğ','İ':'i','I':'ı','Ö':'ö','Ş':'ş','Ü':'ü',\n","    '\\u00A0': ' '\n","})\n","PUNCTS = \"\\t\\n\\r\\f\\v!\\\"#$%&'()*+,./:;<=>?@[\\\\]^`{|}~\"\n","MULTISPACE_RE = re.compile(r\"\\s+\")\n","PUNCT_RE = re.compile(\"[\" + re.escape(PUNCTS) + \"]+\")\n","\n","def preprocess_address(text: str) -> str:\n","    if not isinstance(text, str):\n","        return \"\"\n","    t = text.translate(TR_MAP).lower()\n","    t = PUNCT_RE.sub(\" \", t)\n","    t = MULTISPACE_RE.sub(\" \", t).strip()\n","    return t\n","\n","# -----------------------------\n","# 1) IO\n","# -----------------------------\n","TRAIN_PATH = os.getenv(\"TRAIN_PATH\", \"train.csv\")\n","TEST_PATH  = os.getenv(\"TEST_PATH\",  \"test.csv\")\n","SUBMIT_OUT = os.getenv(\"SUBMIT_OUT\", \"submission.csv\")\n","\n","print(\"Loading data...\")\n","train_df = pd.read_csv(TRAIN_PATH)\n","test_df  = pd.read_csv(TEST_PATH)\n","assert {\"address\",\"label\"}.issubset(train_df.columns)\n","assert \"address\" in test_df.columns\n","if \"id\" not in test_df.columns:\n","    test_df[\"id\"] = np.arange(len(test_df), dtype=np.int64)\n","print(train_df.head(2))\n","print(test_df.head(2))\n","\n","# -----------------------------\n","# 2) Preprocess\n","# -----------------------------\n","print(\"Preprocessing texts...\")\n","train_texts = train_df[\"address\"].astype(str).map(preprocess_address)\n","test_texts  = test_df[\"address\"].astype(str).map(preprocess_address)\n","labels = train_df[\"label\"].to_numpy()\n","\n","# exact‑match dict (cheap quick win)\n","from collections import defaultdict, Counter\n","print(\"Building exact‑match dictionary from train...\")\n","_addr_to_labels = defaultdict(Counter)\n","for a, y in zip(train_texts, labels):\n","    _addr_to_labels[a][y] += 1\n","EXACT_MAP = {a: cnt.most_common(1)[0][0] for a, cnt in _addr_to_labels.items()}\n","print(f\"Exact dictionary size: {len(EXACT_MAP):,}\")\n","\n","# -----------------------------\n","# 3) Vectorizers & SVD\n","# -----------------------------\n","USE_WORD_NGRAMS = int(os.getenv(\"USE_WORD_NGRAMS\", 0))\n","CHAR_MAX_FEATS  = int(os.getenv(\"CHAR_MAX_FEATS\", 40000))\n","WORD_MAX_FEATS  = int(os.getenv(\"WORD_MAX_FEATS\", 20000))\n","C_MIN           = int(os.getenv(\"CHAR_NGRAM_MIN\", 3))\n","C_MAX           = int(os.getenv(\"CHAR_NGRAM_MAX\", 6))\n","SVD_DIM         = int(os.getenv(\"SVD_DIM\", 384))\n","RANDOM_STATE    = 42\n","\n","print(f\"Fitting TF‑IDF char_wb({C_MIN},{C_MAX}), max_features={CHAR_MAX_FEATS}...\")\n","char_vect = TfidfVectorizer(analyzer='char_wb', ngram_range=(C_MIN, C_MAX), max_features=CHAR_MAX_FEATS)\n","Xc_train = char_vect.fit_transform(train_texts)\n","\n","if USE_WORD_NGRAMS:\n","    print(f\"Fitting TF‑IDF word(1,2), max_features={WORD_MAX_FEATS}...\")\n","    word_vect = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=WORD_MAX_FEATS)\n","    Xw_train = word_vect.fit_transform(train_texts)\n","    X_train_sparse = hstack([Xc_train, Xw_train]).tocsr()\n","    del Xw_train\n","else:\n","    X_train_sparse = Xc_train.tocsr()\n","\n","del Xc_train; gc.collect()\n","\n","print(f\"Fitting TruncatedSVD to {SVD_DIM} dims...\")\n","svd = TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_STATE)\n","X_train_red = svd.fit_transform(X_train_sparse).astype(np.float32)\n","X_train_sparse = None; gc.collect()\n","\n","# L2 normalize for cosine stability\n","X_train_norm = normalize(X_train_red)  # float64 by default\n","X_train_norm = X_train_norm.astype(np.float32, copy=False)\n","\n","# -----------------------------\n","# 4) Build searchers: instance‑kNN (default) and optional label‑mean\n","# -----------------------------\n","USE_INSTANCE_SEARCH = True\n","inst_k = int(os.getenv(\"K_INST\", 3))  # majority over K neighbors\n","\n","print(\"Fitting instance‑level NearestNeighbors (cosine, brute)...\")\n","inst_nbrs = NearestNeighbors(metric='cosine', algorithm='brute')\n","inst_nbrs.fit(X_train_norm)\n","TRAIN_LABELS = labels\n","\n","# Optional: also build label‑mean index (fallback or comparison)\n","USE_LABEL_MEAN = int(os.getenv(\"USE_LABEL_MEAN\", 0))\n","if USE_LABEL_MEAN:\n","    print(\"Computing label‑mean embeddings (optional)...\")\n","    unique_labels, inv = np.unique(labels, return_inverse=True)\n","    counts = np.bincount(inv).astype(np.int32)\n","    sums = np.zeros((len(unique_labels), SVD_DIM), dtype=np.float32)\n","    for i in range(X_train_red.shape[0]):\n","        sums[inv[i]] += X_train_red[i]\n","    label_embeds = sums / np.maximum(counts[:,None], 1)\n","    label_embeds = normalize(label_embeds).astype(np.float32, copy=False)\n","    print(\"Fitting label‑mean NearestNeighbors (cosine, brute)...\")\n","    label_nbrs = NearestNeighbors(metric='cosine', algorithm='brute')\n","    label_nbrs.fit(label_embeds)\n","    K_LABEL = int(os.getenv(\"K_LABEL\", 25))\n","    # light reranker helpers\n","    TOKEN_RE = re.compile(r\"[a-z0-9çğıöşü]+\")\n","    DIGIT_RE = re.compile(r\"\\b\\d+[a-z]?\\b\")\n","    def tokenset(s: str): return set(TOKEN_RE.findall(s))\n","    def numset(s: str):    return set(DIGIT_RE.findall(s))\n","    print(\"Preparing representative text per label for rerank...\")\n","    label_first_text = {}\n","    for a, yi in zip(train_texts, inv):\n","        if yi not in label_first_text:\n","            label_first_text[yi] = a\n","    label_tokens = [tokenset(label_first_text.get(i, \"\")) for i in range(len(unique_labels))]\n","    label_nums   = [numset(label_first_text.get(i, \"\"))   for i in range(len(unique_labels))]\n","    def rerank_candidates(test_s, cand_idx, cand_dist):\n","        base = 1.0 - cand_dist\n","        tt, tn = tokenset(test_s), numset(test_s)\n","        best, best_score = 0, -1e9\n","        for j, yi in enumerate(cand_idx):\n","            tok = len(tt & label_tokens[yi]) / max(len(tt | label_tokens[yi]), 1)\n","            num = 1.0 if tn and (tn & label_nums[yi]) else 0.0\n","            score = base[j] + 0.03*tok + 0.05*num\n","            if score > best_score:\n","                best, best_score = j, score\n","        return cand_idx[best]\n","    TOPK_RERANK = int(os.getenv(\"TOPK_RERANK\", 0))\n","\n","# -----------------------------\n","# 5) Predict in batches\n","# -----------------------------\n","BATCH = int(os.getenv(\"TEST_BATCH\", 50000))\n","ids = test_df[\"id\"].to_numpy()\n","predicted_labels = np.empty(len(test_df), dtype=labels.dtype)\n","\n","print(f\"Predicting test in batches of {BATCH} (instance‑kNN, k={inst_k})...\")\n","start, n = 0, len(test_df)\n","while start < n:\n","    end = min(start + BATCH, n)\n","    chunk_ser = test_texts.iloc[start:end]\n","    chunk = chunk_ser.to_numpy()\n","\n","    # A) exact matches (direct label)\n","    pred_batch = np.array([EXACT_MAP.get(s, None) for s in chunk], dtype=object)\n","\n","    need_mask = np.fromiter((p is None for p in pred_batch), count=len(pred_batch), dtype=bool)\n","    if need_mask.any():\n","        need_idx = np.where(need_mask)[0]\n","        need_texts = chunk_ser.iloc[need_idx]\n","\n","        Xc = char_vect.transform(need_texts)\n","        if USE_WORD_NGRAMS:\n","            Xw = word_vect.transform(need_texts)\n","            Xt = hstack([Xc, Xw]).tocsr(); del Xw\n","        else:\n","            Xt = Xc.tocsr()\n","        del Xc; gc.collect()\n","\n","        Xt_red = svd.transform(Xt).astype(np.float32); Xt = None; gc.collect()\n","        Xt_norm = normalize(Xt_red).astype(np.float32, copy=False)\n","\n","        # Instance search (default)\n","        dist, idx = inst_nbrs.kneighbors(Xt_norm, n_neighbors=max(1, inst_k), return_distance=True)\n","        # Majority vote across K neighbors (tie -> closest)\n","        if inst_k == 1:\n","            pred_labels_need = TRAIN_LABELS[idx[:,0]]\n","        else:\n","            pred_labels_need = np.empty(len(need_idx), dtype=TRAIN_LABELS.dtype)\n","            for r in range(len(need_idx)):\n","                cand = TRAIN_LABELS[idx[r]]\n","                # count and tie‑break by nearest\n","                vals, counts = np.unique(cand, return_counts=True)\n","                best = vals[np.argmax(counts)]\n","                # If tie, choose label of nearest neighbor\n","                if (counts == counts.max()).sum() > 1:\n","                    best = cand[0]\n","                pred_labels_need[r] = best\n","        pred_batch[need_idx] = pred_labels_need\n","\n","    # Optional label‑mean fallback (only if some are still None)\n","    if USE_LABEL_MEAN and (pred_batch == None).any():\n","        need_idx = np.where(pred_batch == None)[0]\n","        if len(need_idx) > 0:\n","            need_texts = chunk_ser.iloc[need_idx]\n","            Xc = char_vect.transform(need_texts)\n","            if USE_WORD_NGRAMS:\n","                Xw = word_vect.transform(need_texts)\n","                Xt = hstack([Xc, Xw]).tocsr(); del Xw\n","            else:\n","                Xt = Xc.tocsr()\n","            del Xc; gc.collect()\n","            Xt_red = svd.transform(Xt).astype(np.float32); Xt = None; gc.collect()\n","            Xt_norm = normalize(Xt_red).astype(np.float32, copy=False)\n","            dist, idx = label_nbrs.kneighbors(Xt_norm, n_neighbors=max(1, K_LABEL), return_distance=True)\n","            if K_LABEL == 1 or TOPK_RERANK <= 1:\n","                pred_labels_need = unique_labels[idx[:,0]]\n","            else:\n","                pred_labels_need = np.empty(len(need_idx), dtype=unique_labels.dtype)\n","                for j in range(len(need_idx)):\n","                    yi = rerank_candidates(chunk[need_idx[j]], idx[j], dist[j])\n","                    pred_labels_need[j] = unique_labels[yi]\n","            pred_batch[need_idx] = pred_labels_need\n","\n","    predicted_labels[start:end] = pred_batch\n","    print(f\"Processed {end}/{n} rows...\")\n","    start = end\n","\n","# -----------------------------\n","# 6) Save submission\n","# -----------------------------\n","sub = pd.DataFrame({\"id\": ids, \"label\": predicted_labels})\n","sub.to_csv(SUBMIT_OUT, index=False)\n","print(f\"Saved: {SUBMIT_OUT}\")\n","\n","# -----------------------------\n","# Notes\n","# -----------------------------\n","# * If RAM is tight, reduce CHAR_MAX_FEATS and/or SVD_DIM.\n","# * If speed is tight, disable USE_WORD_NGRAMS (default 0) and keep char 3‑6.\n","# * Usually (char‑only + SVD 384 + instance k=3 + L2 norm) >> label‑mean.\n","# * For even faster ANN, replace inst_nbrs with faiss IndexFlatIP on L2‑normalized vectors.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5JKPOtfZfzm","executionInfo":{"status":"ok","timestamp":1756203701594,"user_tz":-180,"elapsed":8366836,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"633c062a-198b-477e-ad87-445510d214d8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","                                             address  label\n","0  Akarca Mah. Adnan Menderes Cad. 864.Sok. No:15...   8831\n","1  Cumhuriye Mah. Hükümet Cad. Sivriler İşhanı No...   8810\n","   id                                            address\n","0   0    Menderes mahallesi 1013 sok No.40 Daire.2 Kat.2\n","1   1  250. Sk. No:14 B Blok Kat:5 Daire:14\\n3. Halil...\n","Preprocessing texts...\n","Building exact‑match dictionary from train...\n","Exact dictionary size: 835,663\n","Fitting TF‑IDF char_wb(3,6), max_features=40000...\n","Fitting TruncatedSVD to 384 dims...\n","Fitting instance‑level NearestNeighbors (cosine, brute)...\n","Predicting test in batches of 50000 (instance‑kNN, k=3)...\n","Processed 50000/217241 rows...\n","Processed 100000/217241 rows...\n","Processed 150000/217241 rows...\n","Processed 200000/217241 rows...\n","Processed 217241/217241 rows...\n","Saved: submission.csv\n"]}]},{"cell_type":"code","source":["# P0: Fast Address Matching (Batch + SVD + NearestNeighbors)\n","# -----------------------------------------------------------\n","# Colab-friendly, scalable baseline without Python O(N*M) loops.\n","# - Char + Word TF-IDF (sparse)\n","# - hstack -> TruncatedSVD (e.g., 256-D)\n","# - Label mean embeddings\n","# - NearestNeighbors (cosine, brute) for fast batch prediction\n","# - Memory-friendly batching for test transform & search\n","# - Float32 everywhere possible\n","#\n","# Expected speedup: >10-100x over per-address/per-label loops.\n","# -----------------------------------------------------------\n","\n","import os, sys, gc, math, time, warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.sparse import hstack, csr_matrix\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.neighbors import NearestNeighbors\n","\n","# -----------------------------\n","# 0) Utility: lightweight preprocessing\n","# -----------------------------\n","\n","TR_MAP = str.maketrans({\n","    'Ç':'ç','Ğ':'ğ','İ':'i','I':'ı','Ö':'ö','Ş':'ş','Ü':'ü',\n","    '\\u00A0': ' '  # non‑breaking space -> space\n","})\n","\n","PUNCTS = \"\\t\\n\\r\\f\\v!\\\"#$%&'()*+,./:;<=>?@[\\\\]^`{|}~\"  # Turkish letters kept\n","\n","import re\n","MULTISPACE_RE = re.compile(r\"\\s+\")\n","PUNCT_RE = re.compile(\"[\" + re.escape(PUNCTS) + \"]+\")\n","\n","\n","def preprocess_address(text: str) -> str:\n","    if not isinstance(text, str):\n","        return \"\"\n","    t = text.translate(TR_MAP)\n","    t = t.lower()\n","    # normalize punctuation/whitespace but keep numbers and Turkish chars\n","    t = PUNCT_RE.sub(\" \", t)\n","    t = MULTISPACE_RE.sub(\" \", t).strip()\n","    return t\n","\n","# -----------------------------\n","# 1) Load data\n","# -----------------------------\n","\n","# Expected columns:\n","#   train.csv: address, label\n","#   test.csv : id, address  (if no id, we auto-generate)\n","\n","TRAIN_PATH = os.getenv(\"TRAIN_PATH\", \"train.csv\")\n","TEST_PATH  = os.getenv(\"TEST_PATH\",  \"test.csv\")\n","SUBMIT_OUT = os.getenv(\"SUBMIT_OUT\", \"submission.csv\")\n","\n","print(\"Loading data...\")\n","train_df = pd.read_csv(TRAIN_PATH)\n","test_df  = pd.read_csv(TEST_PATH)\n","print(train_df.head(2))\n","print(test_df.head(2))\n","\n","assert \"address\" in train_df.columns and \"label\" in train_df.columns, \"train.csv must have columns: address,label\"\n","assert \"address\" in test_df.columns, \"test.csv must have column: address\"\n","\n","if \"id\" not in test_df.columns:\n","    test_df[\"id\"] = np.arange(len(test_df), dtype=np.int64)\n","\n","# -----------------------------\n","# 2) Preprocess (vectorized)\n","# -----------------------------\n","\n","print(\"Preprocessing texts (train/test)...\")\n","train_texts = train_df[\"address\"].astype(str).map(preprocess_address)\n","test_texts  = test_df[\"address\"].astype(str).map(preprocess_address)\n","\n","# -----------------------------\n","# 3) TF‑IDF (char + word) and SVD dimensionality reduction\n","# -----------------------------\n","\n","# NOTE: Keep sizes moderate to control RAM; tune as needed.\n","CHAR_MAX_FEATS = int(os.getenv(\"CHAR_MAX_FEATS\", 20000))\n","WORD_MAX_FEATS = int(os.getenv(\"WORD_MAX_FEATS\", 20000))\n","SVD_DIM        = int(os.getenv(\"SVD_DIM\", 256))\n","RANDOM_STATE   = 42\n","\n","print(f\"Fitting TF‑IDF (char_wb 3‑5, max_features={CHAR_MAX_FEATS})...\")\n","char_vect = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_features=CHAR_MAX_FEATS)\n","X_char_train = char_vect.fit_transform(train_texts)\n","\n","print(f\"Fitting TF‑IDF (word 1‑2, max_features={WORD_MAX_FEATS})...\")\n","word_vect = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=WORD_MAX_FEATS)\n","X_word_train = word_vect.fit_transform(train_texts)\n","\n","print(\"Stacking sparse matrices (train)...\")\n","X_train_sparse = hstack([X_char_train, X_word_train]).tocsr()\n","\n","print(f\"Fitting TruncatedSVD to {SVD_DIM} dims (LSA)...\")\n","svd = TruncatedSVD(n_components=SVD_DIM, random_state=RANDOM_STATE)\n","X_train_red = svd.fit_transform(X_train_sparse).astype(np.float32)\n","\n","# Free large sparse parts not needed after transform for training\n","X_char_train = X_word_train = X_train_sparse = None; gc.collect()\n","\n","# -----------------------------\n","# 4) Label mean embeddings (float32)\n","# -----------------------------\n","\n","print(\"Computing label mean embeddings...\")\n","labels = train_df[\"label\"].to_numpy()\n","unique_labels, inv = np.unique(labels, return_inverse=True)\n","\n","# Accumulate sums per label index\n","label_count = np.bincount(inv).astype(np.int32)\n","label_sum = np.zeros((len(unique_labels), SVD_DIM), dtype=np.float32)\n","for i in range(X_train_red.shape[0]):\n","    label_sum[inv[i]] += X_train_red[i]\n","\n","label_embeds = (label_sum / np.maximum(label_count[:, None], 1)).astype(np.float32)\n","\n","# -----------------------------\n","# 5) Fit NearestNeighbors (cosine, brute)\n","# -----------------------------\n","\n","print(\"Fitting NearestNeighbors (cosine, brute)...\")\n","nbrs = NearestNeighbors(metric='cosine', algorithm='brute')\n","nbrs.fit(label_embeds)\n","\n","# -----------------------------\n","# 6) Transform TEST in batches and search top‑1\n","# -----------------------------\n","\n","BATCH = int(os.getenv(\"TEST_BATCH\", 50000))  # tune to your RAM\n","K = int(os.getenv(\"TOPK\", 1))  # P0 = 1; can increase later\n","\n","print(f\"Transforming & searching test in batches of {BATCH}...\")\n","ids = test_df[\"id\"].to_numpy()\n","all_pred_idx = np.empty(len(test_df), dtype=np.int32)\n","\n","n = len(test_df)\n","start = 0\n","while start < n:\n","    end = min(start + BATCH, n)\n","    chunk = test_texts.iloc[start:end]\n","\n","    # sparse transform (char + word)\n","    Xc = char_vect.transform(chunk)\n","    Xw = word_vect.transform(chunk)\n","    Xt = hstack([Xc, Xw]).tocsr()\n","    Xc = Xw = None; gc.collect()\n","\n","    # SVD to dense float32\n","    Xt_red = svd.transform(Xt).astype(np.float32)\n","    Xt = None; gc.collect()\n","\n","    # cosine kneighbors (returns distance; smaller is better)\n","    dist, idx = nbrs.kneighbors(Xt_red, n_neighbors=K, return_distance=True)\n","    all_pred_idx[start:end] = idx[:,0].astype(np.int32)\n","\n","    print(f\"Processed {end}/{n} test rows...\")\n","    start = end\n","\n","predicted_labels = unique_labels[all_pred_idx]\n","\n","# -----------------------------\n","# 7) Save submission\n","# -----------------------------\n","\n","sub = pd.DataFrame({\"id\": ids, \"label\": predicted_labels})\n","sub.to_csv(SUBMIT_OUT, index=False)\n","print(f\"Saved: {SUBMIT_OUT}\")\n","\n","# -----------------------------\n","# 8) Notes / Tuning\n","# -----------------------------\n","# - Increase SVD_DIM (e.g., 384) if RAM allows; improves recall a bit.\n","# - If train is very large, you can SVD-fit on a stratified sample and then transform all.\n","# - For even faster search, swap NearestNeighbors with FAISS (IndexFlatIP on normalized vectors)\n","#   after L2-normalizing embeddings; cosine ~ dot on normalized vectors.\n","# - For better accuracy later (P1), do city/district blocking before kneighbors and rerank top‑K.\n","\n","\n","# =============================\n","# 9) P2a – Label Prototypes (multi‑centroid per label, drop‑in)\n","# =============================\n","# Amaç: Label ortalaması tek modlu kabul ediyor; çok modlu etiketlerde\n","# isabet düşüyor. Her label için 1‑3 prototip (medoid benzeri) seçip\n","# prototipler üzerinde arama yapmak genelde 0.30‑0.40 bandını yukarı taşır.\n","# Aşağıdaki kod P0’a drop‑in: SVD çıktısını L2 normalize edip, her label’dan\n","# birkaç temsilci seçer ve NearestNeighbors’i prototiplere kurar.\n","\n","import numpy as np\n","from sklearn.preprocessing import normalize\n","\n","# --- 9.1) Train embeddings'i L2 normalize et (cosine için şart) ---\n","X_train_norm = normalize(X_train_red).astype(np.float32, copy=False)\n","\n","# --- 9.2) Farthest‑Point Sampling ile label prototipleri seç ---\n","# Hızlı ve stabil: KMeans yerine, label içinden en temsilî 1‑3 noktayı seçiyoruz.\n","\n","def build_label_prototypes_fps(X_norm: np.ndarray, labels_np: np.ndarray,\n","                               unique_labels_np: np.ndarray, inv_idx: np.ndarray,\n","                               max_k: int = 3) -> tuple[np.ndarray, np.ndarray]:\n","    \"\"\"X_norm: (N, D) L2‑normalize edilmiş train embedding\n","       labels_np: (N,) train label'ları\n","       unique_labels_np: benzersiz label dizisi\n","       inv_idx: her train satırı için unique_labels indeksine işaret eden dizi\n","       Çıktı: (P, D) prototip matris, (P,) prototip label'ları\n","    \"\"\"\n","    protos = []\n","    proto_labs = []\n","    D = X_norm.shape[1]\n","\n","    for li in range(len(unique_labels_np)):\n","        idxs = np.where(inv_idx == li)[0]\n","        cnt = len(idxs)\n","        if cnt == 0:\n","            continue\n","        # Küçük sınıflar için 1 prototip yeterli; büyüdükçe 2‑3\n","        if cnt < 20:\n","            k = 1\n","        elif cnt < 80:\n","            k = 2\n","        else:\n","            k = 3\n","        k = min(k, max_k, cnt)\n","\n","        Xi = X_norm[idxs]  # (cnt, D)\n","        # 1) İlk prototip: label ortalamasına en benzer nokta (medoid‑of‑mean)\n","        mu = Xi.mean(axis=0, dtype=np.float32)\n","        mu /= (np.linalg.norm(mu) + 1e-9)\n","        sims = Xi @ mu\n","        chosen_local = [int(np.argmax(sims))]\n","\n","        # 2..k) Farthest‑Point: seçili noktalara en uzak (min max sim) yeni nokta\n","        while len(chosen_local) < k:\n","            C = Xi[chosen_local]                    # (t, D)\n","            sims_to_chosen = Xi @ C.T               # (cnt, t)\n","            nearest_sim = np.max(sims_to_chosen, axis=1)  # (cnt,)\n","            pick = int(np.argmin(nearest_sim))\n","            if pick in chosen_local:\n","                break\n","            chosen_local.append(pick)\n","\n","        for loc in chosen_local:\n","            protos.append(Xi[loc])\n","            proto_labs.append(unique_labels_np[li])\n","\n","    proto_mat = np.vstack(protos).astype(np.float32)\n","    # Güvenlik için tekrar normalize\n","    proto_mat = proto_mat / (np.linalg.norm(proto_mat, axis=1, keepdims=True) + 1e-9)\n","    return proto_mat, np.asarray(proto_labs)\n","\n","print(\"Building label prototypes (FPS)...\")\n","proto_matrix, proto_labels = build_label_prototypes_fps(\n","    X_train_norm, labels, unique_labels, inv, max_k=int(os.getenv(\"MAX_PROTO_PER_LABEL\", 3))\n",")\n","print(f\"Prototypes total: {len(proto_labels):,}\")\n","\n","# --- 9.3) NearestNeighbors'i prototiplerde kur ---\n","print(\"Fitting NearestNeighbors on prototypes (cosine, brute)...\")\n","proto_nbrs = NearestNeighbors(metric='cosine', algorithm='brute')\n","proto_nbrs.fit(proto_matrix)\n","\n","# --- 9.4) Test akışını prototiplere göre değiştir ---\n","# Orijinal 6) bloğunu yorumlayıp aşağıdaki sürümü kullanın:\n","\n","# BATCH = int(os.getenv(\"TEST_BATCH\", 50000))\n","# K = int(os.getenv(\"TOPK\", 1))\n","\n","print(f\"Transforming & searching test against prototypes in batches of {BATCH}...\")\n","ids = test_df[\"id\"].to_numpy()\n","all_pred = np.empty(len(test_df), dtype=proto_labels.dtype)\n","\n","start = 0; n = len(test_df)\n","while start < n:\n","    end = min(start + BATCH, n)\n","    chunk = test_texts.iloc[start:end]\n","\n","    Xc = char_vect.transform(chunk)\n","    Xw = word_vect.transform(chunk)\n","    Xt = hstack([Xc, Xw]).tocsr(); Xc = Xw = None; gc.collect()\n","\n","    Xt_red = svd.transform(Xt).astype(np.float32); Xt = None; gc.collect()\n","    Xt_norm = normalize(Xt_red).astype(np.float32, copy=False)\n","\n","    dist, idx = proto_nbrs.kneighbors(Xt_norm, n_neighbors=max(1, K), return_distance=True)\n","    # Tek komşu → o prototipin label'ı\n","    all_pred[start:end] = proto_labels[idx[:, 0]]\n","    print(f\"Processed {end}/{n} test rows (prototypes)...\")\n","    start = end\n","\n","sub = pd.DataFrame({\"id\": ids, \"label\": all_pred})\n","sub.to_csv(SUBMIT_OUT, index=False)\n","print(f\"Saved (prototypes): {SUBMIT_OUT}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RShEGtwqKsPF","executionInfo":{"status":"ok","timestamp":1756208758224,"user_tz":-180,"elapsed":477779,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"3421babc-9f54-41c1-e31f-e90a877bbab4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","                                             address  label\n","0  Akarca Mah. Adnan Menderes Cad. 864.Sok. No:15...   8831\n","1  Cumhuriye Mah. Hükümet Cad. Sivriler İşhanı No...   8810\n","   id                                            address\n","0   0    Menderes mahallesi 1013 sok No.40 Daire.2 Kat.2\n","1   1  250. Sk. No:14 B Blok Kat:5 Daire:14\\n3. Halil...\n","Preprocessing texts (train/test)...\n","Fitting TF‑IDF (char_wb 3‑5, max_features=20000)...\n","Fitting TF‑IDF (word 1‑2, max_features=20000)...\n","Stacking sparse matrices (train)...\n","Fitting TruncatedSVD to 256 dims (LSA)...\n","Computing label mean embeddings...\n","Fitting NearestNeighbors (cosine, brute)...\n","Transforming & searching test in batches of 50000...\n","Processed 50000/217241 test rows...\n","Processed 100000/217241 test rows...\n","Processed 150000/217241 test rows...\n","Processed 200000/217241 test rows...\n","Processed 217241/217241 test rows...\n","Saved: submission.csv\n","Building label prototypes (FPS)...\n","Prototypes total: 25,427\n","Fitting NearestNeighbors on prototypes (cosine, brute)...\n","Transforming & searching test against prototypes in batches of 50000...\n","Processed 50000/217241 test rows (prototypes)...\n","Processed 100000/217241 test rows (prototypes)...\n","Processed 150000/217241 test rows (prototypes)...\n","Processed 200000/217241 test rows (prototypes)...\n","Processed 217241/217241 test rows (prototypes)...\n","Saved (prototypes): submission.csv\n"]}]}]}