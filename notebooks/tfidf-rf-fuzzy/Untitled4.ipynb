{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPmVAoY0TnpMF6sr0svRETe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Colab ortamı için kütüphane kurulumu\n","!pip install --upgrade pip\n","\n","# Ana kütüphaneler (Colab'da genelde yüklü ama güncelleyelim)\n","!pip install pandas numpy matplotlib seaborn scikit-learn\n","\n","# XGBoost ve LightGBM (Colab uyumlu versiyonlar)\n","!pip install xgboost lightgbm\n","\n","# CatBoost (Colab'da genelde sorunsuz)\n","!pip install catboost\n","\n","# Text processing\n","!pip install fuzzywuzzy python-levenshtein\n","\n","# NetworkX (opsiyonel - eğer network analysis yaparsak)\n","!pip install networkx\n","\n","# Alternatif eğer sorun çıkarsa:\n","# !pip install xgboost==1.7.6  # Stable version\n","# !pip install lightgbm==3.3.5  # Stable version\n","# !pip install catboost==1.2     # Stable version\n","\n","print(\"✅ Tüm kütüphaneler yüklendi!\")\n","\n","# Test et\n","try:\n","    import pandas as pd\n","    import numpy as np\n","    import xgboost as xgb\n","    import lightgbm as lgb\n","    from catboost import CatBoostClassifier\n","    from fuzzywuzzy import fuzz\n","    print(\"✅ Import testleri başarılı!\")\n","except ImportError as e:\n","    print(f\"❌ Import hatası: {e}\")\n","    print(\"Manuel yükleme gerekebilir.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SzjbNh5nzwbi","executionInfo":{"status":"ok","timestamp":1755849933173,"user_tz":-180,"elapsed":39938,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"a6d8ce0d-2456-4607-8570-033919dbc212"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n","Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-25.2\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n","Collecting catboost\n","  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n","Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.8\n","Collecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n","Collecting python-levenshtein\n","  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n","Collecting Levenshtein==0.27.1 (from python-levenshtein)\n","  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-levenshtein)\n","  Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n","Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n","Downloading rapidfuzz-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-levenshtein\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [python-levenshtein]\n","\u001b[1A\u001b[2KSuccessfully installed Levenshtein-0.27.1 fuzzywuzzy-0.18.0 python-levenshtein-0.27.1 rapidfuzz-3.13.0\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n","✅ Tüm kütüphaneler yüklendi!\n","✅ Import testleri başarılı!\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from collections import defaultdict, Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from xgboost import XGBClassifier\n","import lightgbm as lgb\n","from catboost import CatBoostClassifier\n","import networkx as nx\n","from scipy.spatial.distance import cosine\n","from fuzzywuzzy import fuzz\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class AdvancedAddressResolver:\n","    def __init__(self):\n","        \"\"\"Advanced multi-stage address resolution system\"\"\"\n","        # Stage 1: Hierarchical clustering\n","        self.geographic_clusterer = GeographicHierarchyClusterer()\n","\n","        # Stage 2: Semantic similarity\n","        self.semantic_matcher = SemanticAddressMatcher()\n","\n","        # Stage 3: Advanced ensemble\n","        self.ensemble_predictor = AdvancedEnsemblePredictor()\n","\n","        # Stage 4: Post-processing\n","        self.post_processor = AddressPostProcessor()\n","\n","        # Address database\n","        self.address_database = AddressDatabase()\n","\n","    def fit(self, addresses, labels):\n","        \"\"\"Multi-stage training process\"\"\"\n","        print(\"🚀 Advanced Address Resolution Training Started...\")\n","\n","        # Stage 1: Build address database and hierarchy\n","        print(\"1️⃣ Building geographic hierarchy...\")\n","        self.address_database.build(addresses, labels)\n","        geographic_features = self.geographic_clusterer.fit_transform(addresses, labels)\n","\n","        # Stage 2: Train semantic matcher\n","        print(\"2️⃣ Training semantic matcher...\")\n","        semantic_features = self.semantic_matcher.fit_transform(addresses, labels)\n","\n","        # Stage 3: Combine features and train ensemble\n","        print(\"3️⃣ Training advanced ensemble...\")\n","        combined_features = np.hstack([geographic_features, semantic_features])\n","        self.ensemble_predictor.fit(combined_features, labels)\n","\n","        # Stage 4: Prepare post-processor\n","        print(\"4️⃣ Configuring post-processor...\")\n","        self.post_processor.configure(addresses, labels, self.address_database)\n","\n","        print(\"✅ Training completed!\")\n","        return self\n","\n","    def predict(self, addresses):\n","        \"\"\"Multi-stage prediction process\"\"\"\n","        print(\"🔮 Advanced prediction started...\")\n","\n","        # Stage 1: Extract geographic features\n","        geographic_features = self.geographic_clusterer.transform(addresses)\n","\n","        # Stage 2: Extract semantic features\n","        semantic_features = self.semantic_matcher.transform(addresses)\n","\n","        # Stage 3: Ensemble prediction\n","        combined_features = np.hstack([geographic_features, semantic_features])\n","        raw_predictions = self.ensemble_predictor.predict(combined_features)\n","\n","        # Stage 4: Post-process with similarity and rules\n","        final_predictions = self.post_processor.refine_predictions(\n","            addresses, raw_predictions\n","        )\n","\n","        print(\"✅ Prediction completed!\")\n","        return final_predictions\n","\n","\n","class GeographicHierarchyClusterer:\n","    def __init__(self):\n","        self.province_clusters = {}\n","        self.district_clusters = {}\n","        self.neighborhood_clusters = {}\n","        self.vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1,3))\n","\n","    def fit_transform(self, addresses, labels):\n","        \"\"\"Create geographic hierarchy features\"\"\"\n","        # Parse all addresses for geographic info\n","        parsed_data = []\n","        for addr, label in zip(addresses, labels):\n","            parsed = self._parse_address_components(addr)\n","            parsed['label'] = label\n","            parsed_data.append(parsed)\n","\n","        # Build province-level clusters\n","        self._build_province_clusters(parsed_data)\n","\n","        # Build district-level clusters\n","        self._build_district_clusters(parsed_data)\n","\n","        # Build neighborhood-level clusters\n","        self._build_neighborhood_clusters(parsed_data)\n","\n","        # Extract features\n","        features = self._extract_hierarchy_features(addresses)\n","        return features\n","\n","    def transform(self, addresses):\n","        \"\"\"Transform new addresses using fitted hierarchy\"\"\"\n","        return self._extract_hierarchy_features(addresses)\n","\n","    def _parse_address_components(self, address):\n","        \"\"\"Enhanced address parsing with comprehensive Turkey locations\"\"\"\n","        if pd.isna(address):\n","            address = \"\"\n","        address = str(address).lower()\n","\n","        # Import comprehensive location data\n","        from turkey_locations import TURKEY_LOCATIONS, find_location_in_address\n","\n","        # All provinces (81 il)\n","        provinces = {}\n","        for prov, variants in TURKEY_LOCATIONS['provinces'].items():\n","            provinces[prov] = variants\n","\n","        # All districts (comprehensive list)\n","        districts = {}\n","        for category in ['izmir_districts', 'manisa_districts', 'denizli_districts',\n","                        'mugla_districts', 'aydin_districts', 'istanbul_districts', 'ankara_districts']:\n","            if category in TURKEY_LOCATIONS:\n","                for dist, variants in TURKEY_LOCATIONS[category].items():\n","                    districts[dist] = variants\n","\n","        # Extract components\n","        components = {\n","            'province': None,\n","            'district': None,\n","            'neighborhood': None,\n","            'street': None,\n","            'number': None\n","        }\n","\n","        # Extract province using comprehensive list\n","        for prov, variants in provinces.items():\n","            if any(var.lower() in address for var in variants):\n","                components['province'] = prov\n","                break\n","\n","        # Extract district using comprehensive list\n","        for dist, variants in districts.items():\n","            if any(var.lower() in address for var in variants):\n","                components['district'] = dist\n","                break\n","\n","        # Extract neighborhood\n","        mah_match = re.search(r'(\\w+)\\s+(mah|mahalle|mahallesi)', address)\n","        if mah_match:\n","            components['neighborhood'] = mah_match.group(1)\n","\n","        # Extract street\n","        sok_match = re.search(r'(\\d+\\.?\\s*)(sok|sokak|sokağı)', address)\n","        if sok_match:\n","            components['street'] = sok_match.group(1).strip()\n","\n","        # Extract number\n","        no_match = re.search(r'(no|numara)\\s*:?\\s*(\\d+)', address)\n","        if no_match:\n","            components['number'] = no_match.group(2)\n","\n","        return components\n","\n","    def _build_province_clusters(self, parsed_data):\n","        \"\"\"Build province-level label clusters\"\"\"\n","        province_labels = defaultdict(list)\n","        for item in parsed_data:\n","            if item['province']:\n","                province_labels[item['province']].append(item['label'])\n","\n","        for province, labels in province_labels.items():\n","            self.province_clusters[province] = set(labels)\n","\n","    def _build_district_clusters(self, parsed_data):\n","        \"\"\"Build district-level label clusters\"\"\"\n","        district_labels = defaultdict(list)\n","        for item in parsed_data:\n","            if item['district']:\n","                district_labels[item['district']].append(item['label'])\n","\n","        for district, labels in district_labels.items():\n","            self.district_clusters[district] = set(labels)\n","\n","    def _build_neighborhood_clusters(self, parsed_data):\n","        \"\"\"Build neighborhood-level label clusters\"\"\"\n","        neighborhood_labels = defaultdict(list)\n","        for item in parsed_data:\n","            if item['neighborhood']:\n","                neighborhood_labels[item['neighborhood']].append(item['label'])\n","\n","        for neighborhood, labels in neighborhood_labels.items():\n","            self.neighborhood_clusters[neighborhood] = set(labels)\n","\n","    def _extract_hierarchy_features(self, addresses):\n","        \"\"\"Extract features based on geographic hierarchy\"\"\"\n","        features = []\n","\n","        for address in addresses:\n","            parsed = self._parse_address_components(address)\n","\n","            # Geographic hierarchy features\n","            feature_row = []\n","\n","            # Province features (one-hot)\n","            provinces = ['izmir', 'manisa', 'denizli', 'mugla', 'aydin', 'usak']\n","            for prov in provinces:\n","                feature_row.append(1 if parsed['province'] == prov else 0)\n","\n","            # District features (one-hot for top districts)\n","            top_districts = ['bornova', 'konak', 'karsiyaka', 'pamukkale', 'fethiye', 'bodrum']\n","            for dist in top_districts:\n","                feature_row.append(1 if parsed['district'] == dist else 0)\n","\n","            # Completeness features\n","            feature_row.append(1 if parsed['province'] else 0)\n","            feature_row.append(1 if parsed['district'] else 0)\n","            feature_row.append(1 if parsed['neighborhood'] else 0)\n","            feature_row.append(1 if parsed['street'] else 0)\n","            feature_row.append(1 if parsed['number'] else 0)\n","\n","            # Cluster size features (proxy for label frequency)\n","            prov_cluster_size = len(self.province_clusters.get(parsed['province'], set()))\n","            dist_cluster_size = len(self.district_clusters.get(parsed['district'], set()))\n","            neigh_cluster_size = len(self.neighborhood_clusters.get(parsed['neighborhood'], set()))\n","\n","            feature_row.extend([\n","                np.log1p(prov_cluster_size),\n","                np.log1p(dist_cluster_size),\n","                np.log1p(neigh_cluster_size)\n","            ])\n","\n","            features.append(feature_row)\n","\n","        return np.array(features)\n","\n","\n","class SemanticAddressMatcher:\n","    def __init__(self):\n","        self.tfidf_vectorizer = TfidfVectorizer(\n","            max_features=5000,\n","            ngram_range=(1, 4),\n","            analyzer='char_wb'\n","        )\n","        self.svd = TruncatedSVD(n_components=100, random_state=42)\n","        self.address_embeddings = None\n","        self.label_centroids = {}\n","\n","    def fit_transform(self, addresses, labels):\n","        \"\"\"Create semantic embeddings and label centroids\"\"\"\n","        # Clean addresses\n","        cleaned = [self._clean_address(addr) for addr in addresses]\n","\n","        # Create TF-IDF features\n","        tfidf_features = self.tfidf_vectorizer.fit_transform(cleaned)\n","\n","        # Reduce dimensionality\n","        reduced_features = self.svd.fit_transform(tfidf_features)\n","        self.address_embeddings = reduced_features\n","\n","        # Calculate label centroids\n","        unique_labels = np.unique(labels)\n","        for label in unique_labels:\n","            mask = np.array(labels) == label\n","            if np.any(mask):\n","                centroid = np.mean(reduced_features[mask], axis=0)\n","                self.label_centroids[label] = centroid\n","\n","        # Generate similarity features\n","        similarity_features = self._generate_similarity_features(reduced_features, labels)\n","\n","        return np.hstack([reduced_features, similarity_features])\n","\n","    def transform(self, addresses):\n","        \"\"\"Transform new addresses to semantic features\"\"\"\n","        cleaned = [self._clean_address(addr) for addr in addresses]\n","        tfidf_features = self.tfidf_vectorizer.transform(cleaned)\n","        reduced_features = self.svd.transform(tfidf_features)\n","\n","        similarity_features = self._generate_similarity_features(reduced_features, None)\n","\n","        return np.hstack([reduced_features, similarity_features])\n","\n","    def _clean_address(self, address):\n","        \"\"\"Advanced address cleaning\"\"\"\n","        if pd.isna(address):\n","            return \"\"\n","\n","        address = str(address).lower()\n","\n","        # Remove noise\n","        address = re.sub(r'[^\\w\\s]', ' ', address)\n","        address = re.sub(r'\\s+', ' ', address)\n","\n","        # Expand abbreviations\n","        replacements = {\n","            'mah': 'mahalle', 'sok': 'sokak', 'cd': 'cadde',\n","            'apt': 'apartman', 'no': 'numara', 'bl': 'blok'\n","        }\n","\n","        for abbr, full in replacements.items():\n","            address = re.sub(rf'\\b{abbr}\\b', full, address)\n","\n","        return address.strip()\n","\n","    def _generate_similarity_features(self, embeddings, labels=None):\n","        \"\"\"Generate features based on similarity to label centroids\"\"\"\n","        if not self.label_centroids:\n","            return np.zeros((len(embeddings), 20))  # Placeholder\n","\n","        similarity_features = []\n","        centroid_list = list(self.label_centroids.values())\n","\n","        for embedding in embeddings:\n","            similarities = []\n","            for centroid in centroid_list:\n","                sim = 1 - cosine(embedding, centroid)\n","                similarities.append(max(0, sim))  # Ensure non-negative\n","\n","            # Top-K similarities as features\n","            similarities.sort(reverse=True)\n","            top_similarities = similarities[:20]  # Top 20\n","\n","            # Pad if needed\n","            while len(top_similarities) < 20:\n","                top_similarities.append(0.0)\n","\n","            similarity_features.append(top_similarities)\n","\n","        return np.array(similarity_features)\n","\n","\n","class AdvancedEnsemblePredictor:\n","    def __init__(self):\n","        # Multiple diverse models\n","        self.models = {\n","            'xgb': XGBClassifier(\n","                n_estimators=500,\n","                max_depth=15,\n","                learning_rate=0.02,\n","                subsample=0.8,\n","                colsample_bytree=0.8,\n","                random_state=42\n","            ),\n","            'lgb': lgb.LGBMClassifier(\n","                n_estimators=500,\n","                max_depth=15,\n","                learning_rate=0.02,\n","                feature_fraction=0.8,\n","                bagging_fraction=0.8,\n","                random_state=42,\n","                verbose=-1\n","            ),\n","            'catboost': CatBoostClassifier(\n","                iterations=300,\n","                depth=12,\n","                learning_rate=0.02,\n","                random_state=42,\n","                verbose=False\n","            ),\n","            'rf': RandomForestClassifier(\n","                n_estimators=300,\n","                max_depth=20,\n","                min_samples_split=5,\n","                min_samples_leaf=2,\n","                random_state=42,\n","                n_jobs=-1\n","            )\n","        }\n","\n","        self.scaler = StandardScaler()\n","\n","    def fit(self, X, y):\n","        \"\"\"Train ensemble of models\"\"\"\n","        X_scaled = self.scaler.fit_transform(X)\n","\n","        for name, model in self.models.items():\n","            print(f\"Training {name}...\")\n","            model.fit(X_scaled, y)\n","\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"Ensemble prediction with weighted voting\"\"\"\n","        X_scaled = self.scaler.transform(X)\n","\n","        predictions = {}\n","        for name, model in self.models.items():\n","            predictions[name] = model.predict(X_scaled)\n","\n","        # Weighted ensemble (based on expected performance)\n","        weights = {'xgb': 0.35, 'lgb': 0.3, 'catboost': 0.25, 'rf': 0.1}\n","\n","        final_predictions = []\n","        for i in range(len(X_scaled)):\n","            # Weighted voting\n","            vote_counts = defaultdict(float)\n","            for name, preds in predictions.items():\n","                vote_counts[preds[i]] += weights[name]\n","\n","            # Select prediction with highest weight\n","            best_prediction = max(vote_counts.items(), key=lambda x: x[1])[0]\n","            final_predictions.append(best_prediction)\n","\n","        return np.array(final_predictions)\n","\n","\n","class AddressPostProcessor:\n","    def __init__(self):\n","        self.address_similarity_threshold = 0.85\n","        self.label_lookup = {}\n","\n","    def configure(self, addresses, labels, address_database):\n","        \"\"\"Configure post-processor with training data\"\"\"\n","        # Build exact and fuzzy lookup\n","        for addr, label in zip(addresses, labels):\n","            clean_addr = self._normalize_for_lookup(addr)\n","            if clean_addr not in self.label_lookup:\n","                self.label_lookup[clean_addr] = []\n","            self.label_lookup[clean_addr].append(label)\n","\n","        # Keep most common label for each address\n","        for addr in self.label_lookup:\n","            self.label_lookup[addr] = Counter(self.label_lookup[addr]).most_common(1)[0][0]\n","\n","    def refine_predictions(self, addresses, predictions):\n","        \"\"\"Post-process predictions with similarity matching\"\"\"\n","        refined = predictions.copy()\n","\n","        for i, addr in enumerate(addresses):\n","            clean_addr = self._normalize_for_lookup(addr)\n","\n","            # Exact match lookup\n","            if clean_addr in self.label_lookup:\n","                refined[i] = self.label_lookup[clean_addr]\n","                continue\n","\n","            # Fuzzy matching\n","            best_match = None\n","            best_score = 0\n","\n","            for lookup_addr, lookup_label in list(self.label_lookup.items())[:1000]:  # Limit for performance\n","                score = fuzz.ratio(clean_addr, lookup_addr) / 100.0\n","                if score > best_score and score > self.address_similarity_threshold:\n","                    best_score = score\n","                    best_match = lookup_label\n","\n","            if best_match is not None:\n","                refined[i] = best_match\n","\n","        return refined\n","\n","    def _normalize_for_lookup(self, address):\n","        \"\"\"Normalize address for lookup\"\"\"\n","        if pd.isna(address):\n","            return \"\"\n","\n","        address = str(address).lower()\n","        address = re.sub(r'[^\\w\\s]', '', address)\n","        address = re.sub(r'\\s+', ' ', address)\n","        return address.strip()\n","\n","\n","class AddressDatabase:\n","    def __init__(self):\n","        self.address_to_labels = defaultdict(set)\n","        self.label_to_addresses = defaultdict(set)\n","\n","    def build(self, addresses, labels):\n","        \"\"\"Build bidirectional address-label database\"\"\"\n","        for addr, label in zip(addresses, labels):\n","            clean_addr = str(addr).lower().strip()\n","            self.address_to_labels[clean_addr].add(label)\n","            self.label_to_addresses[label].add(clean_addr)\n","\n","\n","# Main execution\n","def main():\n","    print(\"🚀 Advanced Address Resolution System\")\n","    print(\"Target: 0.80+ score\")\n","\n","    # Load data\n","    train_df = pd.read_csv('train.csv')\n","    test_df = pd.read_csv('test.csv')\n","\n","    print(f\"📊 Data: {len(train_df)} train, {len(test_df)} test\")\n","    print(f\"📊 Unique labels in train: {train_df['label'].nunique()}\")\n","\n","    # Initialize advanced resolver\n","    resolver = AdvancedAddressResolver()\n","\n","    # Train with full dataset\n","    print(\"🎯 Training advanced resolver...\")\n","    resolver.fit(train_df['address'].values, train_df['label'].values)\n","\n","    # Predict on test set\n","    print(\"🔮 Generating advanced predictions...\")\n","    predictions = resolver.predict(test_df['address'].values)\n","\n","    # Create submission\n","    submission = pd.DataFrame({\n","        'id': test_df['id'],\n","        'label': predictions\n","    })\n","\n","    # Analysis\n","    print(f\"\\n📊 Prediction Analysis:\")\n","    print(f\"Unique predictions: {len(set(predictions)):,}\")\n","    print(f\"Prediction distribution:\")\n","    pred_counts = pd.Series(predictions).value_counts()\n","    print(f\"  Top prediction: {pred_counts.iloc[0]:,} ({pred_counts.iloc[0]/len(predictions)*100:.2f}%)\")\n","    print(f\"  Singletons: {(pred_counts == 1).sum():,}\")\n","\n","    # Save\n","    submission.to_csv('advanced_submission.csv', index=False)\n","    print(\"✅ Advanced submission saved!\")\n","\n","    return submission\n","\n","if __name__ == \"__main__\":\n","    submission = main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"id":"HGqSz2_j2pRn","executionInfo":{"status":"error","timestamp":1755850675282,"user_tz":-180,"elapsed":6630,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"dcceac43-4b69-4c75-a6f0-81c34347bfb8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 Advanced Address Resolution System\n","Target: 0.80+ score\n","📊 Data: 848237 train, 217241 test\n","📊 Unique labels in train: 10390\n","🎯 Training advanced resolver...\n","🚀 Advanced Address Resolution Training Started...\n","1️⃣ Building geographic hierarchy...\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'turkey_locations'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-552108770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-552108770.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;31m# Train with full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎯 Training advanced resolver...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m     \u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m# Predict on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-552108770.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, addresses, labels)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1️⃣ Building geographic hierarchy...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mgeographic_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeographic_clusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Stage 2: Train semantic matcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-552108770.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, addresses, labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mparsed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_address_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mparsed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-552108770.py\u001b[0m in \u001b[0;36m_parse_address_components\u001b[0;34m(self, address)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Import comprehensive location data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mturkey_locations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTURKEY_LOCATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_location_in_address\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# All provinces (81 il)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'turkey_locations'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}