{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"3d6374a6c36045bda2de711d163dc89e","deepnote_cell_type":"code","execution_context_id":"67fa407c-b99a-496e-bdb6-d9001a844e01","execution_millis":3064,"execution_start":1756243240244,"source_hash":"6daa6b98","colab":{"base_uri":"https://localhost:8080/"},"id":"HTakwEgNtF7V","executionInfo":{"status":"ok","timestamp":1756393215985,"user_tz":-180,"elapsed":828,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"ec3d0203-dbb8-449c-c2d0-82bdb1ce3154"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (217241, 2) | Test shape: (217241, 2)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score\n","\n","\n","TRAIN_PATH = \"/content/test.csv\"\n","TEST_PATH  = \"/content/test.csv\"\n","\n","train_df = pd.read_csv(TRAIN_PATH)\n","test_df  = pd.read_csv(TEST_PATH)\n","\n","print(f\"Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n","\n"]},{"cell_type":"code","source":["# Önce CSV dosyasını kontrol edelim\n","import pandas as pd\n","\n","# CSV'yi yükle ve ilk birkaç satırı kontrol et\n","train_df = pd.read_csv(\"train.csv\")\n","print(\"Kolon adları:\", train_df.columns.tolist())\n","print(\"\\nİlk 5 satır:\")\n","print(train_df.head())\n","print(\"\\nDataFrame shape:\", train_df.shape)\n","\n","# Eğer kolon adlarında sorun varsa, temizleyelim\n","train_df.columns = train_df.columns.str.strip()  # Boşlukları temizle\n","print(\"\\nTemizlenmiş kolon adları:\", train_df.columns.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDgsFfM5vydK","executionInfo":{"status":"ok","timestamp":1756393220595,"user_tz":-180,"elapsed":1489,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"eea2864a-ed56-42c0-8969-878a41645c22"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Kolon adları: ['address', 'label']\n","\n","İlk 5 satır:\n","                                             address  label\n","0  Akarca Mah. Adnan Menderes Cad. 864.Sok. No:15...   8831\n","1  Cumhuriye Mah. Hükümet Cad. Sivriler İşhanı No...   8810\n","2  İsmet inönü mahallesi 2001 sokak no:2 Çeşme be...   3067\n","3  Gazeteci Hasan Tahsin Caddesi, No:10/3,  Gizem...   8210\n","4  Bitez mahallesi Adnan Menderes caddesi gündonu...   9675\n","\n","DataFrame shape: (848237, 2)\n","\n","Temizlenmiş kolon adları: ['address', 'label']\n"]}]},{"cell_type":"code","source":["TRAIN_PATH = \"train.csv\"\n","TEST_PATH  = \"test.csv\"\n","\n","# 1) Oku\n","train_df = pd.read_csv(TRAIN_PATH, dtype=str, keep_default_na=False)\n","test_df  = pd.read_csv(TEST_PATH,  dtype=str, keep_default_na=False)\n","\n","# 2) Kolon adlarını normalize et: BOM, boşluk, büyük/küçük\n","def _fix_cols(df):\n","    df.columns = (\n","        df.columns.astype(str)\n","                  .str.replace('\\ufeff', '', regex=False)  # BOM temizle\n","                  .str.strip()\n","                  .str.lower()\n","    )\n","    return df\n","\n","train_df = _fix_cols(train_df)\n","test_df  = _fix_cols(test_df)\n","\n","# 3) Eğer testte id yoksa oluştur\n","if \"id\" not in test_df.columns:\n","    test_df[\"id\"] = np.arange(len(test_df))\n","\n","# 4) Beklenen kolon adları yoksa alternatifleri dene (örn. 'cluster_id' → 'label', 'adres' → 'address')\n","alt_map = {}\n","if \"label\" not in train_df.columns:\n","    for cand in [\"labels\", \"cluster_id\", \"cluster\", \"etiket\", \"target\", \"y\"]:\n","        if cand in train_df.columns:\n","            alt_map[cand] = \"label\"; break\n","if \"address\" not in train_df.columns:\n","    for cand in [\"adres\", \"addr\"]:\n","        if cand in train_df.columns:\n","            alt_map[cand] = \"address\"; break\n","if alt_map:\n","    train_df = train_df.rename(columns=alt_map)\n","\n","# 5) Son güvenlik: gerçekten var mı?\n","missing = [c for c in [\"address\", \"label\"] if c not in train_df.columns]\n","if missing:\n","    raise ValueError(f\"train.csv missing columns: {missing}. Found columns: {list(train_df.columns)}\")\n","\n","print(f\"Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n","print(\"Train columns:\", list(train_df.columns))\n","print(\"Test columns:\", list(test_df.columns))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNQqCD3kv5yB","executionInfo":{"status":"ok","timestamp":1756393227356,"user_tz":-180,"elapsed":2018,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"7f55b4e1-b904-4e8f-9d7f-e26165926ca3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (848237, 2) | Test shape: (217241, 2)\n","Train columns: ['address', 'label']\n","Test columns: ['id', 'address']\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"1fc1b73772314d00ace4b679815ad4e0","deepnote_cell_type":"code","execution_context_id":"67fa407c-b99a-496e-bdb6-d9001a844e01","execution_millis":13468,"execution_start":1756243243363,"source_hash":"c6b49833","colab":{"base_uri":"https://localhost:8080/"},"id":"3Ur9c4zotF7Y","executionInfo":{"status":"ok","timestamp":1756393241421,"user_tz":-180,"elapsed":12373,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"599a116b-c97c-4fa8-d49b-19726f94bb06"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 848237/848237 [00:08<00:00, 96066.94it/s]\n","100%|██████████| 217241/217241 [00:02<00:00, 93556.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Toplam sınıf: 10390\n"]}],"source":["def clean_address(addr):\n","    addr = str(addr).lower().strip()\n","    addr = re.sub(r\"[.,;:/\\\\\\-]\", \" \", addr)\n","    addr = re.sub(r\"\\s+\", \" \", addr)\n","    addr = addr.replace(\"mh\", \"mahallesi\").replace(\"cd\", \"cadde\").replace(\"sk\", \"sokak\")\n","    addr = addr.replace(\"blv\", \"bulvarı\").replace(\"ap\", \"apartmanı\").replace(\"no\", \"numara\")\n","    return addr\n","\n","tqdm.pandas()\n","train_df['clean_address'] = train_df['address'].progress_apply(clean_address)\n","test_df['clean_address']  = test_df['address'].progress_apply(clean_address)\n","\n","le = LabelEncoder()\n","train_df['label_enc'] = le.fit_transform(train_df['label'])\n","num_classes = len(le.classes_)\n","print(f\"Toplam sınıf: {num_classes}\")\n","\n","train_idx, val_idx = train_test_split(\n","    np.arange(len(train_df)),\n","    test_size=0.1,\n","    stratify=train_df['label_enc'],\n","    random_state=42\n",")\n","\n","X_train = train_df.iloc[train_idx]['clean_address'].values\n","y_train = train_df.iloc[train_idx]['label_enc'].values\n","X_val   = train_df.iloc[val_idx]['clean_address'].values\n","y_val   = train_df.iloc[val_idx]['label_enc'].values\n","X_test  = test_df['clean_address'].values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"caa8cffd3b7b4325bf378f205da6e3a1","deepnote_cell_type":"code","execution_context_id":"67fa407c-b99a-496e-bdb6-d9001a844e01","execution_millis":31880,"execution_start":1756243256883,"source_hash":"1eb122a3","id":"nVsYTVVotF7Z"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Kernel çökmesini önlemek için max_features = 50000-70000 yeterli\n","tfidf = TfidfVectorizer(max_features=100000, ngram_range=(1,2))\n","X_train_tfidf = tfidf.fit_transform(X_train)\n","X_val_tfidf   = tfidf.transform(X_val)\n","X_test_tfidf  = tfidf.transform(X_test)\n","\n","print(\"TF-IDF hazır!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6d12927dd8a54f9792697da7b7dfed7e","deepnote_cell_type":"code","execution_context_id":"67fa407c-b99a-496e-bdb6-d9001a844e01","execution_millis":674,"execution_start":1756243288814,"source_hash":"6e17bdee","id":"aeRF6wjBtF7Z"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","class AddressDataset(Dataset):\n","    def __init__(self, X, y=None):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = torch.tensor(self.X[idx].toarray(), dtype=torch.float32).squeeze(0)\n","        if self.y is not None:\n","            y = torch.tensor(self.y[idx], dtype=torch.long)\n","            return x, y\n","        else:\n","            return x\n","\n","train_dataset = AddressDataset(X_train_tfidf, y_train)\n","val_dataset   = AddressDataset(X_val_tfidf, y_val)\n","\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=128, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1edf2d5adfcf46b6a7e5e861fce2143b","deepnote_cell_type":"code","execution_context_id":"67fa407c-b99a-496e-bdb6-d9001a844e01","execution_millis":779,"execution_start":1756243289543,"source_hash":"f98bed06","id":"tczaCK-gtF7a"},"outputs":[],"source":["import torch.nn as nn\n","\n","class DeepClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, 1024)\n","        self.bn1 = nn.BatchNorm1d(1024)\n","        self.fc2 = nn.Linear(1024, 512)\n","        self.bn2 = nn.BatchNorm1d(512)\n","        self.fc3 = nn.Linear(512, num_classes)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n","        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n","        x = self.fc3(x)\n","        return x\n","\n","class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes, smoothing=0.1, dim=-1):\n","        super().__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","        self.dim = dim\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=self.dim)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n","\n","model = DeepClassifier(input_dim=100000, num_classes=num_classes).to(device)\n","criterion = LabelSmoothingLoss(classes=num_classes, smoothing=0.1)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e3e8c9e199254f70bf1c03ad670a9d4f","deepnote_cell_type":"code","execution_context_id":"67fa407c-b99a-496e-bdb6-d9001a844e01","execution_millis":14190,"execution_start":1756243290373,"source_hash":"9ae66c86","id":"Qc7MX5PxtF7a"},"outputs":[],"source":["num_epochs = 10\n","\n","for epoch in range(1, num_epochs+1):\n","    model.train()\n","    total_loss = 0\n","    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n","        xb, yb = xb.to(device), yb.to(device)\n","        optimizer.zero_grad()\n","        preds = model(xb)\n","        loss = criterion(preds, yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    # Validation\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for xb, yb in val_loader:\n","            xb = xb.to(device)\n","            preds = model(xb)\n","            preds_labels = preds.argmax(dim=1).cpu().numpy()\n","            all_preds.extend(preds_labels)\n","            all_labels.extend(yb.numpy())\n","\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","    print(f\"Epoch {epoch}/{num_epochs} | Train Loss: {total_loss/len(train_loader):.4f} | Val Macro F1: {f1:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"834b5e5ef7264fa3b4cb0a317fde7ba1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"id":"lOc5VQPbtF7b"},"outputs":[],"source":["test_dataset = AddressDataset(X_test_tfidf)\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","\n","model.eval()\n","test_preds = []\n","\n","with torch.no_grad():\n","    for xb in tqdm(test_loader, desc=\"Test Prediction\"):\n","        xb = xb.to(device)\n","        preds = model(xb)\n","        preds_labels = preds.argmax(dim=1).cpu().numpy()\n","        test_preds.extend(preds_labels)\n","\n","test_labels = le.inverse_transform(test_preds)\n","\n","submission_df = pd.DataFrame({\n","    \"id\": test_df[\"id\"],\n","    \"label\": test_labels\n","})\n","\n","submission_df.to_csv(\"submission_0.7plus.csv\", index=False)\n","print(\"✅ Submission dosyası oluşturuldu\")\n"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","id":"_2TTgg1JtF7b"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=34374b7c-5b06-4c6f-aef0-649afe004105' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]},{"cell_type":"code","source":["!pip install unidecode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8ItWeSjxiV9","executionInfo":{"status":"ok","timestamp":1756391641720,"user_tz":-180,"elapsed":7317,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"c3c9daae-5590-4ca5-d853-ea4fef37d698"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n","Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.4.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from tqdm import tqdm\n","from unidecode import unidecode\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# ====================== DATA LOADING ======================\n","TRAIN_PATH = \"/content/train.csv\"\n","TEST_PATH = \"/content/test.csv\"\n","\n","train_df = pd.read_csv(TRAIN_PATH)\n","test_df = pd.read_csv(TEST_PATH)\n","\n","print(f\"Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n","print(f\"Train columns: {train_df.columns.tolist()}\")\n","print(f\"Test columns: {test_df.columns.tolist()}\")\n","\n","# ====================== ENHANCED PREPROCESSING ======================\n","\n","class TurkishAddressPreprocessor:\n","    def __init__(self):  # ÖNEMLİ: __init__ olmalı, _init_ değil!\n","        # Comprehensive abbreviation dictionary\n","        self.abbreviations = {\n","            # Mahalle variations\n","            'mah': 'mahallesi', 'mh': 'mahallesi', 'mah.': 'mahallesi', 'mh.': 'mahallesi',\n","            'mahalle': 'mahallesi', 'mahal': 'mahallesi',\n","\n","            # Cadde variations\n","            'cad': 'caddesi', 'cd': 'caddesi', 'cad.': 'caddesi', 'cd.': 'caddesi',\n","            'cadde': 'caddesi', 'cadd': 'caddesi',\n","\n","            # Sokak variations\n","            'sok': 'sokak', 'sk': 'sokak', 'sok.': 'sokak', 'sk.': 'sokak',\n","            'sokağı': 'sokak', 'sokagi': 'sokak',\n","\n","            # Bulvar variations\n","            'blv': 'bulvari', 'bulv': 'bulvari', 'bulvar': 'bulvari',\n","\n","            # Apartman variations\n","            'apt': 'apartmani', 'ap': 'apartmani', 'apt.': 'apartmani',\n","            'apartman': 'apartmani', 'apart': 'apartmani',\n","\n","            # Other common abbreviations\n","            'no': 'numara', 'no.': 'numara', 'no:': 'numara',\n","            'd': 'daire', 'd.': 'daire', 'dai': 'daire', 'dai.': 'daire',\n","            'k': 'kat', 'k.': 'kat',\n","            'bl': 'blok', 'blk': 'blok',\n","            'sit': 'sitesi', 'sit.': 'sitesi',\n","        }\n","\n","    def normalize_turkish_chars(self, text):\n","        \"\"\"Normalize Turkish characters to ASCII equivalents\"\"\"\n","        # Turkish character mappings\n","        tr_map = {\n","            'ı': 'i', 'İ': 'i', 'I': 'i',\n","            'ğ': 'g', 'Ğ': 'g',\n","            'ü': 'u', 'Ü': 'u',\n","            'ş': 's', 'Ş': 's',\n","            'ö': 'o', 'Ö': 'o',\n","            'ç': 'c', 'Ç': 'c',\n","        }\n","\n","        for tr_char, ascii_char in tr_map.items():\n","            text = text.replace(tr_char, ascii_char)\n","\n","        return text\n","\n","    def clean_address(self, addr):\n","        \"\"\"Enhanced address cleaning with Turkish language support\"\"\"\n","        # Convert to string and lowercase\n","        addr = str(addr).lower().strip()\n","\n","        # Normalize Turkish characters\n","        addr = self.normalize_turkish_chars(addr)\n","\n","        # Remove extra whitespace first\n","        addr = re.sub(r'\\s+', ' ', addr)\n","\n","        # Normalize number formats before punctuation removal\n","        # Handle \"No:5\", \"No 5\", \"no.5\" patterns\n","        addr = re.sub(r'no[\\s:.\\-]*(\\d+)', r'numara \\1', addr)\n","        addr = re.sub(r'(\\d+)[\\s]*no\\b', r'numara \\1', addr)\n","\n","        # Handle \"K:2 D:4\" patterns\n","        addr = re.sub(r'k[\\s:.\\-]*(\\d+)', r'kat \\1', addr)\n","        addr = re.sub(r'd[\\s:.\\-]*(\\d+)', r'daire \\1', addr)\n","\n","        # Remove punctuation\n","        addr = re.sub(r'[.,;:/\\\\\\-]', ' ', addr)\n","\n","        # Expand abbreviations with word boundaries\n","        words = addr.split()\n","        expanded_words = []\n","\n","        for word in words:\n","            # Check if word is an abbreviation\n","            if word in self.abbreviations:\n","                expanded_words.append(self.abbreviations[word])\n","            else:\n","                # Check without trailing dots\n","                word_clean = word.rstrip('.')\n","                if word_clean in self.abbreviations:\n","                    expanded_words.append(self.abbreviations[word_clean])\n","                else:\n","                    expanded_words.append(word)\n","\n","        addr = ' '.join(expanded_words)\n","\n","        # Remove duplicate spaces again\n","        addr = re.sub(r'\\s+', ' ', addr)\n","\n","        # Remove duplicate words (e.g., \"narlidere narlidere\")\n","        words = addr.split()\n","        seen = set()\n","        unique_words = []\n","        for word in words:\n","            if word not in seen or word.isdigit():\n","                unique_words.append(word)\n","                seen.add(word)\n","\n","        addr = ' '.join(unique_words)\n","\n","        return addr.strip()\n","\n","# Initialize preprocessor\n","preprocessor = TurkishAddressPreprocessor()\n","\n","# Apply preprocessing\n","tqdm.pandas()\n","train_df['clean_address'] = train_df['address'].progress_apply(preprocessor.clean_address)\n","test_df['clean_address'] = test_df['address'].progress_apply(preprocessor.clean_address)\n","\n","# Show some examples\n","print(\"\\n=== Preprocessing Examples ===\")\n","for i in range(5):\n","    idx = np.random.randint(len(train_df))\n","    print(f\"\\nOriginal: {train_df.iloc[idx]['address']}\")\n","    print(f\"Cleaned:  {train_df.iloc[idx]['clean_address']}\")\n","\n","# ====================== LABEL ENCODING ======================\n","le = LabelEncoder()\n","train_df['label_enc'] = le.fit_transform(train_df['label'])\n","num_classes = len(le.classes_)\n","print(f\"\\nToplam sınıf: {num_classes}\")\n","\n","# ====================== TRAIN/VAL SPLIT ======================\n","train_idx, val_idx = train_test_split(\n","    np.arange(len(train_df)),\n","    test_size=0.1,\n","    stratify=train_df['label_enc'],\n","    random_state=42\n",")\n","\n","X_train = train_df.iloc[train_idx]['clean_address'].values\n","y_train = train_df.iloc[train_idx]['label_enc'].values\n","X_val = train_df.iloc[val_idx]['clean_address'].values\n","y_val = train_df.iloc[val_idx]['label_enc'].values\n","X_test = test_df['clean_address'].values\n","\n","# ====================== TF-IDF VECTORIZATION ======================\n","# Reduced max_features thanks to better preprocessing\n","tfidf = TfidfVectorizer(\n","    max_features=100000,  # Reduced from 1M to 100K\n","    ngram_range=(1, 3),\n","    min_df=2,           # Ignore very rare terms\n","    max_df=0.99,        # Ignore very common terms\n","    sublinear_tf=True,  # Use sublinear scaling\n","    strip_accents='unicode'\n",")\n","\n","print(\"\\nCreating TF-IDF features...\")\n","X_train_tfidf = tfidf.fit_transform(X_train)\n","X_val_tfidf = tfidf.transform(X_val)\n","X_test_tfidf = tfidf.transform(X_test)\n","\n","print(f\"TF-IDF shape: {X_train_tfidf.shape}\")\n","print(f\"Vocabulary size reduced to: {len(tfidf.vocabulary_)}\")\n","\n","# ====================== PYTORCH DATASET & MODEL ======================\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"\\nDevice: {device}\")\n","\n","class AddressDataset(Dataset):\n","    def __init__(self, X, y=None):  # ÇİFT ALT ÇİZGİ!\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):  # ÇİFT ALT ÇİZGİ!\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):  # ÇİFT ALT ÇİZGİ!\n","        x = torch.tensor(self.X[idx].toarray(), dtype=torch.float32).squeeze(0)\n","        if self.y is not None:\n","            y = torch.tensor(self.y[idx], dtype=torch.long)\n","            return x, y\n","        else:\n","            return x\n","\n","# Create datasets\n","train_dataset = AddressDataset(X_train_tfidf, y_train)\n","val_dataset = AddressDataset(X_val_tfidf, y_val)\n","\n","# Create dataloaders with larger batch size (thanks to reduced features)\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)\n","\n","class DeepClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):  # ÇİFT ALT ÇİZGİ!\n","        super().__init__()  # ÇİFT ALT ÇİZGİ!\n","        self.fc1 = nn.Linear(input_dim, 1024)\n","        self.bn1 = nn.BatchNorm1d(1024)\n","        self.fc2 = nn.Linear(1024, 512)\n","        self.bn2 = nn.BatchNorm1d(512)\n","        self.fc3 = nn.Linear(512, 256)\n","        self.bn3 = nn.BatchNorm1d(256)\n","        self.fc4 = nn.Linear(256, num_classes)\n","\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n","        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n","        x = self.dropout(self.relu(self.bn3(self.fc3(x))))\n","        x = self.fc4(x)\n","        return x\n","\n","class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes, smoothing=0.1, dim=-1):  # ÇİFT ALT ÇİZGİ!\n","        super().__init__()  # ÇİFT ALT ÇİZGİ!\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","        self.dim = dim\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=self.dim)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n","\n","# ====================== MODEL TRAINING ======================\n","# Initialize model with correct input dimension\n","input_dim = X_train_tfidf.shape[1]  # This will be 100000 or less\n","model = DeepClassifier(input_dim=input_dim, num_classes=num_classes).to(device)\n","\n","criterion = LabelSmoothingLoss(classes=num_classes, smoothing=0.1)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","\n","# Training parameters\n","num_epochs = 15\n","best_f1 = 0\n","patience = 3\n","patience_counter = 0\n","\n","print(\"\\n=== Starting Training ===\")\n","for epoch in range(1, num_epochs + 1):\n","    # Training phase\n","    model.train()\n","    total_loss = 0\n","    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n","\n","    for xb, yb in train_pbar:\n","        xb, yb = xb.to(device), yb.to(device)\n","        optimizer.zero_grad()\n","        preds = model(xb)\n","        loss = criterion(preds, yb)\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","        total_loss += loss.item()\n","        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","\n","    # Validation phase\n","    model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\")\n","        for xb, yb in val_pbar:\n","            xb = xb.to(device)\n","            preds = model(xb)\n","            preds_labels = preds.argmax(dim=1).cpu().numpy()\n","            all_preds.extend(preds_labels)\n","            all_labels.extend(yb.numpy())\n","\n","    # Calculate metrics\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","\n","    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Macro F1: {f1:.4f}\")\n","\n","    # Learning rate scheduling\n","    scheduler.step()\n","\n","    # Early stopping\n","    if f1 > best_f1:\n","        best_f1 = f1\n","        patience_counter = 0\n","        # Save best model\n","        torch.save(model.state_dict(), 'best_model.pth')\n","        print(f\"New best F1: {best_f1:.4f} - Model saved!\")\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered. Best F1: {best_f1:.4f}\")\n","            break\n","\n","# Load best model for prediction\n","model.load_state_dict(torch.load('best_model.pth'))\n","\n","# ====================== TEST PREDICTION ======================\n","test_dataset = AddressDataset(X_test_tfidf)\n","test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)\n","\n","model.eval()\n","test_preds = []\n","\n","print(\"\\n=== Making Test Predictions ===\")\n","with torch.no_grad():\n","    for xb in tqdm(test_loader, desc=\"Test Prediction\"):\n","        xb = xb.to(device)\n","        preds = model(xb)\n","        preds_labels = preds.argmax(dim=1).cpu().numpy()\n","        test_preds.extend(preds_labels)\n","\n","# Convert predictions back to original labels\n","test_labels = le.inverse_transform(test_preds)\n","\n","# ====================== CREATE SUBMISSION ======================\n","submission_df = pd.DataFrame({\n","    \"id\": test_df[\"id\"],\n","    \"label\": test_labels\n","})\n","\n","submission_df.to_csv(\"submission_optimized.csv\", index=False)\n","print(\"\\n✅ Submission file created: submission_optimized.csv\")\n","\n","# ====================== ANALYSIS ======================\n","print(\"\\n=== Preprocessing Impact Analysis ===\")\n","print(f\"Original average length: {train_df['address'].str.len().mean():.1f} chars\")\n","print(f\"Cleaned average length: {train_df['clean_address'].str.len().mean():.1f} chars\")\n","print(f\"Reduction: {(1 - train_df['clean_address'].str.len().mean() / train_df['address'].str.len().mean()) * 100:.1f}%\")\n","\n","print(f\"\\nVocabulary size: {len(tfidf.vocabulary_):,}\")\n","print(f\"Memory usage reduced by ~90% compared to 1M features\")\n","print(f\"Training speed increased by using batch size 256 instead of 128\")\n","\n","# Show some test predictions\n","print(\"\\n=== Sample Test Predictions ===\")\n","for i in range(5):\n","    idx = np.random.randint(len(test_df))\n","    print(f\"\\nTest Address: {test_df.iloc[idx]['address']}\")\n","    print(f\"Cleaned: {test_df.iloc[idx]['clean_address']}\")\n","    print(f\"Predicted Label: {submission_df.iloc[idx]['label']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6IGFOCrxVQt","executionInfo":{"status":"ok","timestamp":1756398183995,"user_tz":-180,"elapsed":4798541,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"713c7b41-8a13-4c1b-824d-b570820af11f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (848237, 2) | Test shape: (217241, 2)\n","Train columns: ['address', 'label']\n","Test columns: ['id', 'address']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 848237/848237 [00:26<00:00, 32089.22it/s]\n","100%|██████████| 217241/217241 [00:06<00:00, 32000.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","=== Preprocessing Examples ===\n","\n","Original: kemer mah Ayko 3. Caddesi 14/2 kat 2 daire 4 Efeler\n","Cleaned:  kemer mahallesi ayko 3 caddesi 14 2 kat 2 daire 4 efeler\n","\n","Original: 6417/1 sokak No10 Daire5 Kat3 Yalı mahallesi- İzmir. Kuaför’ ün olduğu bina girişi. KARŞIYAKA / İZMİR\n","Cleaned:  6417 1 sokak numara 10 daire5 kat3 yali mahallesi i̇zmir kuafor’ un oldugu bina girisi karsiyaka i̇zmi̇r\n","\n","Original: Muğla bodrum umurca mahallesi özgün sokak no 9 daire1\n","Cleaned:  mugla bodrum umurca mahallesi ozgun sokak numara 9 daire1\n","\n","Original: UĞUR MUMCU MAH. 1301 SK. NO: 10 D: 23 35660 MENEMEN / İZMİR\n","Cleaned:  ugur mumcu mahallesi 1301 sokak numara 10 daire 23 35660 menemen i̇zmi̇r\n","\n","Original: Atatürk Mah Atatürk mahallesi 1039.sok no.6 daire 7\n","Cleaned:  ataturk mahallesi 1039 sokak numara 6 daire 7\n","\n","Toplam sınıf: 10390\n","\n","Creating TF-IDF features...\n","TF-IDF shape: (763413, 100000)\n","Vocabulary size reduced to: 100000\n","\n","Device: cuda\n","\n","=== Starting Training ===\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/15 [Train]: 100%|██████████| 2983/2983 [04:49<00:00, 10.32it/s, loss=3.6178]\n","Epoch 1/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Train Loss: 5.2522 | Val Macro F1: 0.3680\n","New best F1: 0.3680 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/15 [Train]: 100%|██████████| 2983/2983 [04:49<00:00, 10.31it/s, loss=3.6777]\n","Epoch 2/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Train Loss: 3.7500 | Val Macro F1: 0.4362\n","New best F1: 0.4362 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.36it/s, loss=3.6003]\n","Epoch 3/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Train Loss: 3.4136 | Val Macro F1: 0.4787\n","New best F1: 0.4787 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.34it/s, loss=3.4712]\n","Epoch 4/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Train Loss: 3.1844 | Val Macro F1: 0.5088\n","New best F1: 0.5088 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.33it/s, loss=3.6675]\n","Epoch 5/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Train Loss: 2.9818 | Val Macro F1: 0.5392\n","New best F1: 0.5392 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.35it/s, loss=3.4354]\n","Epoch 6/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 | Train Loss: 2.7832 | Val Macro F1: 0.5652\n","New best F1: 0.5652 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/15 [Train]: 100%|██████████| 2983/2983 [04:47<00:00, 10.36it/s, loss=3.4368]\n","Epoch 7/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 | Train Loss: 2.5897 | Val Macro F1: 0.5898\n","New best F1: 0.5898 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/15 [Train]: 100%|██████████| 2983/2983 [04:47<00:00, 10.37it/s, loss=2.7178]\n","Epoch 8/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 | Train Loss: 2.4181 | Val Macro F1: 0.6066\n","New best F1: 0.6066 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/15 [Train]: 100%|██████████| 2983/2983 [04:47<00:00, 10.38it/s, loss=2.7501]\n","Epoch 9/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 | Train Loss: 2.2861 | Val Macro F1: 0.6157\n","New best F1: 0.6157 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.36it/s, loss=2.3491]\n","Epoch 10/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 | Train Loss: 2.2112 | Val Macro F1: 0.6194\n","New best F1: 0.6194 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.35it/s, loss=2.3777]\n","Epoch 11/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11 | Train Loss: 2.1863 | Val Macro F1: 0.6198\n","New best F1: 0.6198 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.34it/s, loss=2.4934]\n","Epoch 12/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12 | Train Loss: 2.1949 | Val Macro F1: 0.6219\n","New best F1: 0.6219 - Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.33it/s, loss=2.5334]\n","Epoch 13/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13 | Train Loss: 2.2141 | Val Macro F1: 0.6216\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/15 [Train]: 100%|██████████| 2983/2983 [04:48<00:00, 10.33it/s, loss=2.6070]\n","Epoch 14/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14 | Train Loss: 2.2670 | Val Macro F1: 0.6160\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/15 [Train]: 100%|██████████| 2983/2983 [04:49<00:00, 10.32it/s, loss=2.2945]\n","Epoch 15/15 [Val]: 100%|██████████| 332/332 [00:21<00:00, 15.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15 | Train Loss: 2.3901 | Val Macro F1: 0.6028\n","Early stopping triggered. Best F1: 0.6219\n","\n","=== Making Test Predictions ===\n"]},{"output_type":"stream","name":"stderr","text":["Test Prediction: 100%|██████████| 849/849 [00:53<00:00, 16.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Submission file created: submission_optimized.csv\n","\n","=== Preprocessing Impact Analysis ===\n","Original average length: 65.0 chars\n","Cleaned average length: 67.8 chars\n","Reduction: -4.2%\n","\n","Vocabulary size: 100,000\n","Memory usage reduced by ~90% compared to 1M features\n","Training speed increased by using batch size 256 instead of 128\n","\n","=== Sample Test Predictions ===\n","\n","Test Address: 705 sk no 6/1 şehitler mah\n","Cleaned: 705 sokak numara 6 1 sehitler mahallesi\n","Predicted Label: 7719\n","\n","Test Address: osmaniye mah.2.taban sokak no9daire2 kınık/izmir\n","Cleaned: osmaniye mahallesi 2 taban sokak numara 9daire2 kinik izmir\n","Predicted Label: 8513\n","\n","Test Address: Kazımdirik mahallesi 364/8 sok no:4 lumos coffee Lumos coffee avcılar next altı\n","Cleaned: kazimdirik mahallesi 364 8 sokak numara 4 lumos coffee avcilar next alti\n","Predicted Label: 5512\n","\n","Test Address: ORTAKENT YAHŞİ MAH. KARGI CADDESİ SÜMBÜL SOKAK NO 51 CAMEL BEACH REZİDANS\n","Cleaned: ortakent yahsi̇ mahallesi kargi caddesi̇ sumbul sokak numara 51 camel beach rezi̇dans\n","Predicted Label: 9614\n","\n","Test Address: CUMHURİYET MAHALLESİ 5013 SOKAK NO 3 DAİRE 13 ARILAR E BLOK\n","Cleaned: cumhuri̇yet mahallesi̇ 5013 sokak numara 3 dai̇re 13 arilar e blok\n","Predicted Label: 1935\n"]}]}],"metadata":{"deepnote_notebook_id":"739edc796fc242458436be9a60aff74b","language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}