{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28","authorship_tag":"ABX9TyNA3vjdTGdqr1cSJFLwWXlo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# Teknofest x Hepsiburada Address Matching Pipeline\n","# Complete ML solution for Turkish address deduplication and matching\n","\n","# ================================================================\n","# STEP 0: SETUP AND INSTALLATIONS\n","# ================================================================\n","!pip install sentence-transformers faiss-cpu lightgbm scikit-learn pandas numpy\n","!pip install unidecode regex tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND4fXTWfIj41","executionInfo":{"status":"ok","timestamp":1756274789044,"user_tz":-180,"elapsed":16870,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"4caa072e-3d46-4293-942a-43600fb6d0ff"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n","Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.12.0\n","Collecting unidecode\n","  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.4.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import string\n","from collections import defaultdict, Counter\n","import pickle\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import GroupKFold\n","from sklearn.preprocessing import LabelEncoder\n","import lightgbm as lgb\n","\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","from unidecode import unidecode\n","from tqdm import tqdm\n","import gc\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n"],"metadata":{"id":"aiQuSAg_IzoE","executionInfo":{"status":"ok","timestamp":1756275236358,"user_tz":-180,"elapsed":28318,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ================================================================\n","# STEP 1: DATA PREPROCESSING & NORMALIZATION\n","# ================================================================\n","\n","class TurkishAddressNormalizer:\n","    \"\"\"Comprehensive Turkish address normalizer\"\"\"\n","\n","    def __init__(self):\n","        # Turkish abbreviation mappings\n","        self.abbreviations = {\n","            # Neighborhood/District\n","            'mh': 'mahallesi', 'mah': 'mahallesi', 'mahalle': 'mahallesi',\n","\n","            # Street types\n","            'cd': 'caddesi', 'cad': 'caddesi', 'cadde': 'caddesi',\n","            'sk': 'sokagi', 'sok': 'sokagi', 'sokak': 'sokagi',\n","            'blv': 'bulvari', 'bulv': 'bulvari', 'bulvar': 'bulvari',\n","            'osb': 'organize sanayi bolge', 'km': 'kilometre',\n","\n","            # Building types\n","            'apt': 'apartmani', 'ap': 'apartmani', 'apartman': 'apartmani',\n","            'sit': 'sitesi', 'site': 'sitesi',\n","            'blk': 'blok', 'blok': 'blok',\n","            'plz': 'plaza', 'plaza': 'plaza',\n","            'avm': 'alisveris merkezi',\n","\n","            # Address components\n","            'no': 'numara', 'nu': 'numara',\n","            'kt': 'kat', 'kat': 'kat',\n","            'dr': 'daire', 'daire': 'daire', 'da': 'daire',\n","            'pst': 'posta kodu',\n","\n","            # Directions\n","            'kz': 'kuzey', 'gy': 'guney', 'dt': 'dogu', 'bt': 'bati',\n","\n","            # Common words\n","            'yrm': 'yurdu', 'otel': 'oteli', 'hst': 'hastanesi',\n","            'unv': 'universitesi', 'lise': 'lisesi', 'okl': 'okulu'\n","        }\n","\n","        # Regex patterns for component extraction\n","        self.patterns = {\n","            'number': r'(?:no[:\\s]*|numara[:\\s]*|n[:\\s]*)?(\\d+)(?:[/\\-](\\d+))?',\n","            'floor': r'(?:kat[:\\s]*|kt[:\\s]*|k[:\\s]*)?(\\d+)(?:\\s*\\.?\\s*kat)?',\n","            'apartment': r'(?:daire[:\\s]*|dr[:\\s]*|d[:\\s]*)?(\\d+)(?:\\s*\\.?\\s*daire)?',\n","            'block': r'(?:blok[:\\s]*|blk[:\\s]*|b[:\\s]*)?([a-zA-Z]?\\d*)(?:\\s*\\.?\\s*blok)?',\n","            'postal_code': r'\\b(\\d{5})\\b'\n","        }\n","\n","    def normalize_turkish_chars(self, text):\n","        \"\"\"Convert Turkish characters and remove diacritics\"\"\"\n","        turkish_chars = {\n","            'ç': 'c', 'ğ': 'g', 'ı': 'i', 'ö': 'o', 'ş': 's', 'ü': 'u',\n","            'Ç': 'C', 'Ğ': 'G', 'İ': 'I', 'Ö': 'O', 'Ş': 'S', 'Ü': 'U'\n","        }\n","        for tr_char, en_char in turkish_chars.items():\n","            text = text.replace(tr_char, en_char)\n","        return text\n","\n","    def expand_abbreviations(self, text):\n","        \"\"\"Expand common Turkish abbreviations\"\"\"\n","        words = text.split()\n","        expanded_words = []\n","\n","        for word in words:\n","            # Remove punctuation for matching\n","            clean_word = word.strip('.,;:()-').lower()\n","            if clean_word in self.abbreviations:\n","                expanded_words.append(self.abbreviations[clean_word])\n","            else:\n","                expanded_words.append(word)\n","\n","        return ' '.join(expanded_words)\n","\n","    def standardize_numbers(self, text):\n","        \"\"\"Standardize number formats\"\"\"\n","        # Handle \"No:12\", \"No=12\", \"12/3\" patterns\n","        text = re.sub(r'no[:\\s=]*(\\d+)', r'numara \\1', text, flags=re.IGNORECASE)\n","        text = re.sub(r'(\\d+)[/\\-](\\d+)', r'numara \\1 daire \\2', text)\n","\n","        # Handle floor patterns\n","        text = re.sub(r'(\\d+)\\.?\\s*kat', r'\\1 kat', text, flags=re.IGNORECASE)\n","\n","        # Handle apartment patterns\n","        text = re.sub(r'(\\d+)\\.?\\s*daire', r'\\1 daire', text, flags=re.IGNORECASE)\n","\n","        return text\n","\n","    def clean_punctuation(self, text):\n","        \"\"\"Remove unnecessary punctuation and normalize spacing\"\"\"\n","        # Remove special characters but keep Turkish letters\n","        text = re.sub(r'[^\\w\\sçğıöşüÇĞİÖŞÜ]', ' ', text)\n","\n","        # Normalize whitespace\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        return text.strip()\n","\n","    def extract_components(self, text):\n","        \"\"\"Extract address components using regex\"\"\"\n","        components = {}\n","\n","        for component, pattern in self.patterns.items():\n","            match = re.search(pattern, text, re.IGNORECASE)\n","            if match:\n","                if component == 'number' and match.group(2):\n","                    components['number'] = match.group(1)\n","                    components['apartment'] = match.group(2)\n","                else:\n","                    components[component] = match.group(1)\n","\n","        return components\n","\n","    def normalize(self, address):\n","        \"\"\"Apply full normalization pipeline\"\"\"\n","        if pd.isna(address) or not isinstance(address, str):\n","            return \"\"\n","\n","        # Convert to lowercase\n","        address = address.lower()\n","\n","        # Normalize Turkish characters\n","        address = self.normalize_turkish_chars(address)\n","\n","        # Expand abbreviations\n","        address = self.expand_abbreviations(address)\n","\n","        # Standardize numbers\n","        address = self.standardize_numbers(address)\n","\n","        # Clean punctuation\n","        address = self.clean_punctuation(address)\n","\n","        return address\n","\n","# Initialize normalizer\n","normalizer = TurkishAddressNormalizer()"],"metadata":{"id":"HjV_HkV4I5oM","executionInfo":{"status":"ok","timestamp":1756275256763,"user_tz":-180,"elapsed":69,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v76cCCI4IDzl","executionInfo":{"status":"ok","timestamp":1756275285977,"user_tz":-180,"elapsed":25647,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"7258a839-cd39-4090-c7f8-fe3b0f182af1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading datasets...\n","Train shape: (848237, 2)\n","Test shape: (217241, 2)\n","Unique labels in train: 10390\n","Normalizing addresses...\n","After cleaning - Train: 848234, Test: 217241\n"]}],"source":["# ================================================================\n","# STEP 2: DATA LOADING AND PREPROCESSING\n","# ================================================================\n","\n","def load_and_preprocess_data():\n","    \"\"\"Load and preprocess the datasets\"\"\"\n","    print(\"Loading datasets...\")\n","\n","    # Load data (adjust paths as needed)\n","    train_df = pd.read_csv('train.csv')\n","    test_df = pd.read_csv('test.csv')\n","\n","    print(f\"Train shape: {train_df.shape}\")\n","    print(f\"Test shape: {test_df.shape}\")\n","    print(f\"Unique labels in train: {train_df['label'].nunique()}\")\n","\n","    # Normalize addresses\n","    print(\"Normalizing addresses...\")\n","    train_df['normalized_address'] = train_df['address'].apply(normalizer.normalize)\n","    test_df['normalized_address'] = test_df['address'].apply(normalizer.normalize)\n","\n","    # Remove empty addresses\n","    train_df = train_df[train_df['normalized_address'].str.len() > 0].reset_index(drop=True)\n","    test_df = test_df[test_df['normalized_address'].str.len() > 0].reset_index(drop=True)\n","\n","    print(f\"After cleaning - Train: {len(train_df)}, Test: {len(test_df)}\")\n","\n","    return train_df, test_df\n","\n","# Load data\n","train_df, test_df = load_and_preprocess_data()"]},{"cell_type":"code","source":["# ================================================================\n","# STEP 3: BASELINE MODEL - TF-IDF CENTROID\n","# ================================================================\n","\n","class TFIDFCentroidModel:\n","    \"\"\"TF-IDF based centroid classifier\"\"\"\n","\n","    def __init__(self, ngram_range=(2, 11), max_features=None):\n","        self.vectorizer = TfidfVectorizer(\n","            analyzer='char',\n","            ngram_range=ngram_range,\n","            max_features=max_features,\n","            lowercase=True,\n","            strip_accents='unicode',\n","            # YENİ PARAMETRELER:\n","            sublinear_tf=True,      # TF = 1 + log(TF) - büyük farkları azaltır\n","            min_df=2,               # En az 2 dokümanda geçmeli - noise azaltır\n","            max_df=1,            # Çok yaygın terimleri çıkar (%95'ten fazla)\n","            norm='l2',              # L2 normalizasyon\n","            use_idf=True,           # IDF ağırlıklandırma\n","            smooth_idf=True,        # IDF smoothing\n","            dtype=np.float32        # Bellek optimizasyonu\n","        )\n","        self.label_encoder = LabelEncoder()\n","        self.centroids = None\n","        self.labels = None\n","\n","    def fit(self, addresses, labels):\n","        \"\"\"Fit the model on training data\"\"\"\n","        print(\"Fitting TF-IDF vectorizer...\")\n","\n","        # Encode labels\n","        encoded_labels = self.label_encoder.fit_transform(labels)\n","        self.labels = self.label_encoder.classes_\n","\n","        # Vectorize addresses\n","        X = self.vectorizer.fit_transform(addresses)\n","\n","        # Compute centroids for each label\n","        print(\"Computing centroids...\")\n","        self.centroids = np.zeros((len(self.labels), X.shape[1]))\n","\n","        for i, label in enumerate(self.labels):\n","            mask = encoded_labels == i\n","            if mask.sum() > 0:\n","                self.centroids[i] = X[mask].mean(axis=0).A1\n","\n","        print(f\"Model fitted with {len(self.labels)} unique labels\")\n","\n","    def predict(self, addresses, top_k=1):\n","        \"\"\"Predict labels for new addresses\"\"\"\n","        X = self.vectorizer.transform(addresses)\n","\n","        # Compute similarities to centroids\n","        similarities = cosine_similarity(X, self.centroids)\n","\n","        if top_k == 1:\n","            predictions = similarities.argmax(axis=1)\n","            return self.label_encoder.inverse_transform(predictions)\n","        else:\n","            # Return top-k predictions\n","            top_indices = np.argsort(similarities, axis=1)[:, -top_k:][:, ::-1]\n","            top_labels = []\n","            for indices in top_indices:\n","                top_labels.append(self.label_encoder.inverse_transform(indices))\n","            return top_labels, similarities\n","\n","# Train TF-IDF baseline\n","print(\"Training TF-IDF baseline model...\")\n","tfidf_model = TFIDFCentroidModel()\n","tfidf_model.fit(train_df['normalized_address'], train_df['label'])\n","\n","# Predict on test set\n","print(\"Making TF-IDF predictions...\")\n","tfidf_predictions = tfidf_model.predict(test_df['normalized_address'])\n","\n","# Create baseline submission\n","baseline_submission = pd.DataFrame({\n","    'id': test_df['id'],\n","    'label': tfidf_predictions\n","})\n","baseline_submission.to_csv('baseline_submission.csv', index=False)\n","print(\"Baseline submission saved!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSjlkk4VJEs9","executionInfo":{"status":"ok","timestamp":1756275789327,"user_tz":-180,"elapsed":488071,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"d68d7975-480c-4a00-f2d3-89ad2b8fd0cb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Training TF-IDF baseline model...\n","Fitting TF-IDF vectorizer...\n","Computing centroids...\n","Model fitted with 10390 unique labels\n","Making TF-IDF predictions...\n","Baseline submission saved!\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","address_matcher_geo_centroid.py\n","\n","Gelistirilmis Adres Cozumleme (Turkiye)\n","- Char + Word TF-IDF (birlesik) + L2 normalize\n","- Label basina \"centroid\" (prototip) vektorleri (seyrek csr)\n","- GeoDatabase tabanli aday daraltma (il/ilce/posta kodu)\n","- Esnek parametreler (n-gram, min_df, max_features, agirliklar vs.)\n","- Kolay kullanim: fit(train_df), predict(list[str]), evaluate(dev_df)\n","\n","Girdi veri formati:\n","- train_df: columns = ['address', 'label']  (label: int veya str)\n","- dev_df  : columns = ['id','address','label'] veya ['address','label']\n","\n","Not: GeoDatabase icin 'geo_database.py' dosyasini ayni klasorde bulundurun.\n","\"\"\"\n","\n","from __future__ import annotations\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","from typing import List, Dict, Tuple, Optional, Union\n","\n","from scipy.sparse import hstack, csr_matrix, issparse, vstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import accuracy_score\n","\n","# Yerel modul: GeoDatabase\n","try:\n","    from geo_database import GeoDatabase\n","except Exception as e:\n","    raise ImportError(\"geo_database.py ayni klasorde olmali: from geo_database import GeoDatabase\")\n","\n","\n","# -----------------------------\n","# Yardimci: metin standardizasyonu\n","# -----------------------------\n","def _normalize_whitespace(s: str) -> str:\n","    s = re.sub(r\"[,\\.;:]+\", \" \", s)            # yogun noktalamayi bosluk yap\n","    s = re.sub(r\"[\\/|]+\", \" / \", s)            # bolu isaretleri ayrik kalsin\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","_CANON_MAP = [\n","    (r\"\\bmahalles(i|i\\.|i:|i,)?\\b|\\bmah(\\.|:)?\\b\", \" mah \"),\n","    (r\"\\bcaddes(i|i\\.|i:|i,)?\\b|\\bcad(\\.|:)?\\b\", \" cad \"),\n","    (r\"\\bsoka(\\u011f\\u0131|gi|k|k\\.|k:|k,)?\\b|\\bsok(\\.|:)?\\b\", \" sok \"),\n","    (r\"\\bblv(t|d)?(\\.|:)?\\b|\\bbulvar(i|\\u0131)?\\b\", \" bulvar \"),\n","    (r\"\\bno(\\.|:)?\\b\", \" no \"),\n","    (r\"\\bkat(\\.|:)?\\b\", \" kat \"),\n","    (r\"\\bdair(e|e\\.|e:)?\\b|\\bd\\.\\b|\\bd:(?=\\d)\", \" daire \"),\n","    (r\"\\bapt(\\.|:)?\\b|\\bapartman(i|\\u0131)?\\b\", \" apartman \"),\n","    (r\"\\bmevk[ii](i|i\\.|:)?\\b\", \" mevki \"),\n","    (r\"\\b(ilce|il\\u00e7e)\\b\", \" ilce \"),\n","    (r\"\\b(il)\\b\", \" il \"),\n","]\n","\n","def canon_address(text: str) -> str:\n","    \"\"\"Adres metnini hafifce standardize eder (asiri silme yapmaz).\"\"\"\n","    if not isinstance(text, str):\n","        text = \"\" if text is None else str(text)\n","    t = text.lower()\n","    for pat, repl in _CANON_MAP:\n","        t = re.sub(pat, repl, t)\n","    t = re.sub(r\"(no|kat|daire)\\s*[:=]\\s*\", r\"\\1 \", t)\n","    t = re.sub(r\"\\b(\\d{5})\\b\", r\" \\1 \", t)\n","    t = _normalize_whitespace(t)\n","    return t\n","\n","\n","# -----------------------------\n","# Ana Model\n","# -----------------------------\n","class AddressMatcherGeoCentroid:\n","    def __init__(\n","        self,\n","        # Char TF-IDF\n","        char_ngram: Tuple[int,int] = (3,8),\n","        char_max_features: Optional[int] = None,\n","        char_min_df: int = 1,\n","        char_max_df: float = 1.0,\n","        char_sublinear_tf: bool = True,\n","        # Word TF-IDF\n","        use_word: bool = True,\n","        word_ngram: Tuple[int,int] = (1,2),\n","        word_max_features: Optional[int] = 100_000,\n","        word_min_df: int = 1,\n","        word_max_df: float = 1.0,\n","        word_sublinear_tf: bool = True,\n","        # Birleştirme agirliklari\n","        char_weight: float = 1.0,\n","        word_weight: float = 0.7,\n","        # Geo kisitlama\n","        use_geo_filter: bool = True,\n","        geo_intersection_first: bool = True,   # hem il hem ilce varsa once kesisim, sonra birlik\n","        # Aday havuz fallback boyutu (hic geo bulunamazsa)\n","        fallback_top_labels: Optional[int] = None,  # None = tum etiketler\n","        # Rastgelelik\n","        random_state: int = 42\n","    ):\n","        self.char_params = dict(\n","            analyzer='char',\n","            ngram_range=char_ngram,\n","            max_features=char_max_features,\n","            lowercase=True,\n","            strip_accents='unicode',\n","            sublinear_tf=char_sublinear_tf,\n","            min_df=char_min_df,\n","            max_df=char_max_df,\n","            norm='l2',\n","            use_idf=True,\n","            smooth_idf=True,\n","            dtype=np.float32\n","        )\n","        self.word_params = dict(\n","            analyzer='word',\n","            ngram_range=word_ngram,\n","            max_features=word_max_features,\n","            lowercase=True,\n","            strip_accents='unicode',\n","            sublinear_tf=word_sublinear_tf,\n","            min_df=word_min_df,\n","            max_df=word_max_df,\n","            token_pattern=r\"(?u)\\b\\w+\\b\",\n","            norm='l2',\n","            use_idf=True,\n","            smooth_idf=True,\n","            dtype=np.float32\n","        )\n","        self.use_word = use_word\n","        self.char_weight = float(char_weight)\n","        self.word_weight = float(word_weight)\n","        self.use_geo_filter = use_geo_filter\n","        self.geo_intersection_first = geo_intersection_first\n","        self.fallback_top_labels = fallback_top_labels\n","        self.random_state = random_state\n","\n","        # placeholders\n","        self.char_vec: Optional[TfidfVectorizer] = None\n","        self.word_vec: Optional[TfidfVectorizer] = None\n","        self.C: Optional[csr_matrix] = None    # centroid matrix [n_labels, n_feats] L2-normalized\n","        self.labels_: List[Union[int,str]] = []\n","        self.label_to_idx: Dict[Union[int,str], int] = {}\n","        self.idx_to_label: Dict[int, Union[int,str]] = {}\n","\n","        # Geo\n","        self.geo = GeoDatabase()\n","        self.label_provinces: List[set] = []\n","        self.label_districts: List[set] = []\n","        self.province2labels: Dict[str, set] = {}\n","        self.district2labels: Dict[str, set] = {}\n","\n","\n","    # ---------- vektorleştirme ----------\n","    def _fit_vectorizers(self, texts: List[str]):\n","        self.char_vec = TfidfVectorizer(**self.char_params)\n","        X_char = self.char_vec.fit_transform(texts)\n","        if self.use_word:\n","            self.word_vec = TfidfVectorizer(**self.word_params)\n","            X_word = self.word_vec.fit_transform(texts)\n","            if self.word_weight != 1.0:\n","                X_word = X_word * self.word_weight\n","            if self.char_weight != 1.0:\n","                X_char = X_char * self.char_weight\n","            X = hstack([X_char, X_word]).tocsr()\n","        else:\n","            if self.char_weight != 1.0:\n","                X_char = X_char * self.char_weight\n","            X = X_char.tocsr()\n","        return X\n","\n","    def _transform(self, texts: List[str]):\n","        X_char = self.char_vec.transform(texts)\n","        if self.use_word and self.word_vec is not None:\n","            X_word = self.word_vec.transform(texts)\n","            if self.word_weight != 1.0:\n","                X_word = X_word * self.word_weight\n","            if self.char_weight != 1.0:\n","                X_char = X_char * self.char_weight\n","            X = hstack([X_char, X_word]).tocsr()\n","        else:\n","            if self.char_weight != 1.0:\n","                X_char = X_char * self.char_weight\n","            X = X_char.tocsr()\n","        X = normalize(X, norm='l2', axis=1, copy=False)\n","        return X\n","\n","\n","    # ---------- centroid hesaplama (sparse) ----------\n","    @staticmethod\n","    def _label_centroids_sparse(X: csr_matrix, y: np.ndarray, labels: List[Union[int,str]]) -> csr_matrix:\n","        rows = []\n","        for lab in labels:\n","            mask = (y == lab)\n","            Xi = X[mask]\n","            if Xi.shape[0] == 0:\n","                rows.append(csr_matrix((1, X.shape[1]), dtype=X.dtype))\n","                continue\n","            summed = Xi.sum(axis=0)\n","            if not issparse(summed):\n","                summed = csr_matrix(summed)\n","            centroid = summed.multiply(1.0 / Xi.shape[0])\n","            rows.append(centroid)\n","        C = vstack(rows).tocsr()\n","        C = normalize(C, norm='l2', axis=1, copy=False)\n","        return C\n","\n","\n","    # ---------- geo metadata ----------\n","    def _build_geo_metadata(self, texts: List[str], labels: List[Union[int,str]]):\n","        n_labels = len(self.labels_)\n","        provinces = [set() for _ in range(n_labels)]\n","        districts = [set() for _ in range(n_labels)]\n","        lab2idx = {lab: i for i, lab in enumerate(self.labels_)}\n","\n","        for t, lab in zip(texts, labels):\n","            i = lab2idx[lab]\n","            p = self.geo.find_province(t)\n","            d = self.geo.find_district(t)\n","            if p:\n","                provinces[i].add(p)\n","            if d:\n","                districts[i].add(d)\n","\n","        self.label_provinces = provinces\n","        self.label_districts = districts\n","\n","        p2ls = {}\n","        d2ls = {}\n","        for i in range(n_labels):\n","            for p in provinces[i]:\n","                p2ls.setdefault(p, set()).add(i)\n","            for d in districts[i]:\n","                d2ls.setdefault(d, set()).add(i)\n","        self.province2labels = p2ls\n","        self.district2labels = d2ls\n","\n","\n","    def _candidate_label_indices_from_text(self, text: str) -> Optional[List[int]]:\n","        if not self.use_geo_filter:\n","            return None\n","        p = self.geo.find_province(text)\n","        d = self.geo.find_district(text)\n","        cand = set()\n","        if p and d:\n","            l_p = self.province2labels.get(p, set())\n","            l_d = self.district2labels.get(d, set())\n","            inter = l_p.intersection(l_d)\n","            if self.geo_intersection_first and len(inter) > 0:\n","                cand = inter\n","            else:\n","                cand = l_p.union(l_d)\n","        elif p:\n","            cand = self.province2labels.get(p, set())\n","        elif d:\n","            cand = self.district2labels.get(d, set())\n","\n","        if len(cand) == 0:\n","            return None\n","        return sorted(list(cand))\n","\n","\n","    # ---------- public API ----------\n","    def fit(self, train_df: pd.DataFrame):\n","        assert {'address','label'}.issubset(train_df.columns), \"train_df must have ['address','label']\"\n","        texts_raw = train_df['address'].astype(str).tolist()\n","        texts = [canon_address(t) for t in texts_raw]\n","\n","        self.labels_ = list(pd.Index(train_df['label']).unique())\n","        self.label_to_idx = {lab: i for i, lab in enumerate(self.labels_)}\n","        self.idx_to_label = {i: lab for lab, i in self.label_to_idx.items()}\n","\n","        y = train_df['label'].values\n","\n","        X = self._fit_vectorizers(texts)\n","        X = normalize(X, norm='l2', axis=1, copy=False)\n","        self.C = self._label_centroids_sparse(X, y, self.labels_)\n","\n","        if self.use_geo_filter:\n","            self._build_geo_metadata(texts_raw, train_df['label'].tolist())\n","\n","        return self\n","\n","\n","    def predict(self, addresses: List[str], topk: int = 1) -> Union[List[Union[int,str]], Tuple[List[Union[int,str]], np.ndarray]]:\n","        assert self.C is not None, \"Model fit edilmedi.\"\n","\n","        texts_raw = [a if isinstance(a, str) else \"\" for a in addresses]\n","        texts = [canon_address(t) for t in texts_raw]\n","        X = self._transform(texts)\n","\n","        n = X.shape[0]\n","        preds = []\n","        all_scores = []\n","\n","        for i in range(n):\n","            xi = X[i]\n","            cand_idx = self._candidate_label_indices_from_text(texts_raw[i])\n","            if cand_idx is None:\n","                C_sub = self.C\n","                lbls = self.labels_\n","            else:\n","                C_sub = self.C[cand_idx, :]\n","                lbls = [self.labels_[j] for j in cand_idx]\n","\n","            sims = xi @ C_sub.T   # 1 x m\n","            sims = np.asarray(sims.todense()).ravel() if hasattr(sims, \"todense\") else np.asarray(sims).ravel()\n","\n","            if topk == 1:\n","                j = int(np.argmax(sims))\n","                preds.append(lbls[j])\n","                all_scores.append(sims[j])\n","            else:\n","                ord_idx = np.argsort(-sims)[:topk]\n","                preds.append([lbls[j] for j in ord_idx])\n","                all_scores.append(sims[ord_idx])\n","\n","        scores_arr = np.array(all_scores, dtype=np.float32)\n","        if topk == 1:\n","            return preds, scores_arr\n","        return preds, scores_arr\n","\n","\n","    def evaluate(self, df: pd.DataFrame) -> float:\n","        assert {'address','label'}.issubset(df.columns), \"df must have ['address','label']\"\n","        y_true = df['label'].tolist()\n","        y_pred, _ = self.predict(df['address'].tolist(), topk=1)\n","        return accuracy_score(y_true, y_pred)\n","\n","\n","# -----------------------------\n","# Hizli kullanim ornegi\n","# -----------------------------\n","if __name__ == \"__main__\":\n","    data = {\n","        'address': [\n","            \"Istiklal Mah. Cumhuriyet Cad. No: 12 D:3 Gebze Kocaeli 41400\",\n","            \"Akarca Mah. Adnan Menderes Cad. 864. Sok. No:15 D:1 K:2 Fethiye Mugla 48300\",\n","            \"Zeytinburnu Bestelsiz Mah. 58. Bulvar No:10/3 Istanbul 34020\",\n","            \"Serdivan Bahcelievler Mah. 105. Sok. No:7 Adapazari Sakarya 54050\"\n","        ],\n","        'label': [100, 200, 300, 400]\n","    }\n","    train_df = pd.DataFrame(data)\n","\n","    model = AddressMatcherGeoCentroid(\n","        char_ngram=(3,8),\n","        char_max_features=None,\n","        use_word=True,\n","        word_ngram=(1,2),\n","        char_weight=1.0,\n","        word_weight=0.7,\n","        use_geo_filter=True\n","    )\n","\n","    model.fit(train_df)\n","\n","    test_addresses = [\n","        \"Cumhuriyet cd no:12 daire:3 Gebze/KOCAELI 41400\",\n","        \"Adnan Menderes cad. 864 sok no 15 d1 Fethiye Mugla\",\n","    ]\n","\n","    preds, scores = model.predict(test_addresses, topk=1)\n","    for a, p, s in zip(test_addresses, preds, scores):\n","        print(f\"[{a}] -> {p} (score={float(s):.4f})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnhdcBbvcbdj","executionInfo":{"status":"ok","timestamp":1756279999627,"user_tz":-180,"elapsed":1388,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"59326683-1a6b-4bee-9a36-cfb265cb3aaf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[Cumhuriyet cd no:12 daire:3 Gebze/KOCAELI 41400] -> 100 (score=0.7944)\n","[Adnan Menderes cad. 864 sok no 15 d1 Fethiye Mugla] -> 200 (score=0.7330)\n"]}]},{"cell_type":"code","source":["# Gerekli paketler (Colab):\n","# pip install scikit-learn scipy pandas numpy\n","\n","from address_matcher_geo_centroid import AddressMatcherGeoCentroid\n","import pandas as pd\n","\n","# train_df: ['address','label']\n","train_df = pd.read_csv(\"train.csv\")  # kendi yolun\n","\n","model = AddressMatcherGeoCentroid(\n","    char_ngram=(3,8),\n","    use_word=True,\n","    word_ngram=(1,2),\n","    char_weight=1.0,\n","    word_weight=0.7,\n","    use_geo_filter=True,            # geo daraltma açık\n","    geo_intersection_first=True     # hem il hem ilçe varsa kesişimi öncelikle dene\n",").fit(train_df)\n","\n","# Test\n","test = pd.read_csv(\"test.csv\")\n","preds, scores = model.predict(test[\"address\"].tolist(), topk=1)\n","\n","# Submission:\n","out = pd.DataFrame({\"id\": test[\"id\"], \"label\": preds})\n","out.to_csv(\"submission.csv\", index=False)\n"],"metadata":{"id":"9kyaHUuzc3PU"},"execution_count":null,"outputs":[]}]}