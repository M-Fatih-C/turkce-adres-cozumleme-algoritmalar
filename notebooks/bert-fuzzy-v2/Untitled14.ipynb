{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28","authorship_tag":"ABX9TyM+JfpBACA0RxQXuOFOfPvC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install -r requirements.txt\n","!pip install unidecode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1dxnAeRciI6","executionInfo":{"status":"ok","timestamp":1756382814241,"user_tz":-180,"elapsed":5225,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"45c34ad2-df44-41ba-9c7c-a67f3e069c34"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.0.2)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n","Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (5.9.5)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 1)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->-r requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->-r requirements.txt (line 5)) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->-r requirements.txt (line 5)) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->-r requirements.txt (line 5)) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->-r requirements.txt (line 1)) (1.17.0)\n","Collecting unidecode\n","  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n","Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.4.0\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UagXkiycVfs","executionInfo":{"status":"ok","timestamp":1756382925625,"user_tz":-180,"elapsed":104783,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"e6f74a3b-51de-4abd-a43b-18ab1bf93443"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Turkish Address Preprocessing - Start\n","Reading: train.csv\n","\n","Examples (random 10):\n","- BEFORE: Mustafa Kemal mah.acı badem yolu no:54 kat 1\n","  AFTER : mustafa kemal mahalleaci badem yolu numara 54 kat 1\n","- BEFORE: Çamlıçay mah. 5208 sokak no:17\n","  AFTER : camlicay mahalle 5208 sokak numara 17\n","- BEFORE: cüneytbey mahallesi kuva i milliye cad .no 1-7c Menderes izmir\n","  AFTER : cuneytbey mahallesi kuva i milliye caddesi numara 1-7c menderes izmir\n","- BEFORE: KASIMPAŞA MH 250. SOKAK NO:45G\n","  AFTER : kasimpasa mahalle 250 sokak numara45g\n","- BEFORE: Yenimahalle 4741 sokak no 9 daire 6 Yunusemre/Manisa\n","  AFTER : yenimahalle 4741 sokak numara 9 daire 6 yunusemre/manisa\n","- BEFORE: Arnes jetseal ozdemir reduktor arkasi 610 sokak No 49\n","  AFTER : arnes jetseal ozdemir reduktor arkasi 610 sokak numara 49\n","- BEFORE: efeler mah. evliya çelebi caddesi no:1 Alparslan ÜNSAL muayenehanesi\n","  AFTER : efeler mahalle evliya celebi caddesi numara 1 alparslan unsal muayenehanesi\n","- BEFORE: 1127 Sokak No:5 Livai Aydın Evleri 5 B Blok K:2 D:5 \n","  AFTER : 1127 sokak numara 5 livai aydin evleri 5 b blok kat2 daire5\n","- BEFORE: Catalkaya Mah. Mithatpasa Cad. Aydinlar Sitesi No:350 B1 Blok Kat:3 D:6 Narlidere Narlidere\n","  AFTER : catalkaya mahalle mithatpasa caddesi aydinlar sitesi numara 350 b1 blok kat3 daire6 narlidere narlidere\n","- BEFORE: 361 sok no63 d/1 Saffet battal apt.\n","  AFTER : 361 sokak numara 63 daire/1 saffet battal apartmani\n","\n","Summary:\n","- rows processed: 848237\n","- unique before: 847995\n","- unique after : 832340\n","- exact removed: 15897\n","- near removed : 0\n","- final rows   : 832340\n","- elapsed time : 73.85s (11485 rows/sec)\n","Saved: preprocessed_addresses.csv\n","Reading: test.csv\n","\n","Examples (random 10):\n","- BEFORE: Halkapınar mahallesi 1203/10 sokak no 1/A remzibey iş merkezi\n","  AFTER : halkapinar mahallesi 1203/10 sokak numara 1/a remzibey is merkezi\n","- BEFORE: ALPARSLAN TÜRKEŞ CD KOTAN DELUXE ST VİLLA E/1\n","  AFTER : alparslan turkes caddesi kotan deluxe st villa e/1\n","- BEFORE: 4576 sokak merkez mah. No:21 kat:1 ALTINDAĞ/İZMİR\n","  AFTER : 4576 sokak merkez mahalle numara 21 kat1 altindag/izmir\n","- BEFORE: MAVİŞEHİR Mah. 2040, Mavişehir, 35590 Karşıyaka, İzmir pamukkale:6 giris:102 k:14 d:54 \n","  AFTER : mavisehir mahalle 2040 mavisehir 35590 karsiyaka izmir pamukkale 6 giris 102 kat14 daire54\n","- BEFORE: Gül Bahçe Cd. iyte yeni kyk kız yurdu\n","  AFTER : gul bahce caddesi iyte yeni kyk kiz yurdu\n","- BEFORE: İzmir, Urla, Sıramahalle 2on Degirmenler Sokak No21 Sıra Mah\n","  AFTER : izmir urla siramahalle 2on degirmenler sokak numara 21 sira mahalle\n","- BEFORE: 30ağustos mah 4. Cadde No 22 z20 Elit kırtasiye \n","  AFTER : 30agustos mahalle 4 caddesi numara 22 z20 elit kirtasiye\n","- BEFORE: 6794/2 no.13 d.10 eğitimciler sitesi Ahmet taner kışlalı mahallesi Çiğli/izmir AHMET TANER KIŞLALI ÇİĞLİ İzmir\n","  AFTER : 6794/numara 2 13 daire10 egitimciler sitesi ahmet taner kislali mahallesi cigli/izmir ahmet taner kislali cigli izmir\n","- BEFORE: emek mah. mezarlık karşısı  3 sokak hacı musa ab 1 daire 12\n","  AFTER : emek mahalle mezarlik karsisi 3 sokak haci musa ab 1 daire 12\n","- BEFORE: Hacıveli mah 434/2sokak no 4\n","Foça / İzmir / Türkiye\n","  AFTER : haciveli mahalle 434/2sokak numara 4 foca / izmir /\n","\n","Summary:\n","- rows processed: 217241\n","- unique before: 217223\n","- unique after : 216152\n","- exact removed: 1089\n","- near removed : 0\n","- final rows   : 216152\n","- elapsed time : 18.98s (11444 rows/sec)\n","Saved: preprocessed_addresses.csv\n","Done.\n"]}],"source":["\"\"\"\n","Colab-ready Turkish Address Preprocessing Pipeline\n","\n","Implements a high-performance, modular preprocessing suite suitable for 1M+ addresses.\n","\n","Key features:\n","- Deterministic normalization order\n","- Vectorized regex replacements\n","- LRU-cached token-level fuzzy typo correction via rapidfuzz\n","- Optional stopword removal (Turkish)\n","- Exact and optional near-duplicate deduplication with bucketed fuzzy compare\n","- End-to-end CLI entry that reads train/test if present and writes outputs\n","\n","Dependencies: pandas, numpy, unidecode, rapidfuzz (preferred), nltk, scikit-learn (for hashing optional), tqdm (optional)\n","\"\"\"\n","\n","from __future__ import annotations\n","\n","import os\n","import sys\n","import re\n","import time\n","import math\n","import random\n","import warnings\n","from dataclasses import dataclass, field\n","from functools import lru_cache\n","from typing import Dict, Iterable, List, Optional, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","from unidecode import unidecode\n","\n","\n","def _ensure_dependencies_installed() -> None:\n","    \"\"\"Attempt to install runtime dependencies if missing (Colab-friendly).\n","\n","    Uses subprocess to pip install only if imports fail. Keeps overhead minimal\n","    for environments that already satisfy requirements.\n","    \"\"\"\n","    try:\n","        import rapidfuzz  # noqa: F401\n","    except Exception:  # pragma: no cover\n","        try:\n","            import subprocess\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rapidfuzz>=3.6.0\"], stdout=sys.stdout, stderr=sys.stderr)\n","        except Exception:\n","            warnings.warn(\"Failed to auto-install rapidfuzz. Falling back later if needed.\")\n","\n","    try:\n","        import nltk  # noqa: F401\n","    except Exception:  # pragma: no cover\n","        try:\n","            import subprocess\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk>=3.8.1\"], stdout=sys.stdout, stderr=sys.stderr)\n","        except Exception:\n","            warnings.warn(\"Failed to auto-install nltk. Stopword removal may be disabled.\")\n","\n","\n","_ensure_dependencies_installed()\n","\n","\n","try:\n","    from rapidfuzz import process as rf_process\n","    from rapidfuzz import fuzz as rf_fuzz\n","except Exception:  # pragma: no cover\n","    rf_process = None  # type: ignore\n","    rf_fuzz = None  # type: ignore\n","\n","\n","try:\n","    import nltk\n","    from nltk.corpus import stopwords as nltk_stopwords\n","    # Ensure stopwords are available\n","    try:\n","        nltk.data.find(\"corpora/stopwords\")\n","    except LookupError:  # pragma: no cover\n","        nltk.download(\"stopwords\")\n","except Exception:  # pragma: no cover\n","    nltk = None\n","    nltk_stopwords = None  # type: ignore\n","\n","\n","def _safe_lower(text: str) -> str:\n","    \"\"\"Locale-safe lowercasing. For Turkish, plain .lower() is acceptable here\n","    because we unidecode afterwards to normalize Turkish characters.\n","    \"\"\"\n","    return text.lower()\n","\n","\n","def _remove_extra_spaces(text: str) -> str:\n","    return re.sub(r\"\\s+\", \" \", text).strip()\n","\n","\n","@dataclass\n","class TurkishAddressConfig:\n","    \"\"\"Configuration for TurkishAddressPreprocessor.\n","\n","    - abbreviation_map: token-aware deterministic expansions\n","    - typo_candidates: trusted lexicon used by fuzzy correction\n","    - fuzzy_threshold: minimum similarity for applying a token correction\n","    - remove_stopwords: whether to drop non-discriminative tokens\n","    - enable_near_dedup: whether to remove near-duplicates within buckets\n","    - near_dedup_threshold: similarity to consider two addresses near-duplicates\n","    - chunksize: optional chunked processing for very large CSVs\n","    - canonical_slash: canonical separator for house compounds like 5/1 or 5-1\n","    \"\"\"\n","\n","    abbreviation_map: Dict[str, str] = field(default_factory=lambda: {\n","        # mahalle\n","        \"mh\": \"mahalle\",\n","        \"mah\": \"mahalle\",\n","        \"mahalle\": \"mahalle\",\n","        # cadde\n","        \"cd\": \"caddesi\",\n","        \"cad\": \"caddesi\",\n","        \"cadde\": \"caddesi\",\n","        \"cadd\": \"caddesi\",\n","        # sokak\n","        \"sk\": \"sokak\",\n","        \"sok\": \"sokak\",\n","        \"sokak\": \"sokak\",\n","        # bulvar\n","        \"blv\": \"bulvar\",\n","        \"bulv\": \"bulvar\",\n","        \"bulvar\": \"bulvar\",\n","        # apartman\n","        \"apt\": \"apartmani\",\n","        \"ap\": \"apartmani\",\n","        \"apartman\": \"apartmani\",\n","        # numara\n","        \"no\": \"numara\",\n","        \"n\": \"numara\",\n","        # daire\n","        \"d\": \"daire\",\n","        \"daire\": \"daire\",\n","        # kat\n","        \"k\": \"kat\",\n","        \"kat\": \"kat\",\n","    })\n","    typo_candidates: Iterable[str] = field(default_factory=lambda: [\n","        # Common Turkish admin/geographic names and frequent address tokens\n","        \"istanbul\", \"ankara\", \"izmir\", \"bursa\", \"antalya\", \"mugla\", \"fethiye\",\n","        \"karsiyaka\", \"bostanli\", \"uskudar\", \"narlidere\", \"konak\", \"bornova\",\n","        \"cankaya\", \"kecioren\", \"karabaglar\", \"balcova\", \"bayrakli\", \"karabaaglar\",\n","        \"mahalle\", \"caddesi\", \"sokak\", \"bulvar\", \"apartmani\", \"numara\", \"daire\", \"kat\",\n","        # extra variants often seen\n","        \"karsiyaka\", \"uskudar\", \"iskitler\", \"guzelbahce\", \"karabaglar\", \"kecioren\",\n","    ])\n","    fuzzy_threshold: int = 90\n","    remove_stopwords: bool = True\n","    enable_near_dedup: bool = False\n","    near_dedup_threshold: int = 98\n","    chunksize: Optional[int] = None\n","    canonical_slash: str = \"/\"\n","\n","\n","class TurkishAddressPreprocessor:\n","    \"\"\"High-performance Turkish address preprocessor.\n","\n","    Public methods:\n","    - preprocess_address(text) -> str\n","    - preprocess_dataframe(df, col_name) -> pd.DataFrame\n","\n","    Hooks:\n","    - to_embedding_ready(text)\n","    - custom_rules_hook(text)\n","    \"\"\"\n","\n","    def __init__(self, config: Optional[TurkishAddressConfig] = None) -> None:\n","        self.config = config or TurkishAddressConfig()\n","\n","        # Prepare abbreviation patterns (token-aware, with optional dots and colons)\n","        # e.g., \"mah.\", \"mah:\" -> \"mahalle\"\n","        self._abbr_patterns: List[Tuple[re.Pattern[str], str]] = []\n","        for key, value in self.config.abbreviation_map.items():\n","            # word boundary, allow dotted/colon forms and optional trailing dot\n","            # Examples: \"mah\", \"mah.\", \"mah:\", \"mah:\" with number after (handled later)\n","            pattern = re.compile(rf\"(?<![\\w\\d]){re.escape(key)}\\.?\\:?\\b\")\n","            self._abbr_patterns.append((pattern, value))\n","\n","        # Numeric normalization patterns\n","        # Normalize variations of numara: \"no:5\", \"no 5\", \"n: 5\", \"5 no\" -> \"numara 5\"\n","        self._re_numara_colon = re.compile(r\"\\b(?:no|n|numara)\\s*[:\\-]?\\s*(\\d+)\\b\")\n","        self._re_numara_trailing = re.compile(r\"\\b(\\d+)\\s*(?:no|n|numara)\\b\")\n","\n","        # Compound house numbers: 5/1, 5-1 -> canonical 5/1 (configurable)\n","        self._re_house_compound = re.compile(r\"\\b(\\d+)\\s*[\\/-]\\s*(\\d+)\\b\")\n","\n","        # Remove extraneous punctuation but keep digits, letters, space, and separators / -\n","        self._re_punct = re.compile(r\"[^0-9a-z\\s/\\-]\")\n","\n","        # Collapse multiple separators spaces or slashes/dashes\n","        self._re_multi_space = re.compile(r\"\\s+\")\n","        self._re_multi_slash = re.compile(r\"[/]+\")\n","        self._re_multi_dash = re.compile(r\"[-]+\")\n","\n","        # Stopwords\n","        self._stopwords: set[str] = set()\n","        if self.config.remove_stopwords and nltk_stopwords is not None:\n","            try:\n","                self._stopwords = set(nltk_stopwords.words(\"turkish\"))\n","                # Add address-specific non-discriminative tokens\n","                self._stopwords.update({\"il\", \"ilce\", \"turkiye\", \"posta\", \"kodu\"})\n","            except Exception:  # pragma: no cover\n","                self._stopwords = set()\n","\n","        # Prepare fuzzy lexicon\n","        self._lexicon: List[str] = sorted(set(map(str, self.config.typo_candidates)))\n","\n","    # ------------------------------ Core Steps ------------------------------ #\n","\n","    def _normalize_case_and_chars(self, text: str) -> str:\n","        text = _safe_lower(text)\n","        # normalize Turkish characters to ASCII (c, s, g, u, o, i)\n","        text = unidecode(text)\n","        return text\n","\n","    def _expand_abbreviations(self, text: str) -> str:\n","        for pattern, replacement in self._abbr_patterns:\n","            text = pattern.sub(replacement, text)\n","        return text\n","\n","    def _cleanup_punctuation(self, text: str) -> str:\n","        text = self._re_punct.sub(\" \", text)\n","        text = self._re_multi_space.sub(\" \", text)\n","        return text.strip()\n","\n","    def _normalize_numbers(self, text: str) -> str:\n","        # Canonicalize explicit numara specifications\n","        text = self._re_numara_colon.sub(lambda m: f\"numara {m.group(1)}\", text)\n","        text = self._re_numara_trailing.sub(lambda m: f\"numara {m.group(1)}\", text)\n","\n","        # Normalize house compounds like 5/1, 5-1 -> 5/<canonical>\n","        def _compound(m: re.Match[str]) -> str:\n","            left, right = m.group(1), m.group(2)\n","            return f\"{left}{self.config.canonical_slash}{right}\"\n","\n","        text = self._re_house_compound.sub(_compound, text)\n","\n","        # Collapse multiple separators\n","        text = self._re_multi_slash.sub(self.config.canonical_slash, text)\n","        text = self._re_multi_dash.sub(\"-\", text)\n","        return text\n","\n","    def custom_rules_hook(self, text: str) -> str:\n","        \"\"\"Hook for project-specific rules. No-op by default.\"\"\"\n","        return text\n","\n","    def _maybe_remove_stopwords(self, text: str) -> str:\n","        if not self._stopwords:\n","            return text\n","        tokens = [tok for tok in text.split() if tok not in self._stopwords]\n","        return \" \".join(tokens)\n","\n","    # -------------------------- Fuzzy Typo Correction ----------------------- #\n","\n","    @lru_cache(maxsize=100_000)\n","    def _correct_token(self, token: str) -> str:\n","        \"\"\"Correct a single token using rapidfuzz if similar to a trusted lexicon.\n","\n","        Applies only to alphabetic tokens with sufficient length. Returns the\n","        original token if below threshold or library unavailable.\n","        \"\"\"\n","        if rf_process is None or rf_fuzz is None:\n","            return token\n","        if len(token) < 4:\n","            return token\n","        if not token.isalpha():\n","            return token\n","        if token in self._lexicon:\n","            return token\n","\n","        match = rf_process.extractOne(token, self._lexicon, scorer=rf_fuzz.token_sort_ratio)\n","        if match is None:\n","            return token\n","        candidate, score, _ = match\n","        if score >= self.config.fuzzy_threshold:\n","            return str(candidate)\n","        return token\n","\n","    def _apply_fuzzy_corrections(self, text: str) -> str:\n","        tokens = text.split()\n","        corrected = [self._correct_token(tok) for tok in tokens]\n","        return \" \".join(corrected)\n","\n","    # ----------------------------- Public API ------------------------------- #\n","\n","    def preprocess_address(self, text: str) -> str:\n","        \"\"\"Preprocess a single address string to a canonical normalized form.\n","\n","        Steps (in order):\n","        1) lowercase\n","        2) unidecode\n","        3) abbreviation expansion\n","        4) punctuation cleanup & whitespace\n","        5) number and house-number normalization\n","        6) frequent typo correction (token-level rapidfuzz, cached)\n","        7) optional stopword removal\n","        8) final whitespace compaction\n","        \"\"\"\n","        if not isinstance(text, str):\n","            return \"\"\n","        s = text\n","        s = self._normalize_case_and_chars(s)\n","        s = self._expand_abbreviations(s)\n","        s = self._cleanup_punctuation(s)\n","        s = self._normalize_numbers(s)\n","        s = self._apply_fuzzy_corrections(s)\n","        s = self._maybe_remove_stopwords(s)\n","        s = _remove_extra_spaces(s)\n","        s = self.custom_rules_hook(s)\n","        return s\n","\n","    def preprocess_dataframe(self, df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n","        \"\"\"Apply preprocessing to a DataFrame efficiently.\n","\n","        - Leaves original column intact; adds `normalized_address`.\n","        - Vectorizes deterministic regex rules via pandas.str methods where possible.\n","        - Uses cached token-level fuzzy correction.\n","        - Optionally deduplicates exactly and near-duplicates.\n","        \"\"\"\n","        if col_name not in df.columns:\n","            raise ValueError(f\"Column '{col_name}' not found in DataFrame\")\n","\n","        s = df[col_name].astype(str)\n","\n","        # Vectorized lower + unidecode via Series.apply (fast enough, avoids per-row Python in heavy logic)\n","        s = s.str.lower().map(unidecode)\n","\n","        # Abbreviation expansion via sequential regex replace\n","        for pattern, replacement in self._abbr_patterns:\n","            s = s.str.replace(pattern, replacement, regex=True)\n","\n","        # Punctuation cleanup\n","        s = s.str.replace(self._re_punct, \" \", regex=True)\n","        s = s.str.replace(self._re_multi_space, \" \", regex=True).str.strip()\n","\n","        # Number normalization\n","        s = s.str.replace(self._re_numara_colon, lambda m: f\"numara {m.group(1)}\", regex=True)\n","        s = s.str.replace(self._re_numara_trailing, lambda m: f\"numara {m.group(1)}\", regex=True)\n","        s = s.str.replace(self._re_house_compound, lambda m: f\"{m.group(1)}{self.config.canonical_slash}{m.group(2)}\", regex=True)\n","        s = s.str.replace(self._re_multi_slash, self.config.canonical_slash, regex=True)\n","        s = s.str.replace(self._re_multi_dash, \"-\", regex=True)\n","\n","        # Fuzzy corrections: token-level with LRU cache\n","        s = s.apply(self._apply_fuzzy_corrections)\n","\n","        # Optional stopword removal\n","        if self._stopwords:\n","            s = s.apply(self._maybe_remove_stopwords)\n","\n","        # Final compaction\n","        s = s.map(_remove_extra_spaces)\n","\n","        out = df.copy()\n","        out[\"normalized_address\"] = s\n","\n","        return out\n","\n","    # ------------------------------ Dedup Logic ----------------------------- #\n","\n","    @staticmethod\n","    def _blocking_key(text: str) -> str:\n","        alnum = re.sub(r\"[^a-z0-9]\", \"\", text)\n","        prefix = alnum[:10]\n","        length_bin = str(len(alnum) // 5)\n","        return f\"{prefix}|{length_bin}\"\n","\n","    def deduplicate(self, df: pd.DataFrame, near_duplicates: Optional[bool] = None) -> Tuple[pd.DataFrame, Dict[str, int]]:\n","        \"\"\"Deduplicate on `normalized_address`.\n","\n","        Always removes exact duplicates. If near_duplicates is True (or config enabled),\n","        additionally removes near-duplicates within buckets based on a blocking key.\n","        Returns the deduplicated DataFrame and stats.\n","        \"\"\"\n","        if \"normalized_address\" not in df.columns:\n","            raise ValueError(\"DataFrame must contain 'normalized_address' before deduplication\")\n","\n","        start_rows = len(df)\n","        df_ex = df.drop_duplicates(subset=[\"normalized_address\"], keep=\"first\").copy()\n","        exact_removed = start_rows - len(df_ex)\n","\n","        enable_near = self.config.enable_near_dedup if near_duplicates is None else near_duplicates\n","        near_removed = 0\n","\n","        if enable_near and rf_process is not None and rf_fuzz is not None:\n","            # Bucket by blocking key\n","            df_ex[\"_block\"] = df_ex[\"normalized_address\"].map(self._blocking_key)\n","            groups = df_ex.groupby(\"_block\", sort=False)\n","            to_drop_idx: List[int] = []\n","            for _, g in groups:\n","                addrs = g[\"normalized_address\"].tolist()\n","                idxs = g.index.tolist()\n","                # Greedy selection using high threshold; keep the first, drop those >= threshold to it\n","                kept: List[int] = []\n","                for i, base in enumerate(addrs):\n","                    if idxs[i] in to_drop_idx:\n","                        continue\n","                    kept.append(idxs[i])\n","                    # Compare remaining in bucket to base\n","                    for j in range(i + 1, len(addrs)):\n","                        if idxs[j] in to_drop_idx:\n","                            continue\n","                        score = rf_fuzz.token_sort_ratio(base, addrs[j])\n","                        if score >= self.config.near_dedup_threshold:\n","                            to_drop_idx.append(idxs[j])\n","                # continue to next bucket\n","            if to_drop_idx:\n","                df_ex = df_ex.drop(index=to_drop_idx)\n","                near_removed = len(to_drop_idx)\n","            if \"_block\" in df_ex.columns:\n","                df_ex = df_ex.drop(columns=[\"_block\"])\n","\n","        stats = {\n","            \"start_rows\": start_rows,\n","            \"after_exact\": len(df) - exact_removed,\n","            \"exact_removed\": exact_removed,\n","            \"after_all\": len(df_ex),\n","            \"near_removed\": near_removed,\n","        }\n","        return df_ex, stats\n","\n","    # ------------------------------- Hooks --------------------------------- #\n","\n","    def to_embedding_ready(self, text: str) -> str:\n","        \"\"\"Hook for additional steps for embedding models (e.g., TF-IDF/BERT).\n","        Currently no-op; reserved for future extension.\n","        \"\"\"\n","        return text\n","\n","\n","def _print_examples(df: pd.DataFrame, col: str, n: int = 10) -> None:\n","    sample = df.sample(n=min(n, len(df)), random_state=42)\n","    for _, row in sample.iterrows():\n","        before = row[col]\n","        after = row[\"normalized_address\"]\n","        print(f\"- BEFORE: {before}\")\n","        print(f\"  AFTER : {after}\")\n","\n","\n","def _summary_report(df_before: pd.DataFrame, df_after: pd.DataFrame, col: str, dedup_stats: Optional[Dict[str, int]], elapsed: float) -> None:\n","    print(\"\\nSummary:\")\n","    print(f\"- rows processed: {len(df_before)}\")\n","    print(f\"- unique before: {df_before[col].nunique(dropna=False)}\")\n","    print(f\"- unique after : {df_after['normalized_address'].nunique(dropna=False)}\")\n","    if dedup_stats is not None:\n","        print(f\"- exact removed: {dedup_stats.get('exact_removed', 0)}\")\n","        print(f\"- near removed : {dedup_stats.get('near_removed', 0)}\")\n","        print(f\"- final rows   : {dedup_stats.get('after_all', len(df_after))}\")\n","    print(f\"- elapsed time : {elapsed:.2f}s ({len(df_before)/max(elapsed,1e-6):.0f} rows/sec)\")\n","\n","\n","def _process_csv_if_exists(prep: TurkishAddressPreprocessor, path: str, is_train: bool) -> Optional[pd.DataFrame]:\n","    if not os.path.exists(path):\n","        return None\n","    print(f\"Reading: {path}\")\n","    df = pd.read_csv(path)\n","    col = \"address\"\n","    if col not in df.columns:\n","        raise ValueError(f\"Expected column '{col}' in {path}\")\n","    t0 = time.time()\n","    df_out = prep.preprocess_dataframe(df, col)\n","    t1 = time.time()\n","\n","    # Dedup always exact; near if enabled in config\n","    df_dedup, stats = prep.deduplicate(df_out, near_duplicates=None)\n","\n","    # Save\n","    out_path = \"preprocessed_addresses.csv\"\n","    if is_train:\n","        cols = [\"address\", \"normalized_address\", \"label\"] if \"label\" in df.columns else [\"address\", \"normalized_address\"]\n","        save_df = df_dedup.copy()\n","        save_df = save_df[cols]\n","    else:\n","        cols = [\"id\", \"address\", \"normalized_address\"] if \"id\" in df.columns else [\"address\", \"normalized_address\"]\n","        save_df = df_dedup.copy()[cols]\n","    mode = \"w\"\n","    header = True\n","    if os.path.exists(out_path):\n","        # Overwrite to keep most recent run\n","        pass\n","    save_df.to_csv(out_path, index=False, mode=mode, header=header)\n","\n","    # Examples and summary\n","    print(\"\\nExamples (random 10):\")\n","    _print_examples(df_out, col, n=10)\n","    _summary_report(df, df_out, col, stats, elapsed=t1 - t0)\n","    print(f\"Saved: {out_path}\")\n","    return df_out\n","\n","\n","def _run_sanity_tests() -> None:\n","    prep = TurkishAddressPreprocessor()\n","    cases = {\n","        \"Akarca Mah. Adnan Menderes Cad. 864.Sok. No:15 D.1 K.2\": \"akarca mahalle adnan menderes caddesi 864 sokak numara 15 daire 1 kat 2\",\n","        \"Pazaryeri mah. 417. sk. No:6/4 Fethiye/MUĞLA\": \"pazaryeri mahalle 417 sokak numara 6/4 fethiye mugla\",\n","        \"Limanreis Mahallesi Aziz Sokak No 4 Narlıdere İzmir Narlıdere Narlıdere\": \"limanreis mahallesi aziz sokak numara 4 narlidere izmir narlidere narlidere\",\n","        \"1771 sokak no:5 d:5 Kaçuna Apt. Bostanlı Karsıyaka KARŞIYAKA İzmir\": \"1771 sokak numara 5 daire 5 kacuna apartmani bostanli karsiyaka karsiyaka izmir\",\n","    }\n","    for raw, expected_prefix in cases.items():\n","        out = prep.preprocess_address(raw)\n","        # We assert strong invariants: abbreviation expansion and numara format\n","        assert \"numara \" in out, f\"missing 'numara' in {out}\"\n","        assert re.search(r\"\\bnumara \\d+\\b\", out), f\"numara not canonical: {out}\"\n","        # Allow minor fuzzy differences, but ensure prefix matches expected start tokens\n","        assert out.startswith(expected_prefix.split()[0]), f\"unexpected start for {out}\"\n","\n","\n","if __name__ == \"__main__\":\n","    # End-to-end run if train.csv/test.csv present in CWD\n","    print(\"Turkish Address Preprocessing - Start\")\n","    _run_sanity_tests()\n","    config = TurkishAddressConfig(\n","        remove_stopwords=True,\n","        enable_near_dedup=False,  # set True to enable near-duplicate removal\n","        fuzzy_threshold=90,\n","        near_dedup_threshold=98,\n","        canonical_slash=\"/\",\n","    )\n","    preprocessor = TurkishAddressPreprocessor(config)\n","\n","    any_done = False\n","    train_out = _process_csv_if_exists(preprocessor, \"train.csv\", is_train=True)\n","    if train_out is not None:\n","        any_done = True\n","    test_out = _process_csv_if_exists(preprocessor, \"test.csv\", is_train=False)\n","    if test_out is not None:\n","        any_done = True\n","\n","    if not any_done:\n","        print(\"No train.csv or test.csv found in current directory. Nothing to process.\")\n","    print(\"Done.\")\n","\n","# -------------------------- Module-level wrappers -------------------------- #\n","\n","# Lazy singleton to expose simple function API as requested\n","_GLOBAL_PREPROCESSOR: Optional[TurkishAddressPreprocessor] = None\n","\n","\n","def _get_global_preprocessor() -> TurkishAddressPreprocessor:\n","    global _GLOBAL_PREPROCESSOR\n","    if _GLOBAL_PREPROCESSOR is None:\n","        _GLOBAL_PREPROCESSOR = TurkishAddressPreprocessor()\n","    return _GLOBAL_PREPROCESSOR\n","\n","\n","def preprocess_address(text: str) -> str:\n","    \"\"\"Module-level convenience function wrapping TurkishAddressPreprocessor.preprocess_address.\"\"\"\n","    return _get_global_preprocessor().preprocess_address(text)\n","\n","\n","def preprocess_dataframe(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n","    \"\"\"Module-level convenience function wrapping TurkishAddressPreprocessor.preprocess_dataframe.\"\"\"\n","    return _get_global_preprocessor().preprocess_dataframe(df, col_name)\n","\n","\n"]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Turkish Address Normalization Pipeline\n","=====================================\n","\n","Production-ready preprocessing pipeline for Turkish address data with 848,237 training addresses\n","grouped into 10,390 unique labels (average 81.6 addresses per label).\n","\n","Author: NLP Engineer\n","Date: 2024\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import string\n","from collections import defaultdict, Counter\n","import time\n","import psutil\n","import gc\n","from typing import Dict, List, Tuple, Optional, Union\n","from tqdm import tqdm\n","import warnings\n","\n","# Suppress warnings for cleaner output\n","warnings.filterwarnings('ignore')\n","\n","# ================================================================\n","# TURKISH ADDRESS NORMALIZER CLASS\n","# ================================================================\n","\n","class TurkishAddressNormalizer:\n","    \"\"\"\n","    Comprehensive Turkish address normalizer with abbreviation expansion,\n","    typo correction, and structural standardization.\n","\n","    Optimized for processing 1M+ addresses efficiently.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Initialize the normalizer with Turkish-specific rules and mappings.\"\"\"\n","\n","        # Turkish character mapping (ç→c, ş→s, ğ→g, ü→u, ö→o, ı→i)\n","        self.turkish_chars = {\n","            'ç': 'c', 'ğ': 'g', 'ı': 'i', 'ö': 'o', 'ş': 's', 'ü': 'u',\n","            'Ç': 'C', 'Ğ': 'G', 'İ': 'I', 'Ö': 'O', 'Ş': 'S', 'Ü': 'U'\n","        }\n","\n","        # Comprehensive abbreviation expansion mappings\n","        self.abbreviations = {\n","            # Neighborhood/District\n","            'mh': 'mahalle', 'mah': 'mahalle', 'mahalle': 'mahalle',\n","            'mh.': 'mahalle', 'mah.': 'mahalle',\n","\n","            # Street types\n","            'cd': 'caddesi', 'cad': 'caddesi', 'cadde': 'caddesi',\n","            'cd.': 'caddesi', 'cad.': 'caddesi',\n","            'sk': 'sokak', 'sok': 'sokak', 'sokak': 'sokak',\n","            'sk.': 'sokak', 'sok.': 'sokak',\n","            'blv': 'bulvar', 'bulv': 'bulvar', 'bulvar': 'bulvar',\n","            'blv.': 'bulvar', 'bulv.': 'bulvar',\n","\n","            # Building types\n","            'apt': 'apartmani', 'ap': 'apartmani', 'apartman': 'apartmani',\n","            'apt.': 'apartmani', 'ap.': 'apartmani',\n","            'sit': 'sitesi', 'site': 'sitesi',\n","            'sit.': 'sitesi', 'site.': 'sitesi',\n","            'blk': 'blok', 'blok': 'blok',\n","            'blk.': 'blok', 'blok.': 'blok',\n","            'plz': 'plaza', 'plaza': 'plaza',\n","            'plz.': 'plaza', 'plaza.': 'plaza',\n","            'avm': 'alisveris merkezi', 'avm.': 'alisveris merkezi',\n","\n","            # Address components\n","            'no': 'numara', 'nu': 'numara', 'numara': 'numara',\n","            'no.': 'numara', 'nu.': 'numara',\n","            'kt': 'kat', 'kat': 'kat', 'k': 'kat',\n","            'kt.': 'kat', 'kat.': 'kat', 'k.': 'kat',\n","            'dr': 'daire', 'daire': 'daire', 'd': 'daire',\n","            'dr.': 'daire', 'daire.': 'daire', 'd.': 'daire',\n","            'dai': 'daire', 'dai.': 'daire',\n","\n","            # Directions\n","            'kz': 'kuzey', 'gy': 'guney', 'dt': 'dogu', 'bt': 'bati',\n","            'kz.': 'kuzey', 'gy.': 'guney', 'dt.': 'dogu', 'bt.': 'bati',\n","\n","            # Common institutions\n","            'unv': 'universitesi', 'unv.': 'universitesi',\n","            'hst': 'hastanesi', 'hst.': 'hastanesi',\n","            'okl': 'okulu', 'okl.': 'okulu',\n","            'lise': 'lisesi', 'lise.': 'lisesi'\n","        }\n","\n","        # Common typo corrections for Turkish cities/districts\n","        self.typo_corrections = {\n","            'uskuar': 'uskudar', 'isketl': 'iskitler', 'kadikoy': 'kadikoy',\n","            'beyoglu': 'beyoglu', 'sisli': 'sisli', 'besiktas': 'besiktas',\n","            'fatih': 'fatih', 'umraniye': 'umraniye', 'maltepe': 'maltepe',\n","            'pendik': 'pendik', 'tuzla': 'tuzla', 'kartal': 'kartal'\n","        }\n","\n","        # Regex patterns for number normalization\n","        self.number_patterns = [\n","            (r'no[:\\s=]*(\\d+)', r'numara \\1'),  # No:5, No=5, No 5\n","            (r'(\\d+)[/\\-](\\d+)', r'numara \\1 daire \\2'),  # 5/3, 5-3\n","            (r'(\\d+)\\.?\\s*kat', r'\\1 kat'),  # 2.kat, 2 kat\n","            (r'(\\d+)\\.?\\s*daire', r'\\1 daire'),  # 4.daire, 4 daire\n","            (r'(\\d+)\\.?\\s*blok', r'\\1 blok'),  # A.blok, A blok\n","        ]\n","\n","        # Turkish stopwords to optionally remove (while preserving location terms)\n","        self.stopwords = {\n","            've', 'ile', 'bu', 'bir', 'da', 'de', 'mi', 'mu', 'musun', 'musunuz',\n","            'dir', 'dir', 'tir', 'tur', 'dır', 'tır', 'tür', 'dür'\n","        }\n","\n","    def normalize_turkish_chars(self, text: str) -> str:\n","        \"\"\"\n","        Convert Turkish characters to their ASCII equivalents.\n","\n","        Args:\n","            text: Input text with Turkish characters\n","\n","        Returns:\n","            Text with Turkish characters normalized\n","        \"\"\"\n","        for tr_char, en_char in self.turkish_chars.items():\n","            text = text.replace(tr_char, en_char)\n","        return text\n","\n","    def expand_abbreviations(self, text: str) -> str:\n","        \"\"\"\n","        Expand common Turkish abbreviations in address text.\n","\n","        Args:\n","            text: Input text with abbreviations\n","\n","        Returns:\n","            Text with abbreviations expanded\n","        \"\"\"\n","        words = text.split()\n","        expanded_words = []\n","\n","        for word in words:\n","            # Clean word for matching (remove punctuation)\n","            clean_word = word.strip('.,;:()[]{}\"-').lower()\n","\n","            if clean_word in self.abbreviations:\n","                expanded_words.append(self.abbreviations[clean_word])\n","            else:\n","                expanded_words.append(word)\n","\n","        return ' '.join(expanded_words)\n","\n","    def correct_typos(self, text: str, threshold: float = 0.8) -> str:\n","        \"\"\"\n","        Correct common typos in Turkish place names using fuzzy matching.\n","\n","        Args:\n","            text: Input text\n","            threshold: Similarity threshold for correction\n","\n","        Returns:\n","            Text with typos corrected\n","        \"\"\"\n","        words = text.split()\n","        corrected_words = []\n","\n","        for word in words:\n","            clean_word = word.strip('.,;:()[]{}\"-').lower()\n","\n","            # Check for exact typo matches first\n","            if clean_word in self.typo_corrections:\n","                corrected_words.append(self.typo_corrections[clean_word])\n","            else:\n","                corrected_words.append(word)\n","\n","        return ' '.join(corrected_words)\n","\n","    def standardize_numbers(self, text: str) -> str:\n","        \"\"\"\n","        Standardize number formats in addresses.\n","\n","        Args:\n","            text: Input text with various number formats\n","\n","        Returns:\n","            Text with standardized number formats\n","        \"\"\"\n","        for pattern, replacement in self.number_patterns:\n","            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n","\n","        return text\n","\n","    def remove_redundant_locations(self, text: str) -> str:\n","        \"\"\"\n","        Remove redundant location repetitions in addresses.\n","\n","        Args:\n","            text: Input text that may contain repeated location names\n","\n","        Returns:\n","            Text with redundant repetitions removed\n","        \"\"\"\n","        words = text.split()\n","        if len(words) <= 3:\n","            return text\n","\n","        # Simple deduplication of consecutive identical words\n","        deduplicated = []\n","        for i, word in enumerate(words):\n","            if i == 0 or word != words[i-1]:\n","                deduplicated.append(word)\n","\n","        return ' '.join(deduplicated)\n","\n","    def clean_punctuation_and_spacing(self, text: str) -> str:\n","        \"\"\"\n","        Clean punctuation and normalize spacing.\n","\n","        Args:\n","            text: Input text with various punctuation\n","\n","        Returns:\n","            Cleaned text with normalized spacing\n","        \"\"\"\n","        # Remove special characters but preserve alphanumeric and Turkish chars\n","        text = re.sub(r'[^\\w\\s]', ' ', text)\n","\n","        # Normalize whitespace (multiple spaces to single space)\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        # Remove leading/trailing whitespace\n","        return text.strip()\n","\n","    def normalize(self, address: str) -> str:\n","        \"\"\"\n","        Apply full normalization pipeline to a single address.\n","\n","        Args:\n","            address: Raw address string\n","\n","        Returns:\n","            Normalized address string\n","        \"\"\"\n","        if pd.isna(address) or not isinstance(address, str):\n","            return \"\"\n","\n","        # Convert to lowercase\n","        address = address.lower()\n","\n","        # Normalize Turkish characters\n","        address = self.normalize_turkish_chars(address)\n","\n","        # Expand abbreviations\n","        address = self.expand_abbreviations(address)\n","\n","        # Correct typos\n","        address = self.correct_typos(address)\n","\n","        # Standardize numbers\n","        address = self.standardize_numbers(address)\n","\n","        # Remove redundant locations\n","        address = self.remove_redundant_locations(address)\n","\n","        # Clean punctuation and spacing\n","        address = self.clean_punctuation_and_spacing(address)\n","\n","        return address\n","\n","# ================================================================\n","# MAIN PREPROCESSING FUNCTIONS\n","# ================================================================\n","\n","def preprocess_address(text: str) -> str:\n","    \"\"\"\n","    Apply full preprocessing pipeline to a single address.\n","\n","    Args:\n","        text: Raw address string\n","\n","    Returns:\n","        Normalized address string\n","    \"\"\"\n","    normalizer = TurkishAddressNormalizer()\n","    return normalizer.normalize(text)\n","\n","def preprocess_dataframe(df: pd.DataFrame,\n","                        address_col: str = 'address',\n","                        label_col: str = 'label',\n","                        batch_size: int = 10000) -> pd.DataFrame:\n","    \"\"\"\n","    Apply preprocessing to entire dataset with progress tracking and memory optimization.\n","\n","    Args:\n","        df: Input dataframe\n","        address_col: Column name containing addresses\n","        label_col: Column name containing labels (optional)\n","        batch_size: Number of rows to process in each batch\n","\n","    Returns:\n","        DataFrame with additional 'processed_address' column\n","    \"\"\"\n","    print(f\"Starting preprocessing of {len(df):,} addresses...\")\n","\n","    # Create a copy to avoid modifying original\n","    result_df = df.copy()\n","\n","    # Initialize normalizer\n","    normalizer = TurkishAddressNormalizer()\n","\n","    # Process in batches for memory efficiency\n","    processed_addresses = []\n","\n","    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing addresses\"):\n","        batch = df.iloc[i:i+batch_size]\n","\n","        # Process batch\n","        batch_addresses = batch[address_col].apply(normalizer.normalize)\n","        processed_addresses.extend(batch_addresses)\n","\n","        # Clear memory\n","        if i % (batch_size * 5) == 0:\n","            gc.collect()\n","\n","    # Add processed column\n","    result_df['processed_address'] = processed_addresses\n","\n","    # Remove empty processed addresses\n","    initial_count = len(result_df)\n","    result_df = result_df[result_df['processed_address'].str.len() > 0].reset_index(drop=True)\n","    final_count = len(result_df)\n","\n","    print(f\"Preprocessing completed. Removed {initial_count - final_count:,} empty addresses.\")\n","    print(f\"Final dataset size: {final_count:,} addresses\")\n","\n","    return result_df\n","\n","def analyze_preprocessing_impact(df_original: pd.DataFrame,\n","                               df_processed: pd.DataFrame,\n","                               address_col: str = 'address') -> Dict:\n","    \"\"\"\n","    Generate comprehensive statistics on preprocessing effectiveness.\n","\n","    Args:\n","        df_original: Original dataframe\n","        df_processed: Processed dataframe\n","        address_col: Column name containing addresses\n","\n","    Returns:\n","        Dictionary with preprocessing metrics\n","    \"\"\"\n","    print(\"Analyzing preprocessing impact...\")\n","\n","    # Calculate basic statistics\n","    original_lengths = df_original[address_col].str.len()\n","    processed_lengths = df_processed['processed_address'].str.len()\n","\n","    original_word_counts = df_original[address_col].str.split().str.len()\n","    processed_word_counts = df_processed['processed_address'].str.split().str.len()\n","\n","    # Unique address analysis\n","    original_unique = df_original[address_col].nunique()\n","    processed_unique = df_processed['processed_address'].nunique()\n","\n","    # Deduplication statistics\n","    reduction_rate = (original_unique - processed_unique) / original_unique\n","\n","    # Length statistics\n","    avg_length_change = processed_lengths.mean() - original_lengths.mean()\n","    avg_word_change = processed_word_counts.mean() - original_word_counts.mean()\n","\n","    # Create analysis results\n","    analysis = {\n","        'total_addresses': len(df_original),\n","        'processed_addresses': len(df_processed),\n","        'original_unique_addresses': original_unique,\n","        'processed_unique_addresses': processed_unique,\n","        'reduction_rate': reduction_rate,\n","        'avg_original_length': original_lengths.mean(),\n","        'avg_processed_length': processed_lengths.mean(),\n","        'avg_length_change': avg_length_change,\n","        'avg_original_words': original_word_counts.mean(),\n","        'avg_processed_words': processed_word_counts.mean(),\n","        'avg_word_change': avg_word_change,\n","        'length_reduction_percent': (avg_length_change / original_lengths.mean()) * 100,\n","        'word_reduction_percent': (avg_word_change / original_word_counts.mean()) * 100\n","    }\n","\n","    return analysis\n","\n","def display_transformation_examples(df_original: pd.DataFrame,\n","                                  df_processed: pd.DataFrame,\n","                                  address_col: str = 'address',\n","                                  n_examples: int = 10) -> None:\n","    \"\"\"\n","    Display random before/after examples of address transformations.\n","\n","    Args:\n","        df_original: Original dataframe\n","        df_processed: Processed dataframe\n","        address_col: Column name containing addresses\n","        n_examples: Number of examples to display\n","    \"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(f\"RANDOM TRANSFORMATION EXAMPLES (showing {n_examples} examples)\")\n","    print(f\"{'='*80}\")\n","\n","    # Get random indices\n","    random_indices = np.random.choice(len(df_processed),\n","                                    size=min(n_examples, len(df_processed)),\n","                                    replace=False)\n","\n","    for i, idx in enumerate(random_indices, 1):\n","        original = df_original.iloc[idx][address_col]\n","        processed = df_processed.iloc[idx]['processed_address']\n","\n","        print(f\"\\nExample {i}:\")\n","        print(f\"  Original:  {original}\")\n","        print(f\"  Processed: {processed}\")\n","        print(f\"  Length:    {len(original)} → {len(processed)} chars\")\n","        print(f\"  Words:     {len(original.split())} → {len(processed.split())} words\")\n","\n","# ================================================================\n","# UTILITY FUNCTIONS\n","# ================================================================\n","\n","def get_memory_usage() -> float:\n","    \"\"\"Get current memory usage in MB.\"\"\"\n","    process = psutil.Process()\n","    return process.memory_info().rss / 1024 / 1024\n","\n","def save_processed_data(df: pd.DataFrame, filename: str = 'preprocessed_addresses.csv') -> None:\n","    \"\"\"\n","    Save processed data to CSV file.\n","\n","    Args:\n","        df: Processed dataframe\n","        filename: Output filename\n","    \"\"\"\n","    print(f\"Saving processed data to {filename}...\")\n","    df.to_csv(filename, index=False, encoding='utf-8')\n","    print(f\"Data saved successfully! File size: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n","\n","# ================================================================\n","# MAIN EXECUTION\n","# ================================================================\n","\n","def main():\n","    \"\"\"Main execution function with example usage.\"\"\"\n","\n","    print(\"Turkish Address Normalization Pipeline\")\n","    print(\"=\" * 50)\n","\n","    # Example usage with sample data\n","    print(\"\\nCreating sample data for demonstration...\")\n","\n","    # Create sample Turkish addresses\n","    sample_addresses = [\n","        \"Narlıdere İzmir Narlıdere Narlıdere\",\n","        \"Kadıköy Mah. Cadde No:5 K:2 D:4\",\n","        \"Beşiktaş Mh. Cd. Apt No5/3\",\n","        \"Üsküdar Mahallesi Sokak No:12 Kat 3\",\n","        \"Şişli Mah. Bulvar No:25 Blok A\",\n","        \"Fatih Mah. Caddesi No:8 Daire 5\",\n","        \"Maltepe Mah. Sokak No:15 Kat 2\",\n","        \"Pendik Mah. Cadde No:30 Blok B\",\n","        \"Tuzla Mah. Bulvar No:42 Daire 8\",\n","        \"Kartal Mah. Sokak No:18 Kat 4\"\n","    ]\n","\n","    sample_labels = [f\"label_{i}\" for i in range(len(sample_addresses))]\n","\n","    # Create sample dataframe\n","    sample_df = pd.DataFrame({\n","        'id': range(len(sample_addresses)),\n","        'address': sample_addresses,\n","        'label': sample_labels\n","    })\n","\n","    print(f\"Sample data created: {len(sample_df)} addresses\")\n","\n","    # Start preprocessing\n","    start_time = time.time()\n","    start_memory = get_memory_usage()\n","\n","    print(f\"\\nStarting preprocessing at {start_time:.2f}s, Memory: {start_memory:.2f} MB\")\n","\n","    # Preprocess the data\n","    processed_df = preprocess_dataframe(sample_df, address_col='address', label_col='label')\n","\n","    # Calculate processing time and memory\n","    end_time = time.time()\n","    end_memory = get_memory_usage()\n","    processing_time = end_time - start_time\n","    memory_change = end_memory - start_memory\n","\n","    print(f\"\\nPreprocessing completed in {processing_time:.2f} seconds\")\n","    print(f\"Memory usage: {start_memory:.2f} → {end_memory:.2f} MB (change: {memory_change:+.2f} MB)\")\n","\n","    # Analyze impact\n","    analysis = analyze_preprocessing_impact(sample_df, processed_df, 'address')\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"PREPROCESSING ANALYSIS RESULTS\")\n","    print(f\"{'='*60}\")\n","\n","    for key, value in analysis.items():\n","        if isinstance(value, float):\n","            if 'rate' in key or 'percent' in key:\n","                print(f\"{key.replace('_', ' ').title()}: {value:.2%}\")\n","            else:\n","                print(f\"{key.replace('_', ' ').title()}: {value:.2f}\")\n","        else:\n","            print(f\"{key.replace('_', ' ').title()}: {value:,}\")\n","\n","    # Display examples\n","    display_transformation_examples(sample_df, processed_df, 'address', n_examples=5)\n","\n","    # Save results\n","    save_processed_data(processed_df, 'sample_preprocessed_addresses.csv')\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n","    print(f\"{'='*60}\")\n","\n","    return processed_df, analysis\n","\n","if __name__ == \"__main__\":\n","    # Prefer Kaggle-style orchestration if data present; otherwise run demo main()\n","    try:\n","        # Added: Kaggle-style preprocessing + submission orchestration\n","        import os as _os\n","        def run_preprocessing_and_submission(data_dir: str = \".\", batch_size: int = 10000):\n","            \"\"\"\n","            Run preprocessing for train/test and build submission.csv by exact match on processed_address.\n","\n","            - Reads train.csv (address,label) and test.csv (id,address) with dtype=str\n","            - Uses TurkishAddressNormalizer().normalize in batches (no normalization logic changes)\n","            - Saves train_preprocessed.csv (address,processed_address,label)\n","            - Saves test_preprocessed.csv (id,address,processed_address)\n","            - Builds processed_address -> mode(label) mapping from train and assigns to test\n","            - Fallback for unmatched: global most frequent label from train\n","            - Writes submission.csv with columns id,label preserving original order\n","            - Prints 10 random examples per split and coverage/time stats\n","            \"\"\"\n","            start = time.time()\n","            train_path = _os.path.join(data_dir, \"train.csv\")\n","            test_path = _os.path.join(data_dir, \"test.csv\")\n","            ss_path = _os.path.join(data_dir, \"sample_submission.csv\")\n","\n","            # Load CSVs\n","            print(\"Loading CSVs...\")\n","            train_df = pd.read_csv(train_path, dtype=str, keep_default_na=False)\n","            test_df = pd.read_csv(test_path, dtype=str, keep_default_na=False)\n","\n","            assert 'address' in train_df.columns and 'label' in train_df.columns, \"train.csv must have columns address,label\"\n","            assert 'id' in test_df.columns and 'address' in test_df.columns, \"test.csv must have columns id,address\"\n","\n","            # Initialize normalizer\n","            normalizer = TurkishAddressNormalizer()\n","\n","            def _normalize_series_in_batches(series: pd.Series, batch_size: int) -> List[str]:\n","                out: List[str] = []\n","                for i in tqdm(range(0, len(series), batch_size), desc=\"Normalizing\", unit=\"rows\"):\n","                    batch = series.iloc[i:i+batch_size]\n","                    out.extend(batch.apply(normalizer.normalize))\n","                    if (i // batch_size) % 5 == 0:\n","                        gc.collect()\n","                return out\n","\n","            # Process train\n","            print(f\"Processing train: {len(train_df):,} rows (batch_size={batch_size})\")\n","            train_proc = train_df.copy()\n","            train_proc['processed_address'] = _normalize_series_in_batches(train_proc['address'], batch_size)\n","            # Save train preprocessed\n","            train_out_cols = ['address', 'processed_address', 'label']\n","            train_proc[train_out_cols].to_csv(_os.path.join(data_dir, 'train_preprocessed.csv'), index=False, encoding='utf-8')\n","\n","            # Process test (do not drop any rows; preserve order)\n","            print(f\"Processing test: {len(test_df):,} rows (batch_size={batch_size})\")\n","            test_proc = test_df.copy()\n","            test_proc['processed_address'] = _normalize_series_in_batches(test_proc['address'], batch_size)\n","            # Save test preprocessed\n","            test_out_cols = ['id', 'address', 'processed_address']\n","            test_proc[test_out_cols].to_csv(_os.path.join(data_dir, 'test_preprocessed.csv'), index=False, encoding='utf-8')\n","\n","            # Print examples\n","            def _print_examples(df: pd.DataFrame, addr_col: str, proc_col: str, name: str):\n","                print(f\"\\nExamples ({name}) - 10 random:\")\n","                sample = df.sample(n=min(10, len(df)), random_state=42)\n","                for _, r in sample.iterrows():\n","                    print(f\"- BEFORE: {r[addr_col]}\")\n","                    print(f\"  AFTER : {r[proc_col]}\")\n","            _print_examples(train_proc, 'address', 'processed_address', 'train')\n","            _print_examples(test_proc, 'address', 'processed_address', 'test')\n","\n","            # Build mapping: processed_address -> mode(label)\n","            print(\"\\nBuilding label mapping (processed_address -> mode(label))...\")\n","            # global most frequent label\n","            global_mode_label = train_proc['label'].mode(dropna=False)[0]\n","            # mode per processed address\n","            vc = train_proc.groupby('processed_address')['label'].agg(lambda x: x.value_counts(dropna=False).idxmax()).reset_index()\n","            vc.columns = ['processed_address', 'label_mode']\n","\n","            # Join to test by exact processed_address\n","            test_labeled = test_proc.merge(vc, on='processed_address', how='left')\n","            matched = test_labeled['label_mode'].notna().sum()\n","            coverage = matched / len(test_labeled) if len(test_labeled) else 0.0\n","            # Fallback fill\n","            test_labeled['label'] = test_labeled['label_mode'].fillna(global_mode_label)\n","            submission = test_labeled[['id', 'label']].copy()\n","\n","            # Validate against sample_submission if present\n","            if _os.path.exists(ss_path):\n","                ss = pd.read_csv(ss_path, dtype=str, keep_default_na=False)\n","                assert list(ss.columns) == ['id', 'label'], \"sample_submission.csv must have columns id,label\"\n","                assert len(ss) == len(submission), \"submission row count must match sample_submission\"\n","\n","            # Save submission\n","            sub_path = _os.path.join(data_dir, 'submission.csv')\n","            submission.to_csv(sub_path, index=False, encoding='utf-8')\n","\n","            # Reporting\n","            elapsed = time.time() - start\n","            print(\"\\nReporting:\")\n","            print(f\"- Train rows processed: {len(train_df):,}\")\n","            print(f\"- Test rows processed : {len(test_df):,}\")\n","            print(f\"- Train unique before : {train_df['address'].nunique():,}\")\n","            print(f\"- Train unique after  : {train_proc['processed_address'].nunique():,}\")\n","            print(f\"- Submission coverage : {coverage*100:.2f}% matched, {(1-coverage)*100:.2f}% fallback\")\n","            print(f\"- Elapsed time        : {elapsed:.2f}s ({(len(train_df)+len(test_df))/max(elapsed,1e-6):.0f} rows/sec)\")\n","\n","            return train_proc, test_proc, submission\n","\n","        data_present = _os.path.exists(\"train.csv\") and _os.path.exists(\"test.csv\")\n","        if data_present:\n","            print(\"Detected train.csv and test.csv. Running preprocessing and submission...\")\n","            _ = run_preprocessing_and_submission(data_dir=\".\", batch_size=10000)\n","        else:\n","            # Fallback to original demo\n","            processed_data, analysis_results = main()\n","            print(\"\\nPipeline completed successfully!\")\n","    except Exception as e:\n","        print(f\"\\nError during execution: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rO8MlslvexxA","executionInfo":{"status":"ok","timestamp":1756383173713,"user_tz":-180,"elapsed":201888,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"131130a9-21d3-4447-d14c-09e429338284"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected train.csv and test.csv. Running preprocessing and submission...\n","Loading CSVs...\n","Processing train: 848,237 rows (batch_size=10000)\n"]},{"output_type":"stream","name":"stderr","text":["Normalizing: 100%|██████████| 85/85 [00:32<00:00,  2.60rows/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing test: 217,241 rows (batch_size=10000)\n"]},{"output_type":"stream","name":"stderr","text":["Normalizing: 100%|██████████| 22/22 [00:08<00:00,  2.62rows/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Examples (train) - 10 random:\n","- BEFORE: Mustafa Kemal mah.acı badem yolu no:54 kat 1\n","  AFTER : mustafa kemal mah aci badem yolu numara 54 kat 1\n","- BEFORE: Çamlıçay mah. 5208 sokak no:17\n","  AFTER : camlicay mahalle 5208 sokak numara 17\n","- BEFORE: cüneytbey mahallesi kuva i milliye cad .no 1-7c Menderes izmir\n","  AFTER : cuneytbey mahallesi kuva i milliye caddesi numara 1 daire 7c menderes izmir\n","- BEFORE: KASIMPAŞA MH 250. SOKAK NO:45G\n","  AFTER : kasimpasa mahalle 250 sokak numara 45g\n","- BEFORE: Yenimahalle 4741 sokak no 9 daire 6 Yunusemre/Manisa\n","  AFTER : yenimahalle 4741 sokak numara 9 daire 6 yunusemre manisa\n","- BEFORE: Arnes jetseal ozdemir reduktor arkasi 610 sokak No 49\n","  AFTER : arnes jetseal ozdemir reduktor arkasi 610 sokak numara 49\n","- BEFORE: efeler mah. evliya çelebi caddesi no:1 Alparslan ÜNSAL muayenehanesi\n","  AFTER : efeler mahalle evliya celebi caddesi numara 1 alparslan unsal muayenehanesi\n","- BEFORE: 1127 Sokak No:5 Livai Aydın Evleri 5 B Blok K:2 D:5 \n","  AFTER : 1127 sokak numara 5 livai aydin evleri 5 b blok k 2 d 5\n","- BEFORE: Catalkaya Mah. Mithatpasa Cad. Aydinlar Sitesi No:350 B1 Blok Kat:3 D:6 Narlidere Narlidere\n","  AFTER : catalkaya mahalle mithatpasa caddesi aydinlar sitesi numara 350 b1 blok kat 3 d 6 narlidere\n","- BEFORE: 361 sok no63 d/1 Saffet battal apt.\n","  AFTER : 361 sokak numara 63 d 1 saffet battal apartmani\n","\n","Examples (test) - 10 random:\n","- BEFORE: Halkapınar mahallesi 1203/10 sokak no 1/A remzibey iş merkezi\n","  AFTER : halkapinar mahallesi numara 1203 daire 10 sokak numara 1 a remzibey is merkezi\n","- BEFORE: ALPARSLAN TÜRKEŞ CD KOTAN DELUXE ST VİLLA E/1\n","  AFTER : alparslan turkes caddesi kotan deluxe st vi lla e 1\n","- BEFORE: 4576 sokak merkez mah. No:21 kat:1 ALTINDAĞ/İZMİR\n","  AFTER : 4576 sokak merkez mahalle numara 21 kat 1 altindag i zmi r\n","- BEFORE: MAVİŞEHİR Mah. 2040, Mavişehir, 35590 Karşıyaka, İzmir pamukkale:6 giris:102 k:14 d:54 \n","  AFTER : mavi sehi r mahalle 2040 mavisehir 35590 karsiyaka i zmir pamukkale 6 giris 102 k 14 d 54\n","- BEFORE: Gül Bahçe Cd. iyte yeni kyk kız yurdu\n","  AFTER : gul bahce caddesi iyte yeni kyk kiz yurdu\n","- BEFORE: İzmir, Urla, Sıramahalle 2on Degirmenler Sokak No21 Sıra Mah\n","  AFTER : i zmir urla siramahalle 2on degirmenler sokak numara 21 sira mahalle\n","- BEFORE: 30ağustos mah 4. Cadde No 22 z20 Elit kırtasiye \n","  AFTER : 30agustos mahalle 4 caddesi numara 22 z20 elit kirtasiye\n","- BEFORE: 6794/2 no.13 d.10 eğitimciler sitesi Ahmet taner kışlalı mahallesi Çiğli/izmir AHMET TANER KIŞLALI ÇİĞLİ İzmir\n","  AFTER : numara 6794 daire 2 no 13 d 10 egitimciler sitesi ahmet taner kislali mahallesi cigli izmir ahmet taner kislali ci gli i zmir\n","- BEFORE: emek mah. mezarlık karşısı  3 sokak hacı musa ab 1 daire 12\n","  AFTER : emek mahalle mezarlik karsisi 3 sokak haci musa ab 1 daire 12\n","- BEFORE: Hacıveli mah 434/2sokak no 4\n","Foça / İzmir / Türkiye\n","  AFTER : haciveli mahalle numara 434 daire 2sokak numara 4 foca i zmir turkiye\n","\n","Building label mapping (processed_address -> mode(label))...\n","\n","Reporting:\n","- Train rows processed: 848,237\n","- Test rows processed : 217,241\n","- Train unique before : 847,995\n","- Train unique after  : 831,717\n","- Submission coverage : 3.45% matched, 96.55% fallback\n","- Elapsed time        : 200.89s (5304 rows/sec)\n"]}]},{"cell_type":"code","source":["# ================================================================\n","# SUBMISSION ÜRETİCİ (mevcut normalleştirici mantığına dokunmadan)\n","# ================================================================\n","def make_submission(train_csv: str = \"train.csv\",\n","                    test_csv: str = \"test.csv\",\n","                    out_csv: str = \"submission.csv\",\n","                    batch_size: int = 10000):\n","    \"\"\"\n","    Build submission.csv (id,label) using exact match on processed_address.\n","    - Uses the existing TurkishAddressNormalizer logic (no changes).\n","    - For ties/multiple labels per address, picks the most frequent (mode).\n","    - For unmatched test rows, fills with the global most frequent train label.\n","    \"\"\"\n","    import os\n","    if not (os.path.exists(train_csv) and os.path.exists(test_csv)):\n","        raise FileNotFoundError(\"train.csv veya test.csv bulunamadı.\")\n","\n","    # 1) Veri yükle\n","    train_df = pd.read_csv(train_csv, dtype={\"address\": str})\n","    test_df  = pd.read_csv(test_csv,  dtype={\"address\": str})\n","    if \"label\" not in train_df.columns or \"address\" not in train_df.columns:\n","        raise ValueError(\"train.csv 'address' ve 'label' kolonlarını içermeli.\")\n","    if \"address\" not in test_df.columns:\n","        raise ValueError(\"test.csv 'address' kolonu içermeli.\")\n","    if \"id\" not in test_df.columns:\n","        test_df[\"id\"] = np.arange(len(test_df))\n","\n","    # 2) Normalizasyon (mevcut sınıfı kullanarak, mantığı değiştirmeden)\n","    norm = TurkishAddressNormalizer()\n","\n","    # train processed_address\n","    train_proc_list = []\n","    for i in range(0, len(train_df), batch_size):\n","        part = train_df.iloc[i:i+batch_size].copy()\n","        part[\"processed_address\"] = part[\"address\"].apply(norm.normalize)\n","        train_proc_list.append(part[[\"processed_address\", \"label\"]])\n","    train_proc = pd.concat(train_proc_list, axis=0, ignore_index=True)\n","\n","    # test processed_address (satır düşürmeden, bire bir aynı uzunlukta)\n","    test_proc_pa = []\n","    for i in range(0, len(test_df), batch_size):\n","        part = test_df.iloc[i:i+batch_size].copy()\n","        part[\"processed_address\"] = part[\"address\"].apply(norm.normalize)\n","        test_proc_pa.append(part[[\"id\", \"processed_address\"]])\n","    test_proc_pa = pd.concat(test_proc_pa, axis=0, ignore_index=True)\n","\n","    # 3) processed_address -> mode(label) haritası\n","    mode_per_addr = (train_proc\n","                     .groupby(\"processed_address\")[\"label\"]\n","                     .agg(lambda s: s.value_counts().index[0]))\n","\n","    # 4) Test'e label yaz\n","    test_labeled = test_df[[\"id\"]].merge(\n","        test_proc_pa.merge(mode_per_addr.rename(\"label\"),\n","                           how=\"left\",\n","                           left_on=\"processed_address\",\n","                           right_index=True)[[\"id\", \"label\"]],\n","        on=\"id\", how=\"left\"\n","    )\n","\n","    # 5) Boş kalanlar için fallback = train’de en sık görülen label\n","    fallback_label = train_df[\"label\"].value_counts().index[0]\n","    test_labeled[\"label\"] = test_labeled[\"label\"].fillna(fallback_label)\n","\n","    # 6) Çıktı: submission.csv (id,label)\n","    submission = test_labeled[[\"id\", \"label\"]].copy()\n","    submission.to_csv(out_csv, index=False, encoding=\"utf-8\")\n","    print(f\"Saved: {out_csv}  (rows={len(submission):,})\")\n"],"metadata":{"id":"jKEwo8EngKQU","executionInfo":{"status":"ok","timestamp":1756381635292,"user_tz":-180,"elapsed":54,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Exact-Match Labeling (No Normalization)\n","=======================================\n","\n","Objective:\n","- Learn address -> label mapping from train.csv using exact string equality.\n","- Apply to test.csv by exact join on 'address'.\n","- Produce submission.csv in (id,label) format.\n","- No normalization, no lowercasing, no punctuation edits. Pure exact match.\n","\n","Files written:\n","- address_label_map.csv       (address,label,count)\n","- unmatched_test_preview.csv  (first 100 unmatched rows for quick inspection)\n","- submission.csv              (id,label)\n","\"\"\"\n","\n","import os\n","import time\n","import gc\n","from typing import Tuple\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","def load_train_test(\n","    train_path: str = \"train.csv\",\n","    test_path: str = \"test.csv\"\n",") -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"Load train/test with safe dtypes and minimal assumptions.\"\"\"\n","    if not os.path.exists(train_path):\n","        raise FileNotFoundError(f\"Missing {train_path}\")\n","    if not os.path.exists(test_path):\n","        raise FileNotFoundError(f\"Missing {test_path}\")\n","\n","    train = pd.read_csv(train_path, dtype=str, keep_default_na=False)\n","    test  = pd.read_csv(test_path,  dtype=str, keep_default_na=False)\n","\n","    if \"address\" not in train.columns or \"label\" not in train.columns:\n","        raise ValueError(\"train.csv must contain 'address' and 'label' columns.\")\n","    if \"address\" not in test.columns:\n","        raise ValueError(\"test.csv must contain 'address' column.\")\n","    if \"id\" not in test.columns:\n","        # Create sequential ids if missing (common in some setups)\n","        test = test.copy()\n","        test[\"id\"] = np.arange(len(test), dtype=int)\n","\n","    return train, test\n","\n","\n","def build_exact_map(train: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Build address -> mode(label) mapping using exact string equality.\n","    If an address appears with multiple labels, pick the most frequent (mode).\n","    \"\"\"\n","    # Count occurrences per (address, label)\n","    counts = (\n","        train.groupby([\"address\", \"label\"], as_index=False)\n","             .size()\n","             .rename(columns={\"size\": \"count\"})\n","    )\n","\n","    # For each address, select the label with max count\n","    idx = counts.groupby(\"address\")[\"count\"].idxmax()\n","    mapping = counts.loc[idx, [\"address\", \"label\", \"count\"]].reset_index(drop=True)\n","    return mapping\n","\n","\n","def apply_map_and_build_submission(\n","    test: pd.DataFrame,\n","    mapping: pd.DataFrame,\n","    fallback_label: str,\n","    submission_path: str = \"submission.csv\",\n","    unmatched_preview_path: str = \"unmatched_test_preview.csv\",\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Left-join test on address -> label map, fill NAs with fallback label,\n","    and write submission.csv (id,label). Also save a small preview of unmatched rows.\n","    \"\"\"\n","    # Join (exact match)\n","    merged = test.merge(mapping[[\"address\", \"label\"]], on=\"address\", how=\"left\", suffixes=(\"\", \"_from_map\"))\n","\n","    # Report coverage before fill\n","    matched = merged[\"label\"].notna().sum()\n","    total = len(merged)\n","    unmatched = total - matched\n","\n","    # Save a quick preview of unmatched rows (first 100) for debugging\n","    if unmatched > 0:\n","        preview = merged[merged[\"label\"].isna()][[\"id\", \"address\"]].head(100)\n","        preview.to_csv(unmatched_preview_path, index=False, encoding=\"utf-8\")\n","\n","    # Fallback: fill missing labels with global most frequent label from train\n","    merged[\"label\"] = merged[\"label\"].fillna(fallback_label)\n","\n","    # Build submission\n","    submission = merged[[\"id\", \"label\"]].copy()\n","\n","    # Write\n","    submission.to_csv(submission_path, index=False, encoding=\"utf-8\")\n","\n","    # Print stats\n","    print(\"\\n=== Submission Stats ===\")\n","    print(f\"Test rows              : {total:,}\")\n","    print(f\"Matched by exact address: {matched:,} ({matched/total:.2%})\")\n","    print(f\"Filled by fallback      : {unmatched:,} ({unmatched/total:.2%})\")\n","    print(f\"Saved: {submission_path}\")\n","    if unmatched > 0:\n","        print(f\"Unmatched preview saved: {unmatched_preview_path}\")\n","\n","    return submission\n","\n","\n","def main(\n","    train_path: str = \"train.csv\",\n","    test_path: str = \"test.csv\",\n","    map_out: str = \"address_label_map.csv\",\n","    submission_path: str = \"submission.csv\",\n","    unmatched_preview_path: str = \"unmatched_test_preview.csv\",\n","):\n","    t0 = time.time()\n","    print(\"Exact-Match Labeling (no normalization) starting...\")\n","\n","    # 1) Load data\n","    train, test = load_train_test(train_path, test_path)\n","    print(f\"Loaded train: {len(train):,} rows, test: {len(test):,} rows\")\n","\n","    # 2) Global most frequent label (fallback)\n","    fallback_label = train[\"label\"].value_counts().index[0]\n","    print(f\"Global most frequent label (fallback): {fallback_label}\")\n","\n","    # 3) Build exact address -> label mapping\n","    mapping = build_exact_map(train)\n","    print(f\"Unique train addresses: {train['address'].nunique():,}\")\n","    print(f\"Address->label map size: {len(mapping):,}\")\n","    # Save mapping\n","    mapping.to_csv(map_out, index=False, encoding=\"utf-8\")\n","    print(f\"Saved: {map_out}\")\n","\n","    gc.collect()\n","\n","    # 4) Apply map to test and produce submission.csv\n","    submission = apply_map_and_build_submission(\n","        test=test,\n","        mapping=mapping,\n","        fallback_label=fallback_label,\n","        submission_path=submission_path,\n","        unmatched_preview_path=unmatched_preview_path,\n","    )\n","\n","    # 5) Optional: basic sanity checks vs sample_submission.csv if present\n","    if os.path.exists(\"sample_submission.csv\"):\n","        sample = pd.read_csv(\"sample_submission.csv\", dtype=str, keep_default_na=False)\n","        if list(sample.columns) == [\"id\", \"label\"] and len(sample) == len(submission):\n","            print(\"sample_submission.csv shape matches generated submission ✅\")\n","        else:\n","            print(\"Warning: sample_submission.csv shape/columns differ from generated submission.\")\n","\n","    print(f\"\\nDone in {time.time() - t0:.2f}s. submission.csv ready.\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBrX0qdTmWD9","executionInfo":{"status":"ok","timestamp":1756383254238,"user_tz":-180,"elapsed":9924,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"78037785-409f-4e35-c11b-de63ac84d2dc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Exact-Match Labeling (no normalization) starting...\n","Loaded train: 848,237 rows, test: 217,241 rows\n","Global most frequent label (fallback): 5414\n","Unique train addresses: 847,995\n","Address->label map size: 847,995\n","Saved: address_label_map.csv\n","\n","=== Submission Stats ===\n","Test rows              : 217,241\n","Matched by exact address: 118 (0.05%)\n","Filled by fallback      : 217,123 (99.95%)\n","Saved: submission.csv\n","Unmatched preview saved: unmatched_test_preview.csv\n","\n","Done in 9.61s. submission.csv ready.\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Tiered Exact-Match Submission (No Text Normalization on Stored Data)\n","- We DO NOT modify or save normalized text anywhere.\n","- We only create transient \"keys\" for comparison to improve exact-match coverage.\n","\n","Outputs:\n","- submission.csv (id,label)\n","- unmatched_test_preview.csv (first 100 unmatched after all tiers)\n","- coverage report per tier\n","\"\"\"\n","\n","import os\n","import re\n","import time\n","import numpy as np\n","import pandas as pd\n","\n","\n","# -----------------------------\n","# Key builders (comparison-only)\n","# -----------------------------\n","def key_raw(s: pd.Series) -> pd.Series:\n","    # pure exact string\n","    return s\n","\n","def key_lower_ws(s: pd.Series) -> pd.Series:\n","    # case/whitespace-insensitive: strip, collapse spaces, lower, strip NBSP\n","    if s.dtype != \"object\":\n","        s = s.astype(str)\n","    s = (s.str.replace(\"\\u00A0\", \" \", regex=False)  # NBSP -> space\n","           .str.strip()\n","           .str.replace(r\"\\s+\", \" \", regex=True)\n","           .str.lower())\n","    return s\n","\n","def key_lower_ws_fmtlite(s: pd.Series) -> pd.Series:\n","    # same as key_lower_ws + drop only lightweight formatting punctuation\n","    # keep digits, letters (including Turkish), dot, dash, slash, underscore\n","    if s.dtype != \"object\":\n","        s = s.astype(str)\n","    s = key_lower_ws(s)\n","    # remove only these: quotes, commas, semicolons, colon, parens, brackets, braces\n","    s = s.str.replace(r\"[\\\"',;:\\(\\)\\[\\]\\{\\}]\", \" \", regex=True)\n","    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n","    return s\n","\n","\n","# -----------------------------\n","# Pipeline\n","# -----------------------------\n","def load_train_test(train_path=\"train.csv\", test_path=\"test.csv\"):\n","    if not os.path.exists(train_path):\n","        raise FileNotFoundError(f\"Missing {train_path}\")\n","    if not os.path.exists(test_path):\n","        raise FileNotFoundError(f\"Missing {test_path}\")\n","\n","    train = pd.read_csv(train_path, dtype=str, keep_default_na=False)\n","    test  = pd.read_csv(test_path,  dtype=str, keep_default_na=False)\n","\n","    if \"address\" not in train.columns or \"label\" not in train.columns:\n","        raise ValueError(\"train.csv must contain 'address' and 'label'.\")\n","    if \"address\" not in test.columns:\n","        raise ValueError(\"test.csv must contain 'address'.\")\n","    if \"id\" not in test.columns:\n","        test = test.copy()\n","        test[\"id\"] = np.arange(len(test), dtype=int)\n","\n","    return train, test\n","\n","\n","def build_map(train: pd.DataFrame, keyname: str) -> pd.Series:\n","    \"\"\"\n","    Build key -> mode(label) map for a given key column in train.\n","    Returns a Series indexed by key with label as values.\n","    \"\"\"\n","    grp = (train.groupby([keyname, \"label\"], dropna=False)\n","                 .size()\n","                 .rename(\"cnt\")\n","                 .reset_index())\n","    idx = grp.groupby(keyname)[\"cnt\"].idxmax()\n","    mode_map = grp.loc[idx].set_index(keyname)[\"label\"]\n","    return mode_map\n","\n","\n","def tiered_match_submission(\n","    train_csv=\"train.csv\",\n","    test_csv=\"test.csv\",\n","    submission_csv=\"submission.csv\",\n","    unmatched_preview_csv=\"unmatched_test_preview.csv\",\n","    use_tier2=True,\n","    use_tier3=True,\n","):\n","    \"\"\"\n","    3-tier matching without normalizing stored text:\n","      Tier1: raw exact\n","      Tier2: case/whitespace-insensitive\n","      Tier3: Tier2 + lightweight formatting punctuation ignored\n","    \"\"\"\n","\n","    t0 = time.time()\n","    print(\"Loading data…\")\n","    train, test = load_train_test(train_csv, test_csv)\n","    n_test = len(test)\n","\n","    # Global fallback label\n","    fallback_label = train[\"label\"].value_counts().index[0]\n","    print(f\"Fallback (global most frequent label): {fallback_label}\")\n","\n","    # -----------------------------\n","    # Create comparison keys\n","    # -----------------------------\n","    print(\"Building comparison keys…\")\n","    train = train.copy()\n","    test  = test.copy()\n","\n","    # Tier 1 keys\n","    train[\"k_raw\"] = key_raw(train[\"address\"])\n","    test[\"k_raw\"]  = key_raw(test[\"address\"])\n","\n","    # Tier 2 keys\n","    if use_tier2:\n","        train[\"k_lws\"] = key_lower_ws(train[\"address\"])\n","        test[\"k_lws\"]  = key_lower_ws(test[\"address\"])\n","\n","    # Tier 3 keys\n","    if use_tier3:\n","        train[\"k_lwsp\"] = key_lower_ws_fmtlite(train[\"address\"])\n","        test[\"k_lwsp\"]  = key_lower_ws_fmtlite(test[\"address\"])\n","\n","    # -----------------------------\n","    # Build maps per tier\n","    # -----------------------------\n","    print(\"Building maps…\")\n","    map_raw  = build_map(train, \"k_raw\")\n","    map_lws  = build_map(train, \"k_lws\")  if use_tier2 else None\n","    map_lwsp = build_map(train, \"k_lwsp\") if use_tier3 else None\n","\n","    # -----------------------------\n","    # Tiered labeling\n","    # -----------------------------\n","    res = pd.DataFrame({\"id\": test[\"id\"].values, \"label\": pd.Series([pd.NA]*n_test)})\n","\n","    # Tier 1\n","    lab1 = test[\"k_raw\"].map(map_raw)\n","    mask1 = lab1.notna()\n","    res.loc[mask1, \"label\"] = lab1[mask1].values\n","\n","    # Tier 2\n","    if use_tier2:\n","        need = res[\"label\"].isna()\n","        if need.any():\n","            lab2 = test.loc[need, \"k_lws\"].map(map_lws)\n","            m2 = lab2.notna()\n","            res.loc[need[m2].index, \"label\"] = lab2[m2].values\n","\n","    # Tier 3\n","    if use_tier3:\n","        need = res[\"label\"].isna()\n","        if need.any():\n","            lab3 = test.loc[need, \"k_lwsp\"].map(map_lwsp)\n","            m3 = lab3.notna()\n","            res.loc[need[m3].index, \"label\"] = lab3[m3].values\n","\n","    # Coverage stats BEFORE fallback\n","    matched = res[\"label\"].notna().sum()\n","    unmatched = n_test - matched\n","\n","    # Fallback for remaining\n","    res[\"label\"] = res[\"label\"].fillna(fallback_label)\n","\n","    # Write submission\n","    subm = res[[\"id\", \"label\"]].copy()\n","    subm.to_csv(submission_csv, index=False, encoding=\"utf-8\")\n","\n","    # Save unmatched preview\n","    if unmatched > 0:\n","        prev = test.loc[res[\"label\"].isna() == False]  # This would be none after fill; so keep before fill:\n","        # recompute unmatched indices before fill:\n","        # (We saved unmatched count above; get those rows)\n","        need = (pd.isna(lab1) if 'lab1' in locals() else pd.Series([True]*n_test))\n","        if use_tier2:\n","            need = need & test[\"k_lws\"].map(map_lws).isna()\n","        if use_tier3:\n","            need = need & test[\"k_lwsp\"].map(map_lwsp).isna()\n","        preview = test.loc[need, [\"id\", \"address\"]].head(100)\n","        preview.to_csv(unmatched_preview_csv, index=False, encoding=\"utf-8\")\n","\n","    # Reporting\n","    tier1_cov = mask1.sum()\n","    tier2_cov = 0\n","    tier3_cov = 0\n","    if use_tier2:\n","        # coverage exclusively gained at tier 2\n","        tier2_cov = (res[\"label\"].notna().sum() - tier1_cov) if not use_tier3 else None\n","    if use_tier3:\n","        # recompute tier-by-tier coverage explicitly\n","        # For clear numbers:\n","        cov1 = mask1.sum()\n","        need_after1 = ~mask1\n","        cov2 = 0\n","        if use_tier2:\n","            cov2 = test.loc[need_after1, \"k_lws\"].map(map_lws).notna().sum()\n","        need_after2 = need_after1 & ~(test.loc[need_after1, \"k_lws\"].map(map_lws).notna() if use_tier2 else False)\n","        cov3 = test.loc[need_after2, \"k_lwsp\"].map(map_lwsp).notna().sum() if use_tier3 else 0\n","        tier1_cov, tier2_cov, tier3_cov = cov1, cov2, cov3\n","\n","    print(\"\\n=== Coverage Report (before fallback) ===\")\n","    print(f\"Test rows                         : {n_test:,}\")\n","    print(f\"Tier1 (raw exact) matched         : {tier1_cov:,} ({tier1_cov/n_test:.2%})\")\n","    if use_tier2:\n","        print(f\"Tier2 (+lower/whitespace) matched : {tier2_cov:,} ({tier2_cov/n_test:.2%})\")\n","    if use_tier3:\n","        print(f\"Tier3 (+fmt-lite) matched         : {tier3_cov:,} ({tier3_cov/n_test:.2%})\")\n","    print(f\"Total matched before fallback     : {matched:,} ({matched/n_test:.2%})\")\n","    print(f\"Filled by fallback                : {unmatched:,} ({unmatched/n_test:.2%})\")\n","    print(f\"Saved submission: {submission_csv}\")\n","    if unmatched > 0:\n","        print(f\"Unmatched sample written to: {unmatched_preview_csv}\")\n","\n","    # Optional sanity vs sample_submission\n","    if os.path.exists(\"sample_submission.csv\"):\n","        sample = pd.read_csv(\"sample_submission.csv\", dtype=str, keep_default_na=False)\n","        if list(sample.columns) == [\"id\", \"label\"] and len(sample) == len(subm):\n","            print(\"sample_submission.csv shape matches ✅\")\n","        else:\n","            print(\"Note: sample_submission.csv shape/columns differ.\")\n","\n","    print(f\"Done in {time.time() - t0:.2f}s.\")\n","\n","\n","if __name__ == \"__main__\":\n","    # Default: enable tier2 and tier3 matching for better coverage.\n","    # If you want STRICT 100% raw exact (no case/space tolerance), set both to False.\n","    tiered_match_submission(\n","        train_csv=\"train.csv\",\n","        test_csv=\"test.csv\",\n","        submission_csv=\"submission.csv\",\n","        unmatched_preview_csv=\"unmatched_test_preview.csv\",\n","        use_tier2=True,\n","        use_tier3=True,\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"iWSRZJJBnHS9","executionInfo":{"status":"error","timestamp":1756383478008,"user_tz":-180,"elapsed":33667,"user":{"displayName":"Muhammet Fatih Çetintas","userId":"02746250817952515216"}},"outputId":"d9baec77-3a39-4ea0-c1d7-c9dcbcdabbff"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data…\n","Fallback (global most frequent label): 5414\n","Building comparison keys…\n","Building maps…\n"]},{"output_type":"error","ename":"IndexingError","evalue":"Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2579687050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# Default: enable tier2 and tier3 matching for better coverage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# If you want STRICT 100% raw exact (no case/space tolerance), set both to False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     tiered_match_submission(\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mtrain_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mtest_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2579687050.py\u001b[0m in \u001b[0;36mtiered_match_submission\u001b[0;34m(train_csv, test_csv, submission_csv, unmatched_preview_csv, use_tier2, use_tier3)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mlab2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"k_lws\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_lws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlab2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlab2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Tier 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rows_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m             raise IndexingError(\n\u001b[0m\u001b[1;32m   2663\u001b[0m                 \u001b[0;34m\"Unalignable boolean Series provided as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;34m\"indexer (index of the boolean Series and of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."]}]}]}